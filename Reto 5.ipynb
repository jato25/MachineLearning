{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de canciones con perceptrón multinivel\n",
    "### Javier Andres Tellez Ortiz - 201617861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se descarga el conjunto de datos y se descomprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in d:\\programas\\anaconda3\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................................] 12656044 / 12656044"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "from zipfile import ZipFile\n",
    "\n",
    "##Se descarga el archivo del repositorio \n",
    "file = wget.download(\"http://millionsongdataset.com/sites/default/files/AdditionalFiles/msd_genre_dataset.zip\")\n",
    "\n",
    "##Se abre el archivo y se descomprime\n",
    "zpFile = ZipFile(file)\n",
    "zpFile.extractall()\n",
    "zpFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se abre el conjunto de datos, se eliminan los datos con algún elemento faltante y se examina la estructura del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%genre</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>title</th>\n",
       "      <th>loudness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>key</th>\n",
       "      <th>mode</th>\n",
       "      <th>duration</th>\n",
       "      <th>...</th>\n",
       "      <th>var_timbre3</th>\n",
       "      <th>var_timbre4</th>\n",
       "      <th>var_timbre5</th>\n",
       "      <th>var_timbre6</th>\n",
       "      <th>var_timbre7</th>\n",
       "      <th>var_timbre8</th>\n",
       "      <th>var_timbre9</th>\n",
       "      <th>var_timbre10</th>\n",
       "      <th>var_timbre11</th>\n",
       "      <th>var_timbre12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>classic pop and rock</td>\n",
       "      <td>TRFCOOU128F427AEC0</td>\n",
       "      <td>Blue Oyster Cult</td>\n",
       "      <td>Mes Dames Sarat</td>\n",
       "      <td>-8.697</td>\n",
       "      <td>155.007</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>246.33424</td>\n",
       "      <td>...</td>\n",
       "      <td>1255.514569</td>\n",
       "      <td>580.030472</td>\n",
       "      <td>598.485223</td>\n",
       "      <td>575.337671</td>\n",
       "      <td>322.068603</td>\n",
       "      <td>321.726029</td>\n",
       "      <td>232.700609</td>\n",
       "      <td>186.805303</td>\n",
       "      <td>181.938688</td>\n",
       "      <td>151.508011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic pop and rock</td>\n",
       "      <td>TRNJTPB128F427AE9F</td>\n",
       "      <td>Blue Oyster Cult</td>\n",
       "      <td>Screams</td>\n",
       "      <td>-10.659</td>\n",
       "      <td>148.462</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>189.80526</td>\n",
       "      <td>...</td>\n",
       "      <td>2007.653070</td>\n",
       "      <td>1043.474073</td>\n",
       "      <td>585.694981</td>\n",
       "      <td>564.013736</td>\n",
       "      <td>510.177022</td>\n",
       "      <td>400.200186</td>\n",
       "      <td>365.119588</td>\n",
       "      <td>238.099708</td>\n",
       "      <td>197.933757</td>\n",
       "      <td>251.577525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic pop and rock</td>\n",
       "      <td>TRLFJHA128F427AEEA</td>\n",
       "      <td>Blue Oyster Cult</td>\n",
       "      <td>Dance The Night Away</td>\n",
       "      <td>-13.494</td>\n",
       "      <td>112.909</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>158.19710</td>\n",
       "      <td>...</td>\n",
       "      <td>1204.856777</td>\n",
       "      <td>2736.520024</td>\n",
       "      <td>730.233239</td>\n",
       "      <td>665.203452</td>\n",
       "      <td>535.775111</td>\n",
       "      <td>439.335059</td>\n",
       "      <td>486.822970</td>\n",
       "      <td>265.333860</td>\n",
       "      <td>447.097987</td>\n",
       "      <td>251.880724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>classic pop and rock</td>\n",
       "      <td>TRCQZAG128F427DB97</td>\n",
       "      <td>Blue Oyster Cult</td>\n",
       "      <td>Debbie Denise</td>\n",
       "      <td>-12.786</td>\n",
       "      <td>117.429</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>250.22649</td>\n",
       "      <td>...</td>\n",
       "      <td>809.755802</td>\n",
       "      <td>563.908070</td>\n",
       "      <td>492.803819</td>\n",
       "      <td>378.382799</td>\n",
       "      <td>372.875044</td>\n",
       "      <td>231.941957</td>\n",
       "      <td>246.313305</td>\n",
       "      <td>168.400152</td>\n",
       "      <td>85.282462</td>\n",
       "      <td>339.897173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>classic pop and rock</td>\n",
       "      <td>TRNXMNM128F427DB8C</td>\n",
       "      <td>Blue Oyster Cult</td>\n",
       "      <td>(Don't Fear) The Reaper</td>\n",
       "      <td>-14.093</td>\n",
       "      <td>141.536</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>307.06893</td>\n",
       "      <td>...</td>\n",
       "      <td>1093.684935</td>\n",
       "      <td>343.556047</td>\n",
       "      <td>889.163314</td>\n",
       "      <td>218.111796</td>\n",
       "      <td>304.862864</td>\n",
       "      <td>178.352161</td>\n",
       "      <td>440.478867</td>\n",
       "      <td>142.669283</td>\n",
       "      <td>81.061326</td>\n",
       "      <td>208.355152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 %genre            track_id       artist_name  \\\n",
       "0  classic pop and rock  TRFCOOU128F427AEC0  Blue Oyster Cult   \n",
       "1  classic pop and rock  TRNJTPB128F427AE9F  Blue Oyster Cult   \n",
       "2  classic pop and rock  TRLFJHA128F427AEEA  Blue Oyster Cult   \n",
       "3  classic pop and rock  TRCQZAG128F427DB97  Blue Oyster Cult   \n",
       "4  classic pop and rock  TRNXMNM128F427DB8C  Blue Oyster Cult   \n",
       "\n",
       "                     title  loudness    tempo  time_signature  key  mode  \\\n",
       "0          Mes Dames Sarat    -8.697  155.007               1    9     1   \n",
       "1                  Screams   -10.659  148.462               1    4     0   \n",
       "2     Dance The Night Away   -13.494  112.909               1   10     0   \n",
       "3            Debbie Denise   -12.786  117.429               4    7     1   \n",
       "4  (Don't Fear) The Reaper   -14.093  141.536               4    9     0   \n",
       "\n",
       "    duration  ...  var_timbre3  var_timbre4  var_timbre5  var_timbre6  \\\n",
       "0  246.33424  ...  1255.514569   580.030472   598.485223   575.337671   \n",
       "1  189.80526  ...  2007.653070  1043.474073   585.694981   564.013736   \n",
       "2  158.19710  ...  1204.856777  2736.520024   730.233239   665.203452   \n",
       "3  250.22649  ...   809.755802   563.908070   492.803819   378.382799   \n",
       "4  307.06893  ...  1093.684935   343.556047   889.163314   218.111796   \n",
       "\n",
       "   var_timbre7  var_timbre8  var_timbre9  var_timbre10  var_timbre11  \\\n",
       "0   322.068603   321.726029   232.700609    186.805303    181.938688   \n",
       "1   510.177022   400.200186   365.119588    238.099708    197.933757   \n",
       "2   535.775111   439.335059   486.822970    265.333860    447.097987   \n",
       "3   372.875044   231.941957   246.313305    168.400152     85.282462   \n",
       "4   304.862864   178.352161   440.478867    142.669283     81.061326   \n",
       "\n",
       "   var_timbre12  \n",
       "0    151.508011  \n",
       "1    251.577525  \n",
       "2    251.880724  \n",
       "3    339.897173  \n",
       "4    208.355152  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = pd.read_csv(\"msd_genre_dataset.txt\", skiprows = range(9))\n",
    "\n",
    "dataset.dropna()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se obtienen los datos de las clases con los que se generará el modelo.  Previamente, se eliminaron las columnas 'track_id', 'artist_name' y 'title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns = [\"track_id\", \"artist_name\", \"title\"])\n",
    "features = dataset.columns.tolist()\n",
    "dataset = dataset[(dataset[\"%genre\"] == \"jazz and blues\") | (dataset[\"%genre\"] == \"soul and reggae\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se obtiene la cantidad total de datos pertenecientes a cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jazz and blues</th>\n",
       "      <td>4334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soul and reggae</th>\n",
       "      <td>4016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 %genre\n",
       "jazz and blues     4334\n",
       "soul and reggae    4016"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset[\"%genre\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se divide la información del archivo entre etiquetas y carácteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[\"%genre\"].values\n",
    "X = dataset.values[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como se evidenció en entregas anteriores, es necesaria la estándarizacion de los datos. Además se tranforman las etiquetas de cada calse en números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "scaler = StandardScaler()\n",
    "lblEncoder = LabelEncoder()\n",
    "\n",
    "y = lblEncoder.fit(np.unique(y)).transform(y)\n",
    "##Se estandarizan los datos provenientes del archivo\n",
    "X = scaler.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se dividen los datos en datos de entrenamiento y datos de prueba, tomando el 20% del total para este último fin. Antes de realizar la división, los datos son barajados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de prueba clase 0 (jazz and blues): 879\n",
      "Datos de prueba clase 1 (soul and reggae): 791\n",
      "Total datos prueba: 1670\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size = 0.2, random_state = 7861)\n",
    "\n",
    "genres, cantidad = np.unique(y_test, return_counts = True)\n",
    "total_test_data = sum(cantidad)\n",
    "labels = lblEncoder.inverse_transform(genres)\n",
    "print(\"Datos de prueba clase %s (%s): %d\" % (genres[0],labels[0],cantidad[0]))\n",
    "print(\"Datos de prueba clase %s (%s): %d\" % (genres[1],labels[1],cantidad[1]))\n",
    "print(\"Total datos prueba: %d\" % total_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se toma el 10% de los datos de entrenamiento como datos de validación, esto con el objetivo de comparar cada uno de los modelos que serán entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento clase 0 (jazz and blues): 3123\n",
      "Datos de entrenamiento clase 1 (soul and reggae): 2889\n",
      "Datos de validacion clase 0 (jazz and blues): 332\n",
      "Datos de validacion clase 1 (soul and reggae): 336\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.1, random_state = 7861)\n",
    "\n",
    "genres, cantidad = np.unique(y_train, return_counts = True)\n",
    "labels = lblEncoder.inverse_transform(genres)\n",
    "print(\"Datos de entrenamiento clase %s (%s): %d\" % (genres[0],labels[0],cantidad[0]))\n",
    "print(\"Datos de entrenamiento clase %s (%s): %d\" % (genres[1],labels[1],cantidad[1]))\n",
    "\n",
    "genres, cantidad = np.unique(y_validation, return_counts = True)\n",
    "labels = lblEncoder.inverse_transform(genres)\n",
    "print(\"Datos de validacion clase %s (%s): %d\" % (genres[0],labels[0],cantidad[0]))\n",
    "print(\"Datos de validacion clase %s (%s): %d\" % (genres[1],labels[1],cantidad[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se realiza el entrenamiento de las redes neuronales cambiando la cantidad de neuronas en la capa escondida entre 2 y el total de dimensiones del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def getMoldel(neurons, activation='relu', optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=neurons, activation=activation, input_dim=X.shape[1]))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_neurons = []\n",
    "best_models_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 3s 424us/sample - loss: 0.6614 - val_loss: 0.5842\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.5487 - val_loss: 0.5253\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5032 - val_loss: 0.4939\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4755 - val_loss: 0.4723\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4557 - val_loss: 0.4560\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4405 - val_loss: 0.4437\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4294 - val_loss: 0.4334\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4199 - val_loss: 0.4268\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4120 - val_loss: 0.4199\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4054 - val_loss: 0.4138\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3993 - val_loss: 0.4079\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3946 - val_loss: 0.4034\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3899 - val_loss: 0.3994\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3862 - val_loss: 0.3974\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3829 - val_loss: 0.3950\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3803 - val_loss: 0.3914\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3779 - val_loss: 0.3891\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3760 - val_loss: 0.3888\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3742 - val_loss: 0.3857\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3725 - val_loss: 0.3843\n",
      "Epoch 21/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3711 - val_loss: 0.3814\n",
      "Epoch 22/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3697 - val_loss: 0.3802\n",
      "Epoch 23/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3682 - val_loss: 0.3793\n",
      "Epoch 24/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3672 - val_loss: 0.3788\n",
      "Epoch 25/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3663 - val_loss: 0.3778\n",
      "Epoch 26/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3653 - val_loss: 0.3775\n",
      "Epoch 27/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3646 - val_loss: 0.3781\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 198us/sample - loss: 0.7597 - val_loss: 0.5509\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.5273 - val_loss: 0.4568\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4634 - val_loss: 0.4157\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4291 - val_loss: 0.3962\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4084 - val_loss: 0.3870\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3951 - val_loss: 0.3807\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3864 - val_loss: 0.3769\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3796 - val_loss: 0.3727\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3752 - val_loss: 0.3683\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3710 - val_loss: 0.3664\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3673 - val_loss: 0.3617\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3640 - val_loss: 0.3605\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 81us/sample - loss: 0.3612 - val_loss: 0.3587\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3588 - val_loss: 0.3554\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3566 - val_loss: 0.3544\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3542 - val_loss: 0.3512\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3525 - val_loss: 0.3499\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3511 - val_loss: 0.3484\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3493 - val_loss: 0.3481\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3481 - val_loss: 0.3463\n",
      "Epoch 21/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3465 - val_loss: 0.3451\n",
      "Epoch 22/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3452 - val_loss: 0.3437\n",
      "Epoch 23/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3442 - val_loss: 0.3445\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 198us/sample - loss: 0.5547 - val_loss: 0.4574\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4322 - val_loss: 0.3915\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3919 - val_loss: 0.3760\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3765 - val_loss: 0.3683\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3680 - val_loss: 0.3659\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3623 - val_loss: 0.3634\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3577 - val_loss: 0.3621\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3541 - val_loss: 0.3606\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3511 - val_loss: 0.3599\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3486 - val_loss: 0.3582\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3468 - val_loss: 0.3573\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3449 - val_loss: 0.3554\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3432 - val_loss: 0.3549\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3418 - val_loss: 0.3539\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3405 - val_loss: 0.3537\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3396 - val_loss: 0.3525\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3384 - val_loss: 0.3529\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 190us/sample - loss: 0.9546 - val_loss: 0.6986\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.6410 - val_loss: 0.5660\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5505 - val_loss: 0.5120\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5084 - val_loss: 0.4844\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4831 - val_loss: 0.4668\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4651 - val_loss: 0.4528\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4503 - val_loss: 0.4421\n",
      "Epoch 8/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4387 - val_loss: 0.4314\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4283 - val_loss: 0.4218\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4195 - val_loss: 0.4131\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4114 - val_loss: 0.4050\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.4049 - val_loss: 0.3990\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3989 - val_loss: 0.3917\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3934 - val_loss: 0.3862\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3886 - val_loss: 0.3840\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3838 - val_loss: 0.3784\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3803 - val_loss: 0.3735\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3768 - val_loss: 0.3711\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3740 - val_loss: 0.3675\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3712 - val_loss: 0.3680\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 190us/sample - loss: 0.6938 - val_loss: 0.4984\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4824 - val_loss: 0.3972\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4125 - val_loss: 0.3653\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3855 - val_loss: 0.3543\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3731 - val_loss: 0.3498\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3658 - val_loss: 0.3467\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3607 - val_loss: 0.3440\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3562 - val_loss: 0.3432\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3525 - val_loss: 0.3422\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3485 - val_loss: 0.3405\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3456 - val_loss: 0.3399\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3428 - val_loss: 0.3375\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3399 - val_loss: 0.3359\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3377 - val_loss: 0.3339\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3355 - val_loss: 0.3327\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3332 - val_loss: 0.3313\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3314 - val_loss: 0.3307\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3296 - val_loss: 0.3278\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3275 - val_loss: 0.3283\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 198us/sample - loss: 0.5483 - val_loss: 0.4297\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4194 - val_loss: 0.3806\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3833 - val_loss: 0.3686\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3693 - val_loss: 0.3637\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3606 - val_loss: 0.3596\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3554 - val_loss: 0.3576\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3506 - val_loss: 0.3539\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3467 - val_loss: 0.3498\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3436 - val_loss: 0.3498\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 192us/sample - loss: 0.6633 - val_loss: 0.4700\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4263 - val_loss: 0.3995\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3890 - val_loss: 0.3787\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3728 - val_loss: 0.3701\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3632 - val_loss: 0.3650\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3563 - val_loss: 0.3641\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3510 - val_loss: 0.3627\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3465 - val_loss: 0.3592\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3428 - val_loss: 0.3589\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3398 - val_loss: 0.3574\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3367 - val_loss: 0.3574\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3345 - val_loss: 0.3562\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3323 - val_loss: 0.3573\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 208us/sample - loss: 0.5308 - val_loss: 0.4297\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.4085 - val_loss: 0.3810\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3732 - val_loss: 0.3669\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3570 - val_loss: 0.3601\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3482 - val_loss: 0.3562\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3417 - val_loss: 0.3524\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3365 - val_loss: 0.3486\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3329 - val_loss: 0.3448\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3295 - val_loss: 0.3427\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3269 - val_loss: 0.3422\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3245 - val_loss: 0.3404\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3224 - val_loss: 0.3405\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 187us/sample - loss: 0.6151 - val_loss: 0.4604\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4325 - val_loss: 0.3958\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3907 - val_loss: 0.3788\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3728 - val_loss: 0.3701\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3621 - val_loss: 0.3664\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3547 - val_loss: 0.3605\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3488 - val_loss: 0.3562\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3436 - val_loss: 0.3524\n",
      "Epoch 9/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3391 - val_loss: 0.3500\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3351 - val_loss: 0.3462\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3311 - val_loss: 0.3433\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3276 - val_loss: 0.3402\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3245 - val_loss: 0.3374\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3216 - val_loss: 0.3372\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3191 - val_loss: 0.3355\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3170 - val_loss: 0.3340\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3148 - val_loss: 0.3334\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3131 - val_loss: 0.3314\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3115 - val_loss: 0.3304\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3097 - val_loss: 0.3284\n",
      "Epoch 21/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3084 - val_loss: 0.3293\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5358 - val_loss: 0.4402\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4080 - val_loss: 0.3958\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3808 - val_loss: 0.3775\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3670 - val_loss: 0.3682\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3573 - val_loss: 0.3601\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3499 - val_loss: 0.3537\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3446 - val_loss: 0.3482\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3407 - val_loss: 0.3467\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3364 - val_loss: 0.3434\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3333 - val_loss: 0.3411\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3304 - val_loss: 0.3390\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3281 - val_loss: 0.3371\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3256 - val_loss: 0.3353\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3234 - val_loss: 0.3344\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3216 - val_loss: 0.3344\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3193 - val_loss: 0.3341\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3174 - val_loss: 0.3319\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3156 - val_loss: 0.3310\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3133 - val_loss: 0.3330\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 192us/sample - loss: 0.4903 - val_loss: 0.4070\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3928 - val_loss: 0.3767\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3667 - val_loss: 0.3683\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3540 - val_loss: 0.3614\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3453 - val_loss: 0.3550\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3382 - val_loss: 0.3525\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3326 - val_loss: 0.3470\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3281 - val_loss: 0.3439\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3240 - val_loss: 0.3415\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3200 - val_loss: 0.3396\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3166 - val_loss: 0.3397\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5855 - val_loss: 0.4197\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4134 - val_loss: 0.3701\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3776 - val_loss: 0.3525\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3623 - val_loss: 0.3435\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3525 - val_loss: 0.3374\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3459 - val_loss: 0.3330\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3410 - val_loss: 0.3291\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3368 - val_loss: 0.3281\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3321 - val_loss: 0.3244\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3288 - val_loss: 0.3233\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3258 - val_loss: 0.3194\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3222 - val_loss: 0.3198\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 174us/sample - loss: 0.4872 - val_loss: 0.4091\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3908 - val_loss: 0.3740\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3672 - val_loss: 0.3588\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3545 - val_loss: 0.3501\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3454 - val_loss: 0.3428\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3392 - val_loss: 0.3410\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3329 - val_loss: 0.3403\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3287 - val_loss: 0.3345\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3248 - val_loss: 0.3343\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3217 - val_loss: 0.3317\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3183 - val_loss: 0.3313\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3152 - val_loss: 0.3317\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5122 - val_loss: 0.4050\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3960 - val_loss: 0.3662\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3682 - val_loss: 0.3547\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3559 - val_loss: 0.3492\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3480 - val_loss: 0.3437\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3420 - val_loss: 0.3405\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3362 - val_loss: 0.3381\n",
      "Epoch 8/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3314 - val_loss: 0.3356\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3266 - val_loss: 0.3334\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3221 - val_loss: 0.3329\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3181 - val_loss: 0.3328\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3140 - val_loss: 0.3305\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3099 - val_loss: 0.3298\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3065 - val_loss: 0.3291\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3037 - val_loss: 0.3260\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3004 - val_loss: 0.3253\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.2978 - val_loss: 0.3262\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5436 - val_loss: 0.3986\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3932 - val_loss: 0.3567\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3649 - val_loss: 0.3497\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3531 - val_loss: 0.3451\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3450 - val_loss: 0.3400\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3381 - val_loss: 0.3346\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3323 - val_loss: 0.3317\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3269 - val_loss: 0.3280\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3223 - val_loss: 0.3258\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3176 - val_loss: 0.3246\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3141 - val_loss: 0.3231\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3100 - val_loss: 0.3232\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 190us/sample - loss: 0.5081 - val_loss: 0.3937\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3908 - val_loss: 0.3586\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3663 - val_loss: 0.3469\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3530 - val_loss: 0.3403\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3437 - val_loss: 0.3365\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3362 - val_loss: 0.3330\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3306 - val_loss: 0.3299\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3245 - val_loss: 0.3303\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.6550 - val_loss: 0.4280\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4060 - val_loss: 0.3642\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3668 - val_loss: 0.3527\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3532 - val_loss: 0.3457\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3449 - val_loss: 0.3431\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3380 - val_loss: 0.3370\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3325 - val_loss: 0.3316\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3268 - val_loss: 0.3293\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3227 - val_loss: 0.3252\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3180 - val_loss: 0.3226\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3142 - val_loss: 0.3202\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3103 - val_loss: 0.3188\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3079 - val_loss: 0.3167\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3045 - val_loss: 0.3163\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3020 - val_loss: 0.3151\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.2995 - val_loss: 0.3165\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5058 - val_loss: 0.3901\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3884 - val_loss: 0.3586\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3602 - val_loss: 0.3502\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3459 - val_loss: 0.3369\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3358 - val_loss: 0.3300\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3274 - val_loss: 0.3214\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3209 - val_loss: 0.3183\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3151 - val_loss: 0.3147\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3107 - val_loss: 0.3093\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3068 - val_loss: 0.3075\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3035 - val_loss: 0.3064\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3006 - val_loss: 0.3080\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 198us/sample - loss: 0.5580 - val_loss: 0.4138\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3958 - val_loss: 0.3603\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3607 - val_loss: 0.3509\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3474 - val_loss: 0.3453\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3391 - val_loss: 0.3401\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3328 - val_loss: 0.3387\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3271 - val_loss: 0.3334\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3222 - val_loss: 0.3311\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3179 - val_loss: 0.3301\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3134 - val_loss: 0.3322\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.4868 - val_loss: 0.3990\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3822 - val_loss: 0.3770\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3595 - val_loss: 0.3650\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3467 - val_loss: 0.3556\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3383 - val_loss: 0.3516\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3313 - val_loss: 0.3463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3255 - val_loss: 0.3422\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3206 - val_loss: 0.3408\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3160 - val_loss: 0.3408\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 213us/sample - loss: 0.5430 - val_loss: 0.4024\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3969 - val_loss: 0.3672\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3694 - val_loss: 0.3586\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3556 - val_loss: 0.3502\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3458 - val_loss: 0.3461\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3382 - val_loss: 0.3444\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3316 - val_loss: 0.3391\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3263 - val_loss: 0.3384\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3209 - val_loss: 0.3386\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.4799 - val_loss: 0.3936\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3809 - val_loss: 0.3736\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3619 - val_loss: 0.3631\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3519 - val_loss: 0.3600\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3431 - val_loss: 0.3538\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3367 - val_loss: 0.3480\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3303 - val_loss: 0.3458\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3245 - val_loss: 0.3425\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3186 - val_loss: 0.3374\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3140 - val_loss: 0.3354\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3096 - val_loss: 0.3323\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3053 - val_loss: 0.3310\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3018 - val_loss: 0.3279\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2981 - val_loss: 0.3305\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5365 - val_loss: 0.4061\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3905 - val_loss: 0.3755\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3656 - val_loss: 0.3580\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3527 - val_loss: 0.3495\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3429 - val_loss: 0.3417\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3353 - val_loss: 0.3354\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3289 - val_loss: 0.3300\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3233 - val_loss: 0.3277\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3184 - val_loss: 0.3248\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3136 - val_loss: 0.3242\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3093 - val_loss: 0.3253\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 229us/sample - loss: 0.5678 - val_loss: 0.4053\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3881 - val_loss: 0.3649\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3611 - val_loss: 0.3586\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3489 - val_loss: 0.3475\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3404 - val_loss: 0.3401\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3323 - val_loss: 0.3357\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3256 - val_loss: 0.3321\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3199 - val_loss: 0.3307\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3150 - val_loss: 0.3260\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3099 - val_loss: 0.3266\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 169us/sample - loss: 0.4444 - val_loss: 0.3753\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3681 - val_loss: 0.3591\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3509 - val_loss: 0.3521\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3405 - val_loss: 0.3458\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3321 - val_loss: 0.3429\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3256 - val_loss: 0.3393\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3192 - val_loss: 0.3382\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3145 - val_loss: 0.3342\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3087 - val_loss: 0.3324\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3047 - val_loss: 0.3346\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5237 - val_loss: 0.3940\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3839 - val_loss: 0.3598\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3578 - val_loss: 0.3443\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3456 - val_loss: 0.3400\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3374 - val_loss: 0.3386\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3299 - val_loss: 0.3350\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3240 - val_loss: 0.3335\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3180 - val_loss: 0.3346\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 200us/sample - loss: 0.4909 - val_loss: 0.3835\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3763 - val_loss: 0.3656\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3564 - val_loss: 0.3522\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3448 - val_loss: 0.3421\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3363 - val_loss: 0.3370\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3298 - val_loss: 0.3309\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3243 - val_loss: 0.3250\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3187 - val_loss: 0.3272\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5076 - val_loss: 0.3944\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3823 - val_loss: 0.3666\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3603 - val_loss: 0.3502\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3479 - val_loss: 0.3423\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3389 - val_loss: 0.3381\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3328 - val_loss: 0.3334\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3261 - val_loss: 0.3254\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3204 - val_loss: 0.3257\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.4793 - val_loss: 0.3639\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3656 - val_loss: 0.3484\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3463 - val_loss: 0.3410\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3357 - val_loss: 0.3381\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3270 - val_loss: 0.3315\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3192 - val_loss: 0.3300\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3133 - val_loss: 0.3245\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3072 - val_loss: 0.3197\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3025 - val_loss: 0.3197\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2980 - val_loss: 0.3198\n",
      "El modelo con mejor desempeño tiene 19 neuronas en la capa oculta\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 153us/sample - loss: 0.7714 - val_loss: 0.6416\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.6172 - val_loss: 0.5633\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.5528 - val_loss: 0.5144\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.5146 - val_loss: 0.4840\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4895 - val_loss: 0.4642\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.4714 - val_loss: 0.4504\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4576 - val_loss: 0.4406\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.4467 - val_loss: 0.4329\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4376 - val_loss: 0.4254\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.4298 - val_loss: 0.4185\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4233 - val_loss: 0.4128\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.4173 - val_loss: 0.4086\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4126 - val_loss: 0.4036\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - ETA: 0s - loss: 0.406 - 0s 52us/sample - loss: 0.4078 - val_loss: 0.4003\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.4037 - val_loss: 0.3964\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3997 - val_loss: 0.3910\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3960 - val_loss: 0.3892\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3927 - val_loss: 0.3858\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - ETA: 0s - loss: 0.393 - 0s 52us/sample - loss: 0.3896 - val_loss: 0.3832\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3868 - val_loss: 0.3809\n",
      "Epoch 21/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3847 - val_loss: 0.3786\n",
      "Epoch 22/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3824 - val_loss: 0.3760\n",
      "Epoch 23/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3803 - val_loss: 0.3747\n",
      "Epoch 24/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3788 - val_loss: 0.3728\n",
      "Epoch 25/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3773 - val_loss: 0.3729\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 205us/sample - loss: 0.7004 - val_loss: 0.6125\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.5841 - val_loss: 0.5482\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.5342 - val_loss: 0.5107\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.5023 - val_loss: 0.4852\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4734 - val_loss: 0.4512\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4327 - val_loss: 0.4123\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4045 - val_loss: 0.3951\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3908 - val_loss: 0.3855\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3828 - val_loss: 0.3796\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3777 - val_loss: 0.3756\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3738 - val_loss: 0.3725\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3707 - val_loss: 0.3704\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3684 - val_loss: 0.3677\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3667 - val_loss: 0.3663\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3649 - val_loss: 0.3655\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3635 - val_loss: 0.3643\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3619 - val_loss: 0.3635\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3605 - val_loss: 0.3634\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3593 - val_loss: 0.3629\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3581 - val_loss: 0.3616\n",
      "Epoch 21/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3567 - val_loss: 0.3599\n",
      "Epoch 22/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3553 - val_loss: 0.3581\n",
      "Epoch 23/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3537 - val_loss: 0.3572\n",
      "Epoch 24/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3524 - val_loss: 0.3557\n",
      "Epoch 25/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3509 - val_loss: 0.3537\n",
      "Epoch 26/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3496 - val_loss: 0.3535\n",
      "Epoch 27/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3480 - val_loss: 0.3519\n",
      "Epoch 28/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3467 - val_loss: 0.3506\n",
      "Epoch 29/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3453 - val_loss: 0.3491\n",
      "Epoch 30/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3442 - val_loss: 0.3488\n",
      "Epoch 31/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3430 - val_loss: 0.3481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3419 - val_loss: 0.3456\n",
      "Epoch 33/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3412 - val_loss: 0.3457\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5311 - val_loss: 0.4605\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4337 - val_loss: 0.4108\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3983 - val_loss: 0.3933\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3823 - val_loss: 0.3853\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3735 - val_loss: 0.3810\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3675 - val_loss: 0.3788\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3623 - val_loss: 0.3770\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3582 - val_loss: 0.3754\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3549 - val_loss: 0.3749\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3521 - val_loss: 0.3739\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3496 - val_loss: 0.3728\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3473 - val_loss: 0.3704\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3449 - val_loss: 0.3693\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3429 - val_loss: 0.3678\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3412 - val_loss: 0.3659\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3392 - val_loss: 0.3647\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3378 - val_loss: 0.3629\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3364 - val_loss: 0.3632\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5897 - val_loss: 0.5202\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4717 - val_loss: 0.4326\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4165 - val_loss: 0.3939\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3917 - val_loss: 0.3760\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3772 - val_loss: 0.3649\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3674 - val_loss: 0.3563\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3608 - val_loss: 0.3516\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3556 - val_loss: 0.3477\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3515 - val_loss: 0.3452\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3481 - val_loss: 0.3435\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3454 - val_loss: 0.3425\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3432 - val_loss: 0.3422\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3411 - val_loss: 0.3415\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3395 - val_loss: 0.3417\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 200us/sample - loss: 0.5427 - val_loss: 0.4497\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4328 - val_loss: 0.3904\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3947 - val_loss: 0.3702\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3798 - val_loss: 0.3602\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3707 - val_loss: 0.3549\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3642 - val_loss: 0.3486\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3598 - val_loss: 0.3457\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3559 - val_loss: 0.3431\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3527 - val_loss: 0.3401\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3501 - val_loss: 0.3382\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3479 - val_loss: 0.3366\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3457 - val_loss: 0.3359\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3439 - val_loss: 0.3363\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5341 - val_loss: 0.4605\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4244 - val_loss: 0.4035\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3881 - val_loss: 0.3786\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3710 - val_loss: 0.3663\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3615 - val_loss: 0.3591\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3553 - val_loss: 0.3522\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3508 - val_loss: 0.3485\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3472 - val_loss: 0.3456\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3441 - val_loss: 0.3430\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3417 - val_loss: 0.3381\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3395 - val_loss: 0.3362\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3374 - val_loss: 0.3338\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3353 - val_loss: 0.3324\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3336 - val_loss: 0.3310\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3319 - val_loss: 0.3297\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3305 - val_loss: 0.3272\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3288 - val_loss: 0.3267\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3275 - val_loss: 0.3260\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3263 - val_loss: 0.3245\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3248 - val_loss: 0.3242\n",
      "Epoch 21/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3236 - val_loss: 0.3237\n",
      "Epoch 22/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3223 - val_loss: 0.3235\n",
      "Epoch 23/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3209 - val_loss: 0.3227\n",
      "Epoch 24/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3193 - val_loss: 0.3226\n",
      "Epoch 25/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3181 - val_loss: 0.3215\n",
      "Epoch 26/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3164 - val_loss: 0.3204\n",
      "Epoch 27/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3151 - val_loss: 0.3199\n",
      "Epoch 28/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 58us/sample - loss: 0.3137 - val_loss: 0.3194\n",
      "Epoch 29/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3125 - val_loss: 0.3198\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 160us/sample - loss: 0.6070 - val_loss: 0.4556\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4242 - val_loss: 0.3862\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3811 - val_loss: 0.3706\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3650 - val_loss: 0.3633\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3552 - val_loss: 0.3592\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3494 - val_loss: 0.3551\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3442 - val_loss: 0.3516\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3408 - val_loss: 0.3492\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3372 - val_loss: 0.3479\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3339 - val_loss: 0.3450\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3311 - val_loss: 0.3453\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 211us/sample - loss: 0.5184 - val_loss: 0.4106\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4041 - val_loss: 0.3729\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3741 - val_loss: 0.3651\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3614 - val_loss: 0.3626\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3540 - val_loss: 0.3579\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3484 - val_loss: 0.3563\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3435 - val_loss: 0.3535\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3389 - val_loss: 0.3524\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3350 - val_loss: 0.3473\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3318 - val_loss: 0.3447\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3281 - val_loss: 0.3424\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3251 - val_loss: 0.3410\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3220 - val_loss: 0.3403\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3197 - val_loss: 0.3401\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 58us/sample - loss: 0.3177 - val_loss: 0.3379\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3156 - val_loss: 0.3336\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3138 - val_loss: 0.3335\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3123 - val_loss: 0.3336\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5948 - val_loss: 0.4696\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4317 - val_loss: 0.3943\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3853 - val_loss: 0.3685\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3654 - val_loss: 0.3556\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3531 - val_loss: 0.3467\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 58us/sample - loss: 0.3452 - val_loss: 0.3418\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3393 - val_loss: 0.3367\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3343 - val_loss: 0.3342\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3306 - val_loss: 0.3328\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3269 - val_loss: 0.3312\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3243 - val_loss: 0.3270\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3212 - val_loss: 0.3256\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3191 - val_loss: 0.3260\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5622 - val_loss: 0.4308\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4098 - val_loss: 0.3717\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3742 - val_loss: 0.3563\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3606 - val_loss: 0.3499\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3525 - val_loss: 0.3435\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3469 - val_loss: 0.3404\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3419 - val_loss: 0.3360\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3380 - val_loss: 0.3330\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3343 - val_loss: 0.3323\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3311 - val_loss: 0.3290\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3282 - val_loss: 0.3271\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3258 - val_loss: 0.3270\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3231 - val_loss: 0.3253\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3207 - val_loss: 0.3244\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3182 - val_loss: 0.3237\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3159 - val_loss: 0.3215\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3141 - val_loss: 0.3218\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5049 - val_loss: 0.4055\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4066 - val_loss: 0.3766\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3795 - val_loss: 0.3631\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3650 - val_loss: 0.3544\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3545 - val_loss: 0.3472\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3471 - val_loss: 0.3410\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3410 - val_loss: 0.3398\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3352 - val_loss: 0.3341\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3307 - val_loss: 0.3307\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3263 - val_loss: 0.3288\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3229 - val_loss: 0.3258\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3193 - val_loss: 0.3263\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 1s 211us/sample - loss: 0.5021 - val_loss: 0.4027\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3907 - val_loss: 0.3694\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3693 - val_loss: 0.3584\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 58us/sample - loss: 0.3576 - val_loss: 0.3522\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3494 - val_loss: 0.3452\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3427 - val_loss: 0.3433\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3373 - val_loss: 0.3382\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3324 - val_loss: 0.3358\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3282 - val_loss: 0.3332\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3245 - val_loss: 0.3326\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3204 - val_loss: 0.3304\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3176 - val_loss: 0.3290\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3143 - val_loss: 0.3278\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3115 - val_loss: 0.3268\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3088 - val_loss: 0.3265\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3068 - val_loss: 0.3283\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5323 - val_loss: 0.4133\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3983 - val_loss: 0.3760\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3707 - val_loss: 0.3642\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3577 - val_loss: 0.3576\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3486 - val_loss: 0.3501\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3422 - val_loss: 0.3468\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3368 - val_loss: 0.3417\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3327 - val_loss: 0.3423\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.6047 - val_loss: 0.4166\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4091 - val_loss: 0.3682\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3759 - val_loss: 0.3562\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3620 - val_loss: 0.3525\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3528 - val_loss: 0.3461\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3455 - val_loss: 0.3442\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3398 - val_loss: 0.3417\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3348 - val_loss: 0.3395\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3300 - val_loss: 0.3376\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3254 - val_loss: 0.3388\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5205 - val_loss: 0.4208\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4057 - val_loss: 0.3766\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3763 - val_loss: 0.3622\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3616 - val_loss: 0.3569\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3526 - val_loss: 0.3503\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3453 - val_loss: 0.3452\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3387 - val_loss: 0.3428\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3330 - val_loss: 0.3425\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3280 - val_loss: 0.3391\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3239 - val_loss: 0.3350\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3195 - val_loss: 0.3335\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3162 - val_loss: 0.3337\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 211us/sample - loss: 0.5461 - val_loss: 0.3947\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3979 - val_loss: 0.3571\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3709 - val_loss: 0.3458\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3590 - val_loss: 0.3392\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3506 - val_loss: 0.3358\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3437 - val_loss: 0.3322\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3376 - val_loss: 0.3293\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 61us/sample - loss: 0.3322 - val_loss: 0.3285\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3274 - val_loss: 0.3232\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3231 - val_loss: 0.3222\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3190 - val_loss: 0.3196\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3155 - val_loss: 0.3199\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.4676 - val_loss: 0.3970\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3827 - val_loss: 0.3690\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3607 - val_loss: 0.3537\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3472 - val_loss: 0.3390\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3393 - val_loss: 0.3360\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3321 - val_loss: 0.3307\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3258 - val_loss: 0.3277\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3208 - val_loss: 0.3217\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3169 - val_loss: 0.3183\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3132 - val_loss: 0.3191\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.4941 - val_loss: 0.4051\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3869 - val_loss: 0.3688\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3610 - val_loss: 0.3568\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3485 - val_loss: 0.3500\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3402 - val_loss: 0.3432\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3342 - val_loss: 0.3382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3288 - val_loss: 0.3356\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3248 - val_loss: 0.3313\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3202 - val_loss: 0.3325\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 185us/sample - loss: 0.4953 - val_loss: 0.3817\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3766 - val_loss: 0.3491\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3522 - val_loss: 0.3388\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3416 - val_loss: 0.3333\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3334 - val_loss: 0.3289\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3274 - val_loss: 0.3264\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3227 - val_loss: 0.3239\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3178 - val_loss: 0.3235\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3141 - val_loss: 0.3220\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3101 - val_loss: 0.3212\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3066 - val_loss: 0.3211\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3032 - val_loss: 0.3179\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 63us/sample - loss: 0.3003 - val_loss: 0.3167\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 56us/sample - loss: 0.2979 - val_loss: 0.3173\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 221us/sample - loss: 0.4997 - val_loss: 0.3917\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3832 - val_loss: 0.3608\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3603 - val_loss: 0.3489\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3475 - val_loss: 0.3411\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3388 - val_loss: 0.3367\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3321 - val_loss: 0.3321\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3261 - val_loss: 0.3306\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3204 - val_loss: 0.3242\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3163 - val_loss: 0.3228\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3117 - val_loss: 0.3189\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3073 - val_loss: 0.3188\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3033 - val_loss: 0.3139\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3005 - val_loss: 0.3170\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.5280 - val_loss: 0.4102\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3917 - val_loss: 0.3659\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3639 - val_loss: 0.3525\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3501 - val_loss: 0.3479\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3409 - val_loss: 0.3432\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3340 - val_loss: 0.3417\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3278 - val_loss: 0.3390\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3229 - val_loss: 0.3389\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3176 - val_loss: 0.3416\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5090 - val_loss: 0.3665\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3797 - val_loss: 0.3467\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3591 - val_loss: 0.3413\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3469 - val_loss: 0.3362\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3386 - val_loss: 0.3354\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3304 - val_loss: 0.3366\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5034 - val_loss: 0.3843\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3739 - val_loss: 0.3625\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3550 - val_loss: 0.3545\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3447 - val_loss: 0.3500\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3370 - val_loss: 0.3491\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3310 - val_loss: 0.3449\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3249 - val_loss: 0.3401\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3198 - val_loss: 0.3407\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 216us/sample - loss: 0.4797 - val_loss: 0.4014\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3791 - val_loss: 0.3706\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3579 - val_loss: 0.3587\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3473 - val_loss: 0.3508\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3393 - val_loss: 0.3443\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3328 - val_loss: 0.3409\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3271 - val_loss: 0.3378\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3224 - val_loss: 0.3348\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3175 - val_loss: 0.3312\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3129 - val_loss: 0.3266\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3082 - val_loss: 0.3278\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5333 - val_loss: 0.3903\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3806 - val_loss: 0.3588\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3579 - val_loss: 0.3504\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3462 - val_loss: 0.3437\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3380 - val_loss: 0.3381\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3315 - val_loss: 0.3334\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3252 - val_loss: 0.3266\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3201 - val_loss: 0.3260\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3160 - val_loss: 0.3247\n",
      "Epoch 10/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3116 - val_loss: 0.3184\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3080 - val_loss: 0.3136\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3045 - val_loss: 0.3175\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5183 - val_loss: 0.3822\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3838 - val_loss: 0.3533\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3597 - val_loss: 0.3433\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3464 - val_loss: 0.3359\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3377 - val_loss: 0.3325\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3304 - val_loss: 0.3303\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3239 - val_loss: 0.3273\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3189 - val_loss: 0.3277\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.4693 - val_loss: 0.3708\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3681 - val_loss: 0.3517\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3509 - val_loss: 0.3455\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3410 - val_loss: 0.3403\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3320 - val_loss: 0.3328\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3245 - val_loss: 0.3346\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 216us/sample - loss: 0.5440 - val_loss: 0.4170\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3920 - val_loss: 0.3772\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3621 - val_loss: 0.3611\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3472 - val_loss: 0.3522\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3373 - val_loss: 0.3431\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3288 - val_loss: 0.3375\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3229 - val_loss: 0.3313\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3159 - val_loss: 0.3319\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.4644 - val_loss: 0.3709\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3674 - val_loss: 0.3503\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3480 - val_loss: 0.3393\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3355 - val_loss: 0.3351\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3264 - val_loss: 0.3280\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3192 - val_loss: 0.3254\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3124 - val_loss: 0.3197\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3064 - val_loss: 0.3180\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3013 - val_loss: 0.3141\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2969 - val_loss: 0.3134\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2935 - val_loss: 0.3117\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2896 - val_loss: 0.3151\n",
      "El modelo con mejor desempeño tiene 30 neuronas en la capa oculta\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5065 - val_loss: 0.4706\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4457 - val_loss: 0.4314\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4191 - val_loss: 0.4121\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4035 - val_loss: 0.4003\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3933 - val_loss: 0.3899\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3857 - val_loss: 0.3829\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3797 - val_loss: 0.3772\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3747 - val_loss: 0.3734\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3708 - val_loss: 0.3699\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3670 - val_loss: 0.3671\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3644 - val_loss: 0.3658\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3621 - val_loss: 0.3647\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3605 - val_loss: 0.3635\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3588 - val_loss: 0.3630\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3575 - val_loss: 0.3619\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3565 - val_loss: 0.3604\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3556 - val_loss: 0.3603\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3547 - val_loss: 0.3591\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3542 - val_loss: 0.3601\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.7104 - val_loss: 0.5873\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.5704 - val_loss: 0.5196\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.5220 - val_loss: 0.4883\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4937 - val_loss: 0.4678\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4731 - val_loss: 0.4529\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4578 - val_loss: 0.4405\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4452 - val_loss: 0.4304\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4349 - val_loss: 0.4211\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4256 - val_loss: 0.4130\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4183 - val_loss: 0.4063\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4114 - val_loss: 0.4018\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4053 - val_loss: 0.3971\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4000 - val_loss: 0.3918\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3954 - val_loss: 0.3865\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3910 - val_loss: 0.3821\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3875 - val_loss: 0.3782\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3840 - val_loss: 0.3748\n",
      "Epoch 18/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3806 - val_loss: 0.3735\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3778 - val_loss: 0.3700\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3754 - val_loss: 0.3678\n",
      "Epoch 21/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3730 - val_loss: 0.3654\n",
      "Epoch 22/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3706 - val_loss: 0.3631\n",
      "Epoch 23/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3694 - val_loss: 0.3619\n",
      "Epoch 24/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3674 - val_loss: 0.3607\n",
      "Epoch 25/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3659 - val_loss: 0.3580\n",
      "Epoch 26/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3646 - val_loss: 0.3579\n",
      "Epoch 27/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3635 - val_loss: 0.3568\n",
      "Epoch 28/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3623 - val_loss: 0.3562\n",
      "Epoch 29/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3609 - val_loss: 0.3550\n",
      "Epoch 30/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3603 - val_loss: 0.3539\n",
      "Epoch 31/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3592 - val_loss: 0.3537\n",
      "Epoch 32/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3586 - val_loss: 0.3528\n",
      "Epoch 33/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3574 - val_loss: 0.3516\n",
      "Epoch 34/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3570 - val_loss: 0.3510\n",
      "Epoch 35/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3563 - val_loss: 0.3505\n",
      "Epoch 36/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3556 - val_loss: 0.3498\n",
      "Epoch 37/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3552 - val_loss: 0.3499\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5998 - val_loss: 0.5215\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4777 - val_loss: 0.4217\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4185 - val_loss: 0.3878\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3936 - val_loss: 0.3733\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3797 - val_loss: 0.3648\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3708 - val_loss: 0.3583\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3647 - val_loss: 0.3530\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3602 - val_loss: 0.3489\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3565 - val_loss: 0.3478\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3539 - val_loss: 0.3443\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3516 - val_loss: 0.3433\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3494 - val_loss: 0.3419\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3475 - val_loss: 0.3403\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3460 - val_loss: 0.3397\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3442 - val_loss: 0.3389\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3429 - val_loss: 0.3390\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 218us/sample - loss: 0.6660 - val_loss: 0.4934\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4624 - val_loss: 0.4017\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4057 - val_loss: 0.3777\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3843 - val_loss: 0.3694\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3726 - val_loss: 0.3643\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3658 - val_loss: 0.3630\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3600 - val_loss: 0.3605\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3553 - val_loss: 0.3586\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 58us/sample - loss: 0.3513 - val_loss: 0.3553\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3477 - val_loss: 0.3541\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3437 - val_loss: 0.3511\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3407 - val_loss: 0.3481\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3376 - val_loss: 0.3461\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3348 - val_loss: 0.3449\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3327 - val_loss: 0.3437\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3303 - val_loss: 0.3428\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3287 - val_loss: 0.3431\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.8035 - val_loss: 0.5481\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.5053 - val_loss: 0.4398\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4317 - val_loss: 0.4071\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4009 - val_loss: 0.3917\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3841 - val_loss: 0.3825\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3725 - val_loss: 0.3758\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3647 - val_loss: 0.3711\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3591 - val_loss: 0.3669\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3540 - val_loss: 0.3655\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3506 - val_loss: 0.3634\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3473 - val_loss: 0.3614\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3445 - val_loss: 0.3591\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3422 - val_loss: 0.3574\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3398 - val_loss: 0.3566\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3381 - val_loss: 0.3550\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3362 - val_loss: 0.3536\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3344 - val_loss: 0.3524\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3328 - val_loss: 0.3517\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3315 - val_loss: 0.3513\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3301 - val_loss: 0.3503\n",
      "Epoch 21/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3290 - val_loss: 0.3491\n",
      "Epoch 22/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3278 - val_loss: 0.3517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.6972 - val_loss: 0.5194\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4649 - val_loss: 0.4242\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4068 - val_loss: 0.3922\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3833 - val_loss: 0.3771\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3710 - val_loss: 0.3681\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3625 - val_loss: 0.3596\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3563 - val_loss: 0.3532\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3509 - val_loss: 0.3487\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3463 - val_loss: 0.3453\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3425 - val_loss: 0.3416\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3393 - val_loss: 0.3391\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3366 - val_loss: 0.3360\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3341 - val_loss: 0.3343\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3317 - val_loss: 0.3318\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3300 - val_loss: 0.3313\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3281 - val_loss: 0.3304\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3266 - val_loss: 0.3283\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3250 - val_loss: 0.3273\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3239 - val_loss: 0.3267\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3221 - val_loss: 0.3269\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5765 - val_loss: 0.4610\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4217 - val_loss: 0.3893\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3797 - val_loss: 0.3634\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3622 - val_loss: 0.3523\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3525 - val_loss: 0.3487\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3456 - val_loss: 0.3432\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3402 - val_loss: 0.3429\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3363 - val_loss: 0.3382\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3329 - val_loss: 0.3362\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3294 - val_loss: 0.3352\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3267 - val_loss: 0.3338\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3235 - val_loss: 0.3314\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3217 - val_loss: 0.3322\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 226us/sample - loss: 0.5224 - val_loss: 0.4164\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4027 - val_loss: 0.3809\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3745 - val_loss: 0.3671\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3606 - val_loss: 0.3586\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3514 - val_loss: 0.3528\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3447 - val_loss: 0.3474\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3395 - val_loss: 0.3418\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3352 - val_loss: 0.3373\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3320 - val_loss: 0.3352\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3285 - val_loss: 0.3338\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3263 - val_loss: 0.3332\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3239 - val_loss: 0.3318\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3216 - val_loss: 0.3286\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3200 - val_loss: 0.3293\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5489 - val_loss: 0.4230\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4035 - val_loss: 0.3690\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3711 - val_loss: 0.3554\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3573 - val_loss: 0.3436\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3475 - val_loss: 0.3370\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3399 - val_loss: 0.3327\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3346 - val_loss: 0.3275\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3294 - val_loss: 0.3287\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5551 - val_loss: 0.4195\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4063 - val_loss: 0.3738\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3769 - val_loss: 0.3567\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3628 - val_loss: 0.3494\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3530 - val_loss: 0.3444\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3455 - val_loss: 0.3402\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3398 - val_loss: 0.3355\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3343 - val_loss: 0.3350\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3293 - val_loss: 0.3314\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3256 - val_loss: 0.3296\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3220 - val_loss: 0.3292\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3189 - val_loss: 0.3276\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3161 - val_loss: 0.3267\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3135 - val_loss: 0.3256\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3110 - val_loss: 0.3257\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5355 - val_loss: 0.4263\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4019 - val_loss: 0.3763\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3706 - val_loss: 0.3634\n",
      "Epoch 4/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3582 - val_loss: 0.3563\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3502 - val_loss: 0.3506\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3443 - val_loss: 0.3470\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3393 - val_loss: 0.3443\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3352 - val_loss: 0.3431\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3313 - val_loss: 0.3395\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3281 - val_loss: 0.3387\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3249 - val_loss: 0.3362\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3221 - val_loss: 0.3354\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3188 - val_loss: 0.3350\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3163 - val_loss: 0.3356\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5845 - val_loss: 0.4443\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4097 - val_loss: 0.3936\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3747 - val_loss: 0.3760\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3591 - val_loss: 0.3625\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3498 - val_loss: 0.3572\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3427 - val_loss: 0.3517\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3370 - val_loss: 0.3479\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3332 - val_loss: 0.3452\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3284 - val_loss: 0.3418\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3256 - val_loss: 0.3386\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3220 - val_loss: 0.3366\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3193 - val_loss: 0.3340\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3167 - val_loss: 0.3342\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 226us/sample - loss: 0.5290 - val_loss: 0.4354\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4087 - val_loss: 0.3848\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3796 - val_loss: 0.3673\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3646 - val_loss: 0.3539\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3545 - val_loss: 0.3479\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3469 - val_loss: 0.3433\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3408 - val_loss: 0.3366\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3355 - val_loss: 0.3340\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3304 - val_loss: 0.3311\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3261 - val_loss: 0.3284\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3223 - val_loss: 0.3257\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3185 - val_loss: 0.3249\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3155 - val_loss: 0.3267\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.6701 - val_loss: 0.4524\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4200 - val_loss: 0.3872\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3723 - val_loss: 0.3679\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3548 - val_loss: 0.3612\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3458 - val_loss: 0.3577\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3393 - val_loss: 0.3542\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3338 - val_loss: 0.3509\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3294 - val_loss: 0.3476\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3252 - val_loss: 0.3453\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3210 - val_loss: 0.3430\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3175 - val_loss: 0.3428\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3143 - val_loss: 0.3409\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3110 - val_loss: 0.3378\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3078 - val_loss: 0.3348\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3054 - val_loss: 0.3359\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5861 - val_loss: 0.4317\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4094 - val_loss: 0.3795\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3777 - val_loss: 0.3648\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3627 - val_loss: 0.3524\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3522 - val_loss: 0.3446\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3442 - val_loss: 0.3331\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3367 - val_loss: 0.3259\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3305 - val_loss: 0.3182\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3252 - val_loss: 0.3168\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3208 - val_loss: 0.3133\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3153 - val_loss: 0.3081\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3123 - val_loss: 0.3081\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5150 - val_loss: 0.4007\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3849 - val_loss: 0.3647\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3626 - val_loss: 0.3549\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3524 - val_loss: 0.3484\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3455 - val_loss: 0.3449\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3398 - val_loss: 0.3408\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3349 - val_loss: 0.3363\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3308 - val_loss: 0.3349\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3269 - val_loss: 0.3335\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3226 - val_loss: 0.3296\n",
      "Epoch 11/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3194 - val_loss: 0.3275\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3156 - val_loss: 0.3303\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5924 - val_loss: 0.4239\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4072 - val_loss: 0.3761\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3715 - val_loss: 0.3624\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3573 - val_loss: 0.3541\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3483 - val_loss: 0.3484\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3420 - val_loss: 0.3438\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3359 - val_loss: 0.3393\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3312 - val_loss: 0.3346\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3262 - val_loss: 0.3322\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3223 - val_loss: 0.3288\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3181 - val_loss: 0.3250\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3143 - val_loss: 0.3211\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3111 - val_loss: 0.3216\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5318 - val_loss: 0.3913\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3916 - val_loss: 0.3626\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3654 - val_loss: 0.3505\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3518 - val_loss: 0.3389\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3417 - val_loss: 0.3336\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3344 - val_loss: 0.3283\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3276 - val_loss: 0.3241\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3221 - val_loss: 0.3194\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3171 - val_loss: 0.3198\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5189 - val_loss: 0.4254\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3923 - val_loss: 0.3931\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3667 - val_loss: 0.3765\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3534 - val_loss: 0.3726\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3443 - val_loss: 0.3651\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3364 - val_loss: 0.3576\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3308 - val_loss: 0.3546\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3253 - val_loss: 0.3489\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3201 - val_loss: 0.3466\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3155 - val_loss: 0.3421\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3114 - val_loss: 0.3420\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3076 - val_loss: 0.3379\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3032 - val_loss: 0.3358\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3002 - val_loss: 0.3342\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2971 - val_loss: 0.3319\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2942 - val_loss: 0.3315\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2920 - val_loss: 0.3326\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.4813 - val_loss: 0.3868\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3881 - val_loss: 0.3563\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3649 - val_loss: 0.3458\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3530 - val_loss: 0.3395\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3443 - val_loss: 0.3354\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3370 - val_loss: 0.3328\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3313 - val_loss: 0.3302\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3265 - val_loss: 0.3280\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3215 - val_loss: 0.3250\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3167 - val_loss: 0.3224\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3131 - val_loss: 0.3181\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3091 - val_loss: 0.3198\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 179us/sample - loss: 0.4801 - val_loss: 0.4019\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 78us/sample - loss: 0.3870 - val_loss: 0.3753\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3635 - val_loss: 0.3583\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3488 - val_loss: 0.3482\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3395 - val_loss: 0.3427\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3313 - val_loss: 0.3347\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3248 - val_loss: 0.3311\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3193 - val_loss: 0.3310\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3146 - val_loss: 0.3259\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3104 - val_loss: 0.3211\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3058 - val_loss: 0.3198\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3028 - val_loss: 0.3169\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.2989 - val_loss: 0.3168\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2962 - val_loss: 0.3148\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2935 - val_loss: 0.3144\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2909 - val_loss: 0.3131\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.2879 - val_loss: 0.3138\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5141 - val_loss: 0.3937\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3844 - val_loss: 0.3611\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3575 - val_loss: 0.3518\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3446 - val_loss: 0.3422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3360 - val_loss: 0.3403\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3293 - val_loss: 0.3352\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3251 - val_loss: 0.3341\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3196 - val_loss: 0.3365\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5178 - val_loss: 0.4040\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3873 - val_loss: 0.3754\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3589 - val_loss: 0.3598\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3448 - val_loss: 0.3529\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3350 - val_loss: 0.3510\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3281 - val_loss: 0.3453\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3215 - val_loss: 0.3394\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3159 - val_loss: 0.3333\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3116 - val_loss: 0.3317\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3066 - val_loss: 0.3312\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3022 - val_loss: 0.3256\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.2984 - val_loss: 0.3238\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.2950 - val_loss: 0.3235\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.2918 - val_loss: 0.3206\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2885 - val_loss: 0.3177\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.2854 - val_loss: 0.3148\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.2827 - val_loss: 0.3163\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 231us/sample - loss: 0.5543 - val_loss: 0.3999\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3848 - val_loss: 0.3633\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3580 - val_loss: 0.3533\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3455 - val_loss: 0.3434\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3375 - val_loss: 0.3386\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3298 - val_loss: 0.3336\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3243 - val_loss: 0.3330\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3184 - val_loss: 0.3290\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3137 - val_loss: 0.3322\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.4973 - val_loss: 0.3853\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3773 - val_loss: 0.3601\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3577 - val_loss: 0.3512\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3460 - val_loss: 0.3440\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3378 - val_loss: 0.3379\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3307 - val_loss: 0.3346\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3245 - val_loss: 0.3313\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3189 - val_loss: 0.3259\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3140 - val_loss: 0.3277\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.4928 - val_loss: 0.3919\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3729 - val_loss: 0.3683\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3533 - val_loss: 0.3581\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3421 - val_loss: 0.3499\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3343 - val_loss: 0.3484\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3274 - val_loss: 0.3432\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3215 - val_loss: 0.3409\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3162 - val_loss: 0.3385\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3115 - val_loss: 0.3416\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.4988 - val_loss: 0.3971\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3793 - val_loss: 0.3749\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3601 - val_loss: 0.3574\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3482 - val_loss: 0.3472\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3388 - val_loss: 0.3417\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3313 - val_loss: 0.3338\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3242 - val_loss: 0.3319\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3178 - val_loss: 0.3287\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3124 - val_loss: 0.3263\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3086 - val_loss: 0.3206\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3032 - val_loss: 0.3141\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3001 - val_loss: 0.3163\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5029 - val_loss: 0.3859\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3795 - val_loss: 0.3602\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3573 - val_loss: 0.3523\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3454 - val_loss: 0.3446\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3362 - val_loss: 0.3413\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3282 - val_loss: 0.3335\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3203 - val_loss: 0.3266\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3147 - val_loss: 0.3280\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 231us/sample - loss: 0.4983 - val_loss: 0.3814\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3741 - val_loss: 0.3558\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3535 - val_loss: 0.3487\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3423 - val_loss: 0.3389\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3327 - val_loss: 0.3394\n",
      "El modelo con mejor desempeño tiene 16 neuronas en la capa oculta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.6854 - val_loss: 0.6022\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.5829 - val_loss: 0.5449\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.5367 - val_loss: 0.5087\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.5046 - val_loss: 0.4831\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4804 - val_loss: 0.4631\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4613 - val_loss: 0.4469\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4463 - val_loss: 0.4329\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4343 - val_loss: 0.4217\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4243 - val_loss: 0.4125\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4161 - val_loss: 0.4040\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4090 - val_loss: 0.3966\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4031 - val_loss: 0.3902\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3979 - val_loss: 0.3862\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3936 - val_loss: 0.3809\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3896 - val_loss: 0.3769\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3861 - val_loss: 0.3725\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3832 - val_loss: 0.3712\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3807 - val_loss: 0.3687\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3786 - val_loss: 0.3673\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3766 - val_loss: 0.3651\n",
      "Epoch 21/2000\n",
      "6012/6012 [==============================] - 0s 83us/sample - loss: 0.3747 - val_loss: 0.3634\n",
      "Epoch 22/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3732 - val_loss: 0.3633\n",
      "Epoch 23/2000\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3719 - val_loss: 0.3611\n",
      "Epoch 24/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3712 - val_loss: 0.3594\n",
      "Epoch 25/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3698 - val_loss: 0.3590\n",
      "Epoch 26/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3686 - val_loss: 0.3576\n",
      "Epoch 27/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3675 - val_loss: 0.3567\n",
      "Epoch 28/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3667 - val_loss: 0.3556\n",
      "Epoch 29/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3660 - val_loss: 0.3545\n",
      "Epoch 30/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3654 - val_loss: 0.3537\n",
      "Epoch 31/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3642 - val_loss: 0.3545\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 169us/sample - loss: 0.5810 - val_loss: 0.4849\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4794 - val_loss: 0.4184\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4321 - val_loss: 0.3892\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4078 - val_loss: 0.3742\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3944 - val_loss: 0.3656\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3857 - val_loss: 0.3617\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3793 - val_loss: 0.3579\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3744 - val_loss: 0.3559\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3701 - val_loss: 0.3538\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3671 - val_loss: 0.3527\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3643 - val_loss: 0.3508\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3617 - val_loss: 0.3495\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3592 - val_loss: 0.3482\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3572 - val_loss: 0.3478\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3552 - val_loss: 0.3466\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3531 - val_loss: 0.3462\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 63us/sample - loss: 0.3516 - val_loss: 0.3456\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3501 - val_loss: 0.3458\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 169us/sample - loss: 0.6815 - val_loss: 0.5636\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4944 - val_loss: 0.4412\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4219 - val_loss: 0.3912\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3910 - val_loss: 0.3699\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3765 - val_loss: 0.3596\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3686 - val_loss: 0.3548\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3635 - val_loss: 0.3511\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3599 - val_loss: 0.3481\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3573 - val_loss: 0.3470\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3552 - val_loss: 0.3453\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3535 - val_loss: 0.3432\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3519 - val_loss: 0.3432\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.5863 - val_loss: 0.4700\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4329 - val_loss: 0.3903\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3927 - val_loss: 0.3704\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3775 - val_loss: 0.3608\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3686 - val_loss: 0.3535\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3625 - val_loss: 0.3505\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3573 - val_loss: 0.3482\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3530 - val_loss: 0.3459\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3494 - val_loss: 0.3435\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3463 - val_loss: 0.3447\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.7027 - val_loss: 0.5368\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4759 - val_loss: 0.4322\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4118 - val_loss: 0.3929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3835 - val_loss: 0.3748\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3681 - val_loss: 0.3630\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3583 - val_loss: 0.3544\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3515 - val_loss: 0.3480\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3458 - val_loss: 0.3439\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3410 - val_loss: 0.3407\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3369 - val_loss: 0.3371\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3328 - val_loss: 0.3359\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3291 - val_loss: 0.3338\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3263 - val_loss: 0.3326\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3238 - val_loss: 0.3301\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3217 - val_loss: 0.3305\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 236us/sample - loss: 0.6160 - val_loss: 0.4834\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4592 - val_loss: 0.4166\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4128 - val_loss: 0.3922\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3932 - val_loss: 0.3841\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3827 - val_loss: 0.3793\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3757 - val_loss: 0.3764\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3705 - val_loss: 0.3729\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3660 - val_loss: 0.3708\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3621 - val_loss: 0.3684\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3591 - val_loss: 0.3670\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3557 - val_loss: 0.3663\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3528 - val_loss: 0.3637\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3493 - val_loss: 0.3624\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3464 - val_loss: 0.3611\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3437 - val_loss: 0.3590\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3411 - val_loss: 0.3585\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3390 - val_loss: 0.3573\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3371 - val_loss: 0.3565\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3354 - val_loss: 0.3559\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3337 - val_loss: 0.3556\n",
      "Epoch 21/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3322 - val_loss: 0.3563\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.6604 - val_loss: 0.5688\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.5115 - val_loss: 0.4709\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4247 - val_loss: 0.4079\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3865 - val_loss: 0.3853\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3705 - val_loss: 0.3740\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3609 - val_loss: 0.3657\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3545 - val_loss: 0.3630\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3500 - val_loss: 0.3569\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3463 - val_loss: 0.3541\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3434 - val_loss: 0.3508\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3405 - val_loss: 0.3488\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3383 - val_loss: 0.3485\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3366 - val_loss: 0.3477\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3346 - val_loss: 0.3439\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3333 - val_loss: 0.3432\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3316 - val_loss: 0.3450\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5910 - val_loss: 0.4616\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4309 - val_loss: 0.3781\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3807 - val_loss: 0.3518\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3631 - val_loss: 0.3422\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3544 - val_loss: 0.3360\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3486 - val_loss: 0.3322\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3441 - val_loss: 0.3293\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3404 - val_loss: 0.3279\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3374 - val_loss: 0.3268\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3343 - val_loss: 0.3232\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3317 - val_loss: 0.3228\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3292 - val_loss: 0.3222\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3271 - val_loss: 0.3227\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5246 - val_loss: 0.4372\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4146 - val_loss: 0.3772\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3730 - val_loss: 0.3546\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3561 - val_loss: 0.3491\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3478 - val_loss: 0.3450\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3421 - val_loss: 0.3401\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3380 - val_loss: 0.3379\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3341 - val_loss: 0.3364\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3308 - val_loss: 0.3332\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3281 - val_loss: 0.3310\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3251 - val_loss: 0.3312\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5642 - val_loss: 0.4261\n",
      "Epoch 2/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4128 - val_loss: 0.3661\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3723 - val_loss: 0.3491\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3558 - val_loss: 0.3428\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3462 - val_loss: 0.3389\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3398 - val_loss: 0.3368\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3348 - val_loss: 0.3341\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3303 - val_loss: 0.3315\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3263 - val_loss: 0.3308\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3230 - val_loss: 0.3270\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3202 - val_loss: 0.3256\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3172 - val_loss: 0.3272\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5264 - val_loss: 0.4534\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4077 - val_loss: 0.4011\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3767 - val_loss: 0.3808\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3620 - val_loss: 0.3700\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3529 - val_loss: 0.3613\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3460 - val_loss: 0.3546\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3402 - val_loss: 0.3520\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3356 - val_loss: 0.3470\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3313 - val_loss: 0.3446\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3278 - val_loss: 0.3408\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3243 - val_loss: 0.3382\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3213 - val_loss: 0.3360\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3185 - val_loss: 0.3352\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3161 - val_loss: 0.3306\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3141 - val_loss: 0.3310\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 242us/sample - loss: 0.6521 - val_loss: 0.4755\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4327 - val_loss: 0.3986\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3876 - val_loss: 0.3779\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3695 - val_loss: 0.3664\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3582 - val_loss: 0.3604\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3504 - val_loss: 0.3551\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3433 - val_loss: 0.3483\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3377 - val_loss: 0.3445\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3321 - val_loss: 0.3400\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3281 - val_loss: 0.3357\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3240 - val_loss: 0.3335\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3205 - val_loss: 0.3315\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3176 - val_loss: 0.3319\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5160 - val_loss: 0.4130\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3915 - val_loss: 0.3768\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3656 - val_loss: 0.3628\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3537 - val_loss: 0.3564\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3455 - val_loss: 0.3502\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3390 - val_loss: 0.3468\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3335 - val_loss: 0.3410\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3287 - val_loss: 0.3397\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3238 - val_loss: 0.3356\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3196 - val_loss: 0.3329\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3158 - val_loss: 0.3334\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5714 - val_loss: 0.4294\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4208 - val_loss: 0.3842\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3850 - val_loss: 0.3699\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3673 - val_loss: 0.3605\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3560 - val_loss: 0.3547\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3476 - val_loss: 0.3500\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3409 - val_loss: 0.3459\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3350 - val_loss: 0.3444\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3301 - val_loss: 0.3423\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3260 - val_loss: 0.3417\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3216 - val_loss: 0.3357\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3179 - val_loss: 0.3337\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3149 - val_loss: 0.3359\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5242 - val_loss: 0.4005\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3958 - val_loss: 0.3607\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3672 - val_loss: 0.3508\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3553 - val_loss: 0.3464\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3475 - val_loss: 0.3431\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3417 - val_loss: 0.3380\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3365 - val_loss: 0.3350\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3326 - val_loss: 0.3309\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3289 - val_loss: 0.3317\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5366 - val_loss: 0.4184\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3849 - val_loss: 0.3791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3607 - val_loss: 0.3656\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3490 - val_loss: 0.3566\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3423 - val_loss: 0.3511\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3362 - val_loss: 0.3471\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3308 - val_loss: 0.3430\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3259 - val_loss: 0.3383\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3217 - val_loss: 0.3406\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.5556 - val_loss: 0.4162\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3966 - val_loss: 0.3711\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3685 - val_loss: 0.3566\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3550 - val_loss: 0.3509\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3468 - val_loss: 0.3456\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3401 - val_loss: 0.3413\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3344 - val_loss: 0.3416\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 242us/sample - loss: 0.5408 - val_loss: 0.4057\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3956 - val_loss: 0.3658\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3667 - val_loss: 0.3572\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3543 - val_loss: 0.3464\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3451 - val_loss: 0.3407\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3387 - val_loss: 0.3365\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3320 - val_loss: 0.3282\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3264 - val_loss: 0.3246\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3215 - val_loss: 0.3200\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3162 - val_loss: 0.3183\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3120 - val_loss: 0.3144\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3081 - val_loss: 0.3137\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3054 - val_loss: 0.3101\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3016 - val_loss: 0.3085\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2992 - val_loss: 0.3069\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.2969 - val_loss: 0.3075\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.4808 - val_loss: 0.3901\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3831 - val_loss: 0.3630\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3601 - val_loss: 0.3466\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3462 - val_loss: 0.3398\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3367 - val_loss: 0.3317\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3298 - val_loss: 0.3276\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3235 - val_loss: 0.3268\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3177 - val_loss: 0.3200\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3134 - val_loss: 0.3197\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3092 - val_loss: 0.3166\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3057 - val_loss: 0.3169\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5325 - val_loss: 0.4003\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3886 - val_loss: 0.3655\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3623 - val_loss: 0.3530\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3494 - val_loss: 0.3479\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3400 - val_loss: 0.3458\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3331 - val_loss: 0.3350\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3273 - val_loss: 0.3323\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3218 - val_loss: 0.3311\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3167 - val_loss: 0.3288\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3121 - val_loss: 0.3275\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3086 - val_loss: 0.3239\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3051 - val_loss: 0.3270\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5224 - val_loss: 0.4071\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3838 - val_loss: 0.3735\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3616 - val_loss: 0.3586\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3488 - val_loss: 0.3488\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3405 - val_loss: 0.3398\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3326 - val_loss: 0.3363\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3258 - val_loss: 0.3297\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3205 - val_loss: 0.3273\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3146 - val_loss: 0.3246\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3103 - val_loss: 0.3202\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3060 - val_loss: 0.3222\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5109 - val_loss: 0.4220\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3937 - val_loss: 0.3771\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3675 - val_loss: 0.3636\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3549 - val_loss: 0.3524\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3456 - val_loss: 0.3461\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3374 - val_loss: 0.3390\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3309 - val_loss: 0.3366\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3252 - val_loss: 0.3337\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3199 - val_loss: 0.3328\n",
      "Epoch 10/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3154 - val_loss: 0.3252\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3114 - val_loss: 0.3240\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3074 - val_loss: 0.3197\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3040 - val_loss: 0.3188\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3010 - val_loss: 0.3176\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2978 - val_loss: 0.3136\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.2949 - val_loss: 0.3139\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.4734 - val_loss: 0.3893\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3747 - val_loss: 0.3663\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3534 - val_loss: 0.3545\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3410 - val_loss: 0.3464\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3319 - val_loss: 0.3402\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3244 - val_loss: 0.3363\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3181 - val_loss: 0.3358\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3125 - val_loss: 0.3297\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3089 - val_loss: 0.3286\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3045 - val_loss: 0.3260\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3009 - val_loss: 0.3225\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.2968 - val_loss: 0.3234\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.4825 - val_loss: 0.3789\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3748 - val_loss: 0.3570\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3551 - val_loss: 0.3452\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3445 - val_loss: 0.3358\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3362 - val_loss: 0.3339\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3294 - val_loss: 0.3286\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3238 - val_loss: 0.3250\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3183 - val_loss: 0.3220\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3131 - val_loss: 0.3224\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 2s 252us/sample - loss: 0.4833 - val_loss: 0.3943\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3812 - val_loss: 0.3648\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3580 - val_loss: 0.3539\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3457 - val_loss: 0.3467\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3371 - val_loss: 0.3410\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3293 - val_loss: 0.3373\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3222 - val_loss: 0.3313\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3168 - val_loss: 0.3300\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3113 - val_loss: 0.3284\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3078 - val_loss: 0.3256\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3026 - val_loss: 0.3256\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2987 - val_loss: 0.3251\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2958 - val_loss: 0.3277\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5370 - val_loss: 0.4122\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3844 - val_loss: 0.3746\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3587 - val_loss: 0.3599\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3466 - val_loss: 0.3543\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3380 - val_loss: 0.3423\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3322 - val_loss: 0.3376\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3256 - val_loss: 0.3347\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3204 - val_loss: 0.3277\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3154 - val_loss: 0.3251\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3110 - val_loss: 0.3221\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3062 - val_loss: 0.3187\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3031 - val_loss: 0.3159\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2990 - val_loss: 0.3146\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.2957 - val_loss: 0.3160\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.4849 - val_loss: 0.3761\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3694 - val_loss: 0.3528\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3484 - val_loss: 0.3426\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3370 - val_loss: 0.3403\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3296 - val_loss: 0.3381\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3224 - val_loss: 0.3280\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3164 - val_loss: 0.3260\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3115 - val_loss: 0.3236\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3067 - val_loss: 0.3205\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3031 - val_loss: 0.3178\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2995 - val_loss: 0.3189\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.5077 - val_loss: 0.3921\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3753 - val_loss: 0.3626\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3539 - val_loss: 0.3518\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3413 - val_loss: 0.3509\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3336 - val_loss: 0.3396\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3254 - val_loss: 0.3354\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3191 - val_loss: 0.3370\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.4969 - val_loss: 0.3796\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3773 - val_loss: 0.3588\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3566 - val_loss: 0.3479\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3447 - val_loss: 0.3444\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3355 - val_loss: 0.3368\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 63us/sample - loss: 0.3284 - val_loss: 0.3366\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3228 - val_loss: 0.3332\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3168 - val_loss: 0.3320\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3107 - val_loss: 0.3299\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3064 - val_loss: 0.3254\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3018 - val_loss: 0.3295\n",
      "El modelo con mejor desempeño tiene 19 neuronas en la capa oculta\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.7037 - val_loss: 0.5803\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.5300 - val_loss: 0.4696\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.4551 - val_loss: 0.4199\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4200 - val_loss: 0.3950\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4020 - val_loss: 0.3827\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3910 - val_loss: 0.3735\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3837 - val_loss: 0.3670\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3781 - val_loss: 0.3623\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3737 - val_loss: 0.3591\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3701 - val_loss: 0.3572\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3674 - val_loss: 0.3568\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3651 - val_loss: 0.3559\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3631 - val_loss: 0.3568\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5838 - val_loss: 0.5017\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4693 - val_loss: 0.4164\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4150 - val_loss: 0.3887\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3923 - val_loss: 0.3783\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3804 - val_loss: 0.3723\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3736 - val_loss: 0.3694\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3686 - val_loss: 0.3675\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3648 - val_loss: 0.3652\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3620 - val_loss: 0.3636\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3598 - val_loss: 0.3618\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3574 - val_loss: 0.3605\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3559 - val_loss: 0.3606\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 2s 252us/sample - loss: 0.5870 - val_loss: 0.4958\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4648 - val_loss: 0.4231\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4159 - val_loss: 0.3935\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3943 - val_loss: 0.3790\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3818 - val_loss: 0.3697\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3733 - val_loss: 0.3635\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3672 - val_loss: 0.3573\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3622 - val_loss: 0.3509\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3583 - val_loss: 0.3472\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3552 - val_loss: 0.3457\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3525 - val_loss: 0.3436\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3507 - val_loss: 0.3430\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3489 - val_loss: 0.3413\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3473 - val_loss: 0.3407\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3462 - val_loss: 0.3411\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.6193 - val_loss: 0.5288\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4755 - val_loss: 0.4145\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4065 - val_loss: 0.3772\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3821 - val_loss: 0.3650\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3712 - val_loss: 0.3582\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3643 - val_loss: 0.3549\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3594 - val_loss: 0.3516\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3553 - val_loss: 0.3503\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3518 - val_loss: 0.3494\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3487 - val_loss: 0.3481\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3460 - val_loss: 0.3475\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3437 - val_loss: 0.3466\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3414 - val_loss: 0.3450\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3396 - val_loss: 0.3451\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5009 - val_loss: 0.4215\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4143 - val_loss: 0.3833\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3859 - val_loss: 0.3719\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3718 - val_loss: 0.3651\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3632 - val_loss: 0.3610\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3565 - val_loss: 0.3562\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3520 - val_loss: 0.3538\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3476 - val_loss: 0.3506\n",
      "Epoch 9/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3440 - val_loss: 0.3479\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3409 - val_loss: 0.3462\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3383 - val_loss: 0.3429\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3353 - val_loss: 0.3404\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3327 - val_loss: 0.3398\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3299 - val_loss: 0.3404\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.6091 - val_loss: 0.4791\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4405 - val_loss: 0.3951\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3935 - val_loss: 0.3749\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3777 - val_loss: 0.3685\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3690 - val_loss: 0.3640\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3630 - val_loss: 0.3607\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3587 - val_loss: 0.3588\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3547 - val_loss: 0.3569\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3509 - val_loss: 0.3546\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3472 - val_loss: 0.3525\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3436 - val_loss: 0.3503\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3403 - val_loss: 0.3485\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3370 - val_loss: 0.3455\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3337 - val_loss: 0.3428\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3308 - val_loss: 0.3408\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3281 - val_loss: 0.3403\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3252 - val_loss: 0.3399\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3230 - val_loss: 0.3376\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3210 - val_loss: 0.3376\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.6015 - val_loss: 0.4471\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4335 - val_loss: 0.3925\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3956 - val_loss: 0.3732\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3764 - val_loss: 0.3614\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3648 - val_loss: 0.3535\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3565 - val_loss: 0.3464\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3498 - val_loss: 0.3426\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3449 - val_loss: 0.3408\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3407 - val_loss: 0.3379\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3369 - val_loss: 0.3336\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3341 - val_loss: 0.3341\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5693 - val_loss: 0.4573\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4316 - val_loss: 0.3924\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3899 - val_loss: 0.3700\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3707 - val_loss: 0.3596\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3599 - val_loss: 0.3526\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3530 - val_loss: 0.3472\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3475 - val_loss: 0.3437\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3425 - val_loss: 0.3405\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3382 - val_loss: 0.3396\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3341 - val_loss: 0.3349\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3309 - val_loss: 0.3355\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5431 - val_loss: 0.4438\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 58us/sample - loss: 0.4178 - val_loss: 0.3888\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3867 - val_loss: 0.3684\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3721 - val_loss: 0.3580\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3631 - val_loss: 0.3526\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3569 - val_loss: 0.3482\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3515 - val_loss: 0.3421\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3472 - val_loss: 0.3386\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3436 - val_loss: 0.3373\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3402 - val_loss: 0.3340\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3372 - val_loss: 0.3307\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3343 - val_loss: 0.3271\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3320 - val_loss: 0.3273\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 2s 262us/sample - loss: 0.5321 - val_loss: 0.4291\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4181 - val_loss: 0.3831\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3867 - val_loss: 0.3664\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3715 - val_loss: 0.3556\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3608 - val_loss: 0.3494\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3532 - val_loss: 0.3452\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3470 - val_loss: 0.3386\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3414 - val_loss: 0.3364\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3370 - val_loss: 0.3311\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3327 - val_loss: 0.3289\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3287 - val_loss: 0.3277\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3251 - val_loss: 0.3255\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3217 - val_loss: 0.3224\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3185 - val_loss: 0.3231\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 1s 174us/sample - loss: 0.5114 - val_loss: 0.4034\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3999 - val_loss: 0.3710\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3752 - val_loss: 0.3617\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3631 - val_loss: 0.3570\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3548 - val_loss: 0.3509\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3473 - val_loss: 0.3448\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3412 - val_loss: 0.3425\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3362 - val_loss: 0.3384\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3315 - val_loss: 0.3345\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3274 - val_loss: 0.3326\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3236 - val_loss: 0.3297\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3204 - val_loss: 0.3269\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3172 - val_loss: 0.3244\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3150 - val_loss: 0.3215\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3121 - val_loss: 0.3199\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3099 - val_loss: 0.3182\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3078 - val_loss: 0.3178\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3058 - val_loss: 0.3172\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3036 - val_loss: 0.3160\n",
      "Epoch 20/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3021 - val_loss: 0.3193\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 177us/sample - loss: 0.5210 - val_loss: 0.4242\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4041 - val_loss: 0.3788\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3734 - val_loss: 0.3633\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3606 - val_loss: 0.3532\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3522 - val_loss: 0.3473\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3462 - val_loss: 0.3440\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3413 - val_loss: 0.3405\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3370 - val_loss: 0.3376\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3325 - val_loss: 0.3350\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3291 - val_loss: 0.3312\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3257 - val_loss: 0.3304\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3222 - val_loss: 0.3274\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3192 - val_loss: 0.3243\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3164 - val_loss: 0.3203\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3135 - val_loss: 0.3181\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3110 - val_loss: 0.3174\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3090 - val_loss: 0.3152\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3065 - val_loss: 0.3138\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3041 - val_loss: 0.3143\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5411 - val_loss: 0.4223\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4072 - val_loss: 0.3806\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3811 - val_loss: 0.3629\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3670 - val_loss: 0.3520\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3566 - val_loss: 0.3450\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3498 - val_loss: 0.3396\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3438 - val_loss: 0.3334\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3384 - val_loss: 0.3306\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3341 - val_loss: 0.3263\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3293 - val_loss: 0.3235\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3256 - val_loss: 0.3232\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3219 - val_loss: 0.3208\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3183 - val_loss: 0.3208\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5783 - val_loss: 0.4453\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4038 - val_loss: 0.3878\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3678 - val_loss: 0.3711\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3533 - val_loss: 0.3630\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3439 - val_loss: 0.3572\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3367 - val_loss: 0.3535\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3312 - val_loss: 0.3481\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3262 - val_loss: 0.3471\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3223 - val_loss: 0.3455\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 78us/sample - loss: 0.3183 - val_loss: 0.3405\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3152 - val_loss: 0.3385\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3116 - val_loss: 0.3403\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 192us/sample - loss: 0.5722 - val_loss: 0.4457\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4174 - val_loss: 0.3929\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3838 - val_loss: 0.3781\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3680 - val_loss: 0.3688\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3574 - val_loss: 0.3648\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3495 - val_loss: 0.3566\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3431 - val_loss: 0.3542\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3377 - val_loss: 0.3530\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3325 - val_loss: 0.3497\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3283 - val_loss: 0.3483\n",
      "Epoch 11/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3246 - val_loss: 0.3458\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3208 - val_loss: 0.3420\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3178 - val_loss: 0.3402\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3149 - val_loss: 0.3419\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5622 - val_loss: 0.4233\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4087 - val_loss: 0.3854\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3796 - val_loss: 0.3706\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3639 - val_loss: 0.3605\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3531 - val_loss: 0.3530\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3451 - val_loss: 0.3449\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3382 - val_loss: 0.3415\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3328 - val_loss: 0.3352\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3282 - val_loss: 0.3361\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5775 - val_loss: 0.4158\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4049 - val_loss: 0.3656\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3686 - val_loss: 0.3506\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3542 - val_loss: 0.3445\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3442 - val_loss: 0.3395\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3373 - val_loss: 0.3346\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3305 - val_loss: 0.3332\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3253 - val_loss: 0.3286\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3200 - val_loss: 0.3296\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 2s 270us/sample - loss: 0.4699 - val_loss: 0.3936\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3781 - val_loss: 0.3708\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3589 - val_loss: 0.3555\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3475 - val_loss: 0.3466\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3390 - val_loss: 0.3430\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3329 - val_loss: 0.3331\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3268 - val_loss: 0.3295\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3221 - val_loss: 0.3262\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3171 - val_loss: 0.3203\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3130 - val_loss: 0.3222\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.5784 - val_loss: 0.4300\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3917 - val_loss: 0.3815\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3613 - val_loss: 0.3641\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3470 - val_loss: 0.3551\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3380 - val_loss: 0.3469\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3305 - val_loss: 0.3438\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3247 - val_loss: 0.3406\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3201 - val_loss: 0.3342\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3152 - val_loss: 0.3303\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3116 - val_loss: 0.3328\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.4605 - val_loss: 0.3820\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3741 - val_loss: 0.3620\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3547 - val_loss: 0.3531\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3436 - val_loss: 0.3461\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3360 - val_loss: 0.3408\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3300 - val_loss: 0.3380\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3242 - val_loss: 0.3326\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3192 - val_loss: 0.3321\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3149 - val_loss: 0.3274\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3108 - val_loss: 0.3269\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3073 - val_loss: 0.3240\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3037 - val_loss: 0.3235\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3004 - val_loss: 0.3218\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.2970 - val_loss: 0.3249\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.5404 - val_loss: 0.4025\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3833 - val_loss: 0.3666\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3567 - val_loss: 0.3520\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3430 - val_loss: 0.3464\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3338 - val_loss: 0.3394\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3265 - val_loss: 0.3315\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3206 - val_loss: 0.3299\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3149 - val_loss: 0.3269\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3101 - val_loss: 0.3248\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3059 - val_loss: 0.3217\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3021 - val_loss: 0.3217\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.2983 - val_loss: 0.3248\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.5467 - val_loss: 0.3993\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3888 - val_loss: 0.3589\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3611 - val_loss: 0.3470\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3486 - val_loss: 0.3368\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3400 - val_loss: 0.3353\n",
      "Epoch 6/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3327 - val_loss: 0.3316\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3266 - val_loss: 0.3272\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3211 - val_loss: 0.3244\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3160 - val_loss: 0.3213\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3121 - val_loss: 0.3179\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3078 - val_loss: 0.3175\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3042 - val_loss: 0.3195\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.4708 - val_loss: 0.3888\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3754 - val_loss: 0.3683\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3574 - val_loss: 0.3564\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3471 - val_loss: 0.3511\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3402 - val_loss: 0.3470\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3342 - val_loss: 0.3431\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3285 - val_loss: 0.3390\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3234 - val_loss: 0.3353\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3191 - val_loss: 0.3313\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3147 - val_loss: 0.3292\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3100 - val_loss: 0.3256\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3074 - val_loss: 0.3249\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3034 - val_loss: 0.3235\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3016 - val_loss: 0.3191\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.2985 - val_loss: 0.3218\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 169us/sample - loss: 0.4766 - val_loss: 0.3842\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3753 - val_loss: 0.3567\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3534 - val_loss: 0.3449\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3426 - val_loss: 0.3382\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3337 - val_loss: 0.3334\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3265 - val_loss: 0.3313\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3192 - val_loss: 0.3331\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 198us/sample - loss: 0.4923 - val_loss: 0.3837\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3755 - val_loss: 0.3517\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3511 - val_loss: 0.3423\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3383 - val_loss: 0.3331\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3280 - val_loss: 0.3266\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3208 - val_loss: 0.3238\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3146 - val_loss: 0.3235\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3088 - val_loss: 0.3192\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3040 - val_loss: 0.3180\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.2998 - val_loss: 0.3136\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.2969 - val_loss: 0.3138\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 2s 286us/sample - loss: 0.4746 - val_loss: 0.3972\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3708 - val_loss: 0.3675\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3510 - val_loss: 0.3571\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3403 - val_loss: 0.3507\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3322 - val_loss: 0.3403\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3260 - val_loss: 0.3387\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 78us/sample - loss: 0.3197 - val_loss: 0.3395\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 179us/sample - loss: 0.5126 - val_loss: 0.3809\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3768 - val_loss: 0.3516\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3554 - val_loss: 0.3404\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3436 - val_loss: 0.3335\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3354 - val_loss: 0.3289\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3285 - val_loss: 0.3263\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3220 - val_loss: 0.3236\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3176 - val_loss: 0.3187\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3120 - val_loss: 0.3187\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3075 - val_loss: 0.3139\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3037 - val_loss: 0.3167\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.4943 - val_loss: 0.3956\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3790 - val_loss: 0.3633\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3551 - val_loss: 0.3505\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3423 - val_loss: 0.3427\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3336 - val_loss: 0.3361\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3268 - val_loss: 0.3285\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3206 - val_loss: 0.3320\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5090 - val_loss: 0.3971\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3761 - val_loss: 0.3655\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3559 - val_loss: 0.3549\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3454 - val_loss: 0.3464\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3370 - val_loss: 0.3449\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3296 - val_loss: 0.3372\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3236 - val_loss: 0.3341\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3181 - val_loss: 0.3319\n",
      "Epoch 9/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3136 - val_loss: 0.3294\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3089 - val_loss: 0.3261\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3049 - val_loss: 0.3203\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3007 - val_loss: 0.3182\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2971 - val_loss: 0.3216\n",
      "El modelo con mejor desempeño tiene 26 neuronas en la capa oculta\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "dimensions = X.shape[1] + 1\n",
    "earlyStop = EarlyStopping(min_delta=1e-9)\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    models_log_loss_valid = []\n",
    "    \n",
    "    for neurons in range(2,dimensions):\n",
    "        \n",
    "        model = getMoldel(neurons)    \n",
    "        history_data = model.fit(X_train, y_train,\n",
    "                                 validation_data=(X_validation,y_validation),\n",
    "                                 epochs=2000,\n",
    "                                 batch_size=32,\n",
    "                                 verbose=1,\n",
    "                                 callbacks=[earlyStop])\n",
    "\n",
    "        models_log_loss_valid.append(history_data.history['val_loss'][-1])\n",
    "\n",
    "    best_model_position = models_log_loss_valid.index(min(models_log_loss_valid))\n",
    "    best_model_neurons = best_model_position + 2\n",
    "    print(\"El modelo con mejor desempeño tiene %d neuronas en la capa oculta\" % best_model_neurons)\n",
    "\n",
    "    best_models_loss.append(min(models_log_loss_valid))\n",
    "    best_models_neurons.append(best_model_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El proceso de entrenamiento se repitió 5 veces y se obtuvieron los resultados mostrados a continuacón. Para cada repetición se eligió el modelo con mejor rendimiento y este es el que se muestra en la gráfica junto con su respectiva pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "\n",
    "def plot_model_info(x,y,x_label):\n",
    "    \n",
    "    pl.plot(x, y,\n",
    "            linewidth=2,\n",
    "            label='Valor del error')\n",
    "\n",
    "    pl.plot(x,[min(y)]*len(x),\n",
    "            linestyle = '--',\n",
    "            linewidth = 1,\n",
    "            color = 'red',\n",
    "            label = 'Mínimo valor del error')\n",
    "\n",
    "    pl.xlabel(x_label)\n",
    "    pl.ylabel('Función de error')\n",
    "    pl.title('Comportamiento de la función de error')\n",
    "    pl.legend()\n",
    "    pl.show()\n",
    "    \n",
    "plot_model_info(best_models_neurons,best_models_loss,'Número de neuronas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A partir de los datos de la gráfica es posible afirmar que:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La arquitectura con mejor desempeño tiene 19 neuronas en la capa escondida\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_neurons = best_models_neurons[best_models_loss.index(min(best_models_loss))]\n",
    "print('La arquitectura con mejor desempeño tiene %d neuronas en la capa escondida' % (hidden_layer_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una vez obtenida la arquitectura con mejor rendimiento, se busca la función de activación que permite obtener mejores rendimientos. Este procedimiento también es repetido 5 veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 169us/sample - loss: 0.5719 - val_loss: 0.4356\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3961 - val_loss: 0.3828\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3685 - val_loss: 0.3674\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3568 - val_loss: 0.3628\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3484 - val_loss: 0.3541\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3420 - val_loss: 0.3493\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3362 - val_loss: 0.3471\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3319 - val_loss: 0.3421\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3267 - val_loss: 0.3405\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3225 - val_loss: 0.3390\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3191 - val_loss: 0.3373\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3151 - val_loss: 0.3347\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3116 - val_loss: 0.3368\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 174us/sample - loss: 0.6284 - val_loss: 0.3898\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3845 - val_loss: 0.3691\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3756 - val_loss: 0.3672\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3732 - val_loss: 0.3654\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3719 - val_loss: 0.3664\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 185us/sample - loss: 0.5658 - val_loss: 0.4542\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4278 - val_loss: 0.3899\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3899 - val_loss: 0.3751\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3768 - val_loss: 0.3652\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3711 - val_loss: 0.3659\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 179us/sample - loss: 0.4708 - val_loss: 0.3815\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3845 - val_loss: 0.3680\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3717 - val_loss: 0.3603\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3653 - val_loss: 0.3555\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3605 - val_loss: 0.3529\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3564 - val_loss: 0.3478\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3528 - val_loss: 0.3442\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3495 - val_loss: 0.3442\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 2s 340us/sample - loss: 0.5509 - val_loss: 0.3965\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3963 - val_loss: 0.3729\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3756 - val_loss: 0.3621\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3656 - val_loss: 0.3653\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 210us/sample - loss: 0.7918 - val_loss: 0.4292\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.4117 - val_loss: 0.3785\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3801 - val_loss: 0.3653\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3682 - val_loss: 0.3581\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3611 - val_loss: 0.3523\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3558 - val_loss: 0.3524\n",
      "El modelo con mejor rendimiento usó relu como función de activación\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 182us/sample - loss: 0.5733 - val_loss: 0.4177\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3929 - val_loss: 0.3744\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3631 - val_loss: 0.3611\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3496 - val_loss: 0.3544\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3406 - val_loss: 0.3478\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3338 - val_loss: 0.3426\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3277 - val_loss: 0.3393\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3226 - val_loss: 0.3367\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3183 - val_loss: 0.3350\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3142 - val_loss: 0.3305\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3107 - val_loss: 0.3251\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3073 - val_loss: 0.3244\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3039 - val_loss: 0.3241\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3015 - val_loss: 0.3237\n",
      "Epoch 15/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2988 - val_loss: 0.3215\n",
      "Epoch 16/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2976 - val_loss: 0.3175\n",
      "Epoch 17/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2944 - val_loss: 0.3156\n",
      "Epoch 18/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2927 - val_loss: 0.3136\n",
      "Epoch 19/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2912 - val_loss: 0.3165\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.5261 - val_loss: 0.3767\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3855 - val_loss: 0.3695\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3764 - val_loss: 0.3700\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5678 - val_loss: 0.4458\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4247 - val_loss: 0.3862\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3896 - val_loss: 0.3721\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3770 - val_loss: 0.3649\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3711 - val_loss: 0.3628\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3679 - val_loss: 0.3602\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3654 - val_loss: 0.3620\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.4797 - val_loss: 0.3946\n",
      "Epoch 2/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3839 - val_loss: 0.3744\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3704 - val_loss: 0.3649\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3637 - val_loss: 0.3608\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3596 - val_loss: 0.3602\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3553 - val_loss: 0.3570\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3524 - val_loss: 0.3548\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3486 - val_loss: 0.3516\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3462 - val_loss: 0.3525\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 177us/sample - loss: 0.4497 - val_loss: 0.3783\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3726 - val_loss: 0.3707\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3643 - val_loss: 0.3663\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3602 - val_loss: 0.3572\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - ETA: 0s - loss: 0.354 - 0s 60us/sample - loss: 0.3561 - val_loss: 0.3541\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3530 - val_loss: 0.3542\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 182us/sample - loss: 0.5169 - val_loss: 0.3825\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3898 - val_loss: 0.3569\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3678 - val_loss: 0.3540\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3596 - val_loss: 0.3503\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3553 - val_loss: 0.3491\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3500 - val_loss: 0.3467\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3464 - val_loss: 0.3417\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3429 - val_loss: 0.3430\n",
      "El modelo con mejor rendimiento usó relu como función de activación\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5168 - val_loss: 0.4034\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3962 - val_loss: 0.3679\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3693 - val_loss: 0.3532\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3548 - val_loss: 0.3442\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3445 - val_loss: 0.3367\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3368 - val_loss: 0.3300\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3305 - val_loss: 0.3265\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3253 - val_loss: 0.3236\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3211 - val_loss: 0.3242\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.4725 - val_loss: 0.3863\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3793 - val_loss: 0.3769\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3735 - val_loss: 0.3678\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3713 - val_loss: 0.3636\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3705 - val_loss: 0.3664\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5700 - val_loss: 0.4521\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4222 - val_loss: 0.3900\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3882 - val_loss: 0.3747\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3770 - val_loss: 0.3691\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3719 - val_loss: 0.3654\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3689 - val_loss: 0.3627\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3663 - val_loss: 0.3627\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.4834 - val_loss: 0.3815\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3775 - val_loss: 0.3677\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3653 - val_loss: 0.3605\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3601 - val_loss: 0.3559\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3557 - val_loss: 0.3554\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3526 - val_loss: 0.3506\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3492 - val_loss: 0.3510\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 179us/sample - loss: 0.4896 - val_loss: 0.4028\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3880 - val_loss: 0.3833\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3700 - val_loss: 0.3753\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3623 - val_loss: 0.3700\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3579 - val_loss: 0.3686\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3544 - val_loss: 0.3642\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3517 - val_loss: 0.3591\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3489 - val_loss: 0.3601\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.6002 - val_loss: 0.4185\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4023 - val_loss: 0.3869\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3792 - val_loss: 0.3759\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3676 - val_loss: 0.3708\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3606 - val_loss: 0.3621\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3554 - val_loss: 0.3623\n",
      "El modelo con mejor rendimiento usó relu como función de activación\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5665 - val_loss: 0.4308\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4014 - val_loss: 0.3907\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3735 - val_loss: 0.3765\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3595 - val_loss: 0.3662\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3501 - val_loss: 0.3559\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3430 - val_loss: 0.3494\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3366 - val_loss: 0.3470\n",
      "Epoch 8/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3308 - val_loss: 0.3407\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3263 - val_loss: 0.3394\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3208 - val_loss: 0.3383\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3175 - val_loss: 0.3326\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3134 - val_loss: 0.3308\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3098 - val_loss: 0.3289\n",
      "Epoch 14/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3064 - val_loss: 0.3300\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.4620 - val_loss: 0.3910\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 58us/sample - loss: 0.3823 - val_loss: 0.3749\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3756 - val_loss: 0.3672\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3730 - val_loss: 0.3697\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 159us/sample - loss: 0.5398 - val_loss: 0.4418\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4220 - val_loss: 0.3871\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3892 - val_loss: 0.3718\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3775 - val_loss: 0.3656\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3721 - val_loss: 0.3636\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3687 - val_loss: 0.3620\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3661 - val_loss: 0.3631\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.5438 - val_loss: 0.3879\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3879 - val_loss: 0.3672\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3719 - val_loss: 0.3588\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3639 - val_loss: 0.3561\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3593 - val_loss: 0.3531\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3553 - val_loss: 0.3499\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3514 - val_loss: 0.3493\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3478 - val_loss: 0.3488\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3446 - val_loss: 0.3469\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3414 - val_loss: 0.3445\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3386 - val_loss: 0.3434\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3355 - val_loss: 0.3411\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3322 - val_loss: 0.3422\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 179us/sample - loss: 0.4200 - val_loss: 0.3706\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3730 - val_loss: 0.3618\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3618 - val_loss: 0.3568\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3563 - val_loss: 0.3584\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.6264 - val_loss: 0.4486\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4114 - val_loss: 0.3925\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3801 - val_loss: 0.3758\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3668 - val_loss: 0.3659\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3592 - val_loss: 0.3579\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3535 - val_loss: 0.3532\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3490 - val_loss: 0.3507\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3449 - val_loss: 0.3477\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3414 - val_loss: 0.3472\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3385 - val_loss: 0.3448\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3351 - val_loss: 0.3416\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3324 - val_loss: 0.3406\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3294 - val_loss: 0.3410\n",
      "El modelo con mejor rendimiento usó relu como función de activación\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5951 - val_loss: 0.4395\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3980 - val_loss: 0.3788\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3694 - val_loss: 0.3641\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3572 - val_loss: 0.3557\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3487 - val_loss: 0.3477\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3415 - val_loss: 0.3428\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3355 - val_loss: 0.3403\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3305 - val_loss: 0.3363\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3256 - val_loss: 0.3339\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3217 - val_loss: 0.3338\n",
      "Epoch 11/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3178 - val_loss: 0.3318\n",
      "Epoch 12/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3138 - val_loss: 0.3264\n",
      "Epoch 13/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3099 - val_loss: 0.3285\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 161us/sample - loss: 0.4862 - val_loss: 0.3954\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3815 - val_loss: 0.3764\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3744 - val_loss: 0.3691\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3715 - val_loss: 0.3641\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3711 - val_loss: 0.3659\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 164us/sample - loss: 0.5476 - val_loss: 0.4424\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4219 - val_loss: 0.3878\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3889 - val_loss: 0.3746\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3769 - val_loss: 0.3680\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3713 - val_loss: 0.3677\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3681 - val_loss: 0.3628\n",
      "Epoch 7/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3658 - val_loss: 0.3627\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3637 - val_loss: 0.3593\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3621 - val_loss: 0.3592\n",
      "Epoch 10/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3607 - val_loss: 0.3595\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 169us/sample - loss: 0.4876 - val_loss: 0.3792\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3790 - val_loss: 0.3639\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3668 - val_loss: 0.3602\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3615 - val_loss: 0.3563\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3573 - val_loss: 0.3542\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3535 - val_loss: 0.3536\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3507 - val_loss: 0.3512\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3476 - val_loss: 0.3464\n",
      "Epoch 9/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3445 - val_loss: 0.3466\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 177us/sample - loss: 0.5218 - val_loss: 0.3983\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3873 - val_loss: 0.3770\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3732 - val_loss: 0.3694\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3676 - val_loss: 0.3633\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3625 - val_loss: 0.3565\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3588 - val_loss: 0.3544\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3558 - val_loss: 0.3551\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 [==============================] - 1s 166us/sample - loss: 0.5423 - val_loss: 0.4003\n",
      "Epoch 2/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3893 - val_loss: 0.3647\n",
      "Epoch 3/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3692 - val_loss: 0.3532\n",
      "Epoch 4/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3611 - val_loss: 0.3523\n",
      "Epoch 5/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3554 - val_loss: 0.3466\n",
      "Epoch 6/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3518 - val_loss: 0.3453\n",
      "Epoch 7/2000\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3479 - val_loss: 0.3446\n",
      "Epoch 8/2000\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3446 - val_loss: 0.3458\n",
      "El modelo con mejor rendimiento usó relu como función de activación\n"
     ]
    }
   ],
   "source": [
    "activations = ['relu','linear', 'sigmoid', 'tanh', 'selu', 'softplus']\n",
    "neurons = hidden_layer_neurons\n",
    "\n",
    "best_models_activation = []\n",
    "best_models_activations_loss = []\n",
    "\n",
    "for i in range(5): \n",
    "    models_activation_log_loss = []\n",
    "    \n",
    "    for activation in activations:\n",
    "        model = getMoldel(neurons, activation=activation)\n",
    "\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            validation_data=(X_validation,y_validation),\n",
    "                            epochs=2000,\n",
    "                            batch_size=32,\n",
    "                            verbose=1,\n",
    "                            callbacks=[earlyStop])\n",
    "\n",
    "        models_activation_log_loss.append(history.history['val_loss'][-1])\n",
    "\n",
    "    best_activation_model_position = (models_activation_log_loss.index(min(models_activation_log_loss)))\n",
    "    best_activation = activations[best_activation_model_position]\n",
    "    print(\"El modelo con mejor rendimiento usó %s como función de activación\" % best_activation)\n",
    "    \n",
    "    best_models_activation.append(best_activation)\n",
    "    best_models_activations_loss.append(min(models_activation_log_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se grafica la mejor función de activación para cada iteración junto con su respectiva pérdida. Para todos los casos ReLU fue la función con mejor rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durante las pruebas ejecutadas, el modelo con mejor desempeño usó relu como funcion de activacion\n"
     ]
    }
   ],
   "source": [
    "plot_model_info(best_models_activation,best_models_activations_loss,'Funciones de activación')\n",
    "best_activation = best_models_activation[best_models_activations_loss.index(min(best_models_activations_loss))]\n",
    "print('Durante las pruebas ejecutadas, el modelo con mejor desempeño usó %s como funcion de activacion' % best_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De acuerdo con los resultados anteriores, es posible escoger la arquitectura y la función de activación del modelo. A continuación, se procede a probar diferentes algortimos de descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/200\n",
      "6012/6012 [==============================] - 1s 182us/sample - loss: 0.6794 - val_loss: 0.6181\n",
      "Epoch 2/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.6222 - val_loss: 0.5788\n",
      "Epoch 3/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.5927 - val_loss: 0.5534\n",
      "Epoch 4/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.5721 - val_loss: 0.5346\n",
      "Epoch 5/200\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5564 - val_loss: 0.5198\n",
      "Epoch 6/200\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5437 - val_loss: 0.5077\n",
      "Epoch 7/200\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5330 - val_loss: 0.4975\n",
      "Epoch 8/200\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5239 - val_loss: 0.4887\n",
      "Epoch 9/200\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.5160 - val_loss: 0.4810\n",
      "Epoch 10/200\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5090 - val_loss: 0.4742\n",
      "Epoch 11/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.5027 - val_loss: 0.4681\n",
      "Epoch 12/200\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4971 - val_loss: 0.4626\n",
      "Epoch 13/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4920 - val_loss: 0.4575\n",
      "Epoch 14/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4873 - val_loss: 0.4530\n",
      "Epoch 15/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4830 - val_loss: 0.4488\n",
      "Epoch 16/200\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4790 - val_loss: 0.4449\n",
      "Epoch 17/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4754 - val_loss: 0.4414\n",
      "Epoch 18/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4719 - val_loss: 0.4380\n",
      "Epoch 19/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4687 - val_loss: 0.4349\n",
      "Epoch 20/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4657 - val_loss: 0.4320\n",
      "Epoch 21/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4629 - val_loss: 0.4293\n",
      "Epoch 22/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4603 - val_loss: 0.4268\n",
      "Epoch 23/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4577 - val_loss: 0.4244\n",
      "Epoch 24/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4554 - val_loss: 0.4221\n",
      "Epoch 25/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4531 - val_loss: 0.4200\n",
      "Epoch 26/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4510 - val_loss: 0.4180\n",
      "Epoch 27/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4489 - val_loss: 0.4161\n",
      "Epoch 28/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4470 - val_loss: 0.4142\n",
      "Epoch 29/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4451 - val_loss: 0.4125\n",
      "Epoch 30/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4433 - val_loss: 0.4108\n",
      "Epoch 31/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4416 - val_loss: 0.4093\n",
      "Epoch 32/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4400 - val_loss: 0.4077\n",
      "Epoch 33/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4384 - val_loss: 0.4063\n",
      "Epoch 34/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4369 - val_loss: 0.4049\n",
      "Epoch 35/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4355 - val_loss: 0.4036\n",
      "Epoch 36/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4341 - val_loss: 0.4023\n",
      "Epoch 37/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4327 - val_loss: 0.4011\n",
      "Epoch 38/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4314 - val_loss: 0.3999\n",
      "Epoch 39/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4302 - val_loss: 0.3987\n",
      "Epoch 40/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4290 - val_loss: 0.3977\n",
      "Epoch 41/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4278 - val_loss: 0.3966\n",
      "Epoch 42/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4266 - val_loss: 0.3956\n",
      "Epoch 43/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4256 - val_loss: 0.3946\n",
      "Epoch 44/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4245 - val_loss: 0.3937\n",
      "Epoch 45/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4235 - val_loss: 0.3928\n",
      "Epoch 46/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4225 - val_loss: 0.3919\n",
      "Epoch 47/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4215 - val_loss: 0.3911\n",
      "Epoch 48/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4205 - val_loss: 0.3902\n",
      "Epoch 49/200\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.4196 - val_loss: 0.3894\n",
      "Epoch 50/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4187 - val_loss: 0.3887\n",
      "Epoch 51/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4179 - val_loss: 0.3879\n",
      "Epoch 52/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4170 - val_loss: 0.3872\n",
      "Epoch 53/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4162 - val_loss: 0.3865\n",
      "Epoch 54/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4154 - val_loss: 0.3858\n",
      "Epoch 55/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4146 - val_loss: 0.3852\n",
      "Epoch 56/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4139 - val_loss: 0.3845\n",
      "Epoch 57/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4131 - val_loss: 0.3839\n",
      "Epoch 58/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4124 - val_loss: 0.3833\n",
      "Epoch 59/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4117 - val_loss: 0.3827\n",
      "Epoch 60/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4110 - val_loss: 0.3822\n",
      "Epoch 61/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4104 - val_loss: 0.3816\n",
      "Epoch 62/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4097 - val_loss: 0.3811\n",
      "Epoch 63/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4091 - val_loss: 0.3805\n",
      "Epoch 64/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4084 - val_loss: 0.3800\n",
      "Epoch 65/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4078 - val_loss: 0.3795\n",
      "Epoch 66/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4072 - val_loss: 0.3790\n",
      "Epoch 67/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4066 - val_loss: 0.3786\n",
      "Epoch 68/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4061 - val_loss: 0.3781\n",
      "Epoch 69/200\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4055 - val_loss: 0.3777\n",
      "Epoch 70/200\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4050 - val_loss: 0.3772\n",
      "Epoch 71/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4044 - val_loss: 0.3768\n",
      "Epoch 72/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4039 - val_loss: 0.3764\n",
      "Epoch 73/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4034 - val_loss: 0.3760\n",
      "Epoch 74/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4029 - val_loss: 0.3756\n",
      "Epoch 75/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4024 - val_loss: 0.3752\n",
      "Epoch 76/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4019 - val_loss: 0.3748\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4014 - val_loss: 0.3744\n",
      "Epoch 78/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4010 - val_loss: 0.3740\n",
      "Epoch 79/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4005 - val_loss: 0.3737\n",
      "Epoch 80/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.4000 - val_loss: 0.3733\n",
      "Epoch 81/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3996 - val_loss: 0.3730\n",
      "Epoch 82/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3992 - val_loss: 0.3726\n",
      "Epoch 83/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3987 - val_loss: 0.3723\n",
      "Epoch 84/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3983 - val_loss: 0.3720\n",
      "Epoch 85/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3979 - val_loss: 0.3717\n",
      "Epoch 86/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3975 - val_loss: 0.3714\n",
      "Epoch 87/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3971 - val_loss: 0.3710\n",
      "Epoch 88/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3967 - val_loss: 0.3707\n",
      "Epoch 89/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3963 - val_loss: 0.3705\n",
      "Epoch 90/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3960 - val_loss: 0.3702\n",
      "Epoch 91/200\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3956 - val_loss: 0.3699\n",
      "Epoch 92/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3952 - val_loss: 0.3696\n",
      "Epoch 93/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3949 - val_loss: 0.3693\n",
      "Epoch 94/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3945 - val_loss: 0.3691\n",
      "Epoch 95/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3941 - val_loss: 0.3688\n",
      "Epoch 96/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3938 - val_loss: 0.3685\n",
      "Epoch 97/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3935 - val_loss: 0.3683\n",
      "Epoch 98/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3931 - val_loss: 0.3680\n",
      "Epoch 99/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3928 - val_loss: 0.3678\n",
      "Epoch 100/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3925 - val_loss: 0.3675\n",
      "Epoch 101/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3921 - val_loss: 0.3673\n",
      "Epoch 102/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3918 - val_loss: 0.3671\n",
      "Epoch 103/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3915 - val_loss: 0.3668\n",
      "Epoch 104/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3912 - val_loss: 0.3666\n",
      "Epoch 105/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3909 - val_loss: 0.3664\n",
      "Epoch 106/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3906 - val_loss: 0.3661\n",
      "Epoch 107/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3903 - val_loss: 0.3659\n",
      "Epoch 108/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3900 - val_loss: 0.3657\n",
      "Epoch 109/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3897 - val_loss: 0.3655\n",
      "Epoch 110/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3894 - val_loss: 0.3653\n",
      "Epoch 111/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3892 - val_loss: 0.3651\n",
      "Epoch 112/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3889 - val_loss: 0.3649\n",
      "Epoch 113/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3886 - val_loss: 0.3647\n",
      "Epoch 114/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3883 - val_loss: 0.3645\n",
      "Epoch 115/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3881 - val_loss: 0.3643\n",
      "Epoch 116/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3878 - val_loss: 0.3641\n",
      "Epoch 117/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3876 - val_loss: 0.3639\n",
      "Epoch 118/200\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3873 - val_loss: 0.3637\n",
      "Epoch 119/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3870 - val_loss: 0.3636\n",
      "Epoch 120/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3868 - val_loss: 0.3634\n",
      "Epoch 121/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3866 - val_loss: 0.3632\n",
      "Epoch 122/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3863 - val_loss: 0.3630\n",
      "Epoch 123/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3861 - val_loss: 0.3628\n",
      "Epoch 124/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3858 - val_loss: 0.3627\n",
      "Epoch 125/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3856 - val_loss: 0.3625\n",
      "Epoch 126/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3854 - val_loss: 0.3623\n",
      "Epoch 127/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3851 - val_loss: 0.3622\n",
      "Epoch 128/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3849 - val_loss: 0.3620\n",
      "Epoch 129/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3847 - val_loss: 0.3618\n",
      "Epoch 130/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3845 - val_loss: 0.3617\n",
      "Epoch 131/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3843 - val_loss: 0.3615\n",
      "Epoch 132/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3840 - val_loss: 0.3614\n",
      "Epoch 133/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3838 - val_loss: 0.3612\n",
      "Epoch 134/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3836 - val_loss: 0.3611\n",
      "Epoch 135/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3834 - val_loss: 0.3609\n",
      "Epoch 136/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3832 - val_loss: 0.3608\n",
      "Epoch 137/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3830 - val_loss: 0.3606\n",
      "Epoch 138/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3828 - val_loss: 0.3605\n",
      "Epoch 139/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3826 - val_loss: 0.3603\n",
      "Epoch 140/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3824 - val_loss: 0.3602\n",
      "Epoch 141/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3822 - val_loss: 0.3601\n",
      "Epoch 142/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3820 - val_loss: 0.3599\n",
      "Epoch 143/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3818 - val_loss: 0.3598\n",
      "Epoch 144/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3816 - val_loss: 0.3597\n",
      "Epoch 145/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3814 - val_loss: 0.3595\n",
      "Epoch 146/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3812 - val_loss: 0.3594\n",
      "Epoch 147/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3811 - val_loss: 0.3593\n",
      "Epoch 148/200\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3809 - val_loss: 0.3591\n",
      "Epoch 149/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3807 - val_loss: 0.3590\n",
      "Epoch 150/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3805 - val_loss: 0.3589\n",
      "Epoch 151/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3803 - val_loss: 0.3588\n",
      "Epoch 152/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3802 - val_loss: 0.3587\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3800 - val_loss: 0.3585\n",
      "Epoch 154/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3798 - val_loss: 0.3584\n",
      "Epoch 155/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3796 - val_loss: 0.3583\n",
      "Epoch 156/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3795 - val_loss: 0.3582\n",
      "Epoch 157/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3793 - val_loss: 0.3581\n",
      "Epoch 158/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3791 - val_loss: 0.3580\n",
      "Epoch 159/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3790 - val_loss: 0.3578\n",
      "Epoch 160/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3788 - val_loss: 0.3577\n",
      "Epoch 161/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3786 - val_loss: 0.3576\n",
      "Epoch 162/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3785 - val_loss: 0.3575\n",
      "Epoch 163/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3783 - val_loss: 0.3574\n",
      "Epoch 164/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3782 - val_loss: 0.3573\n",
      "Epoch 165/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3780 - val_loss: 0.3572\n",
      "Epoch 166/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3779 - val_loss: 0.3571\n",
      "Epoch 167/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3777 - val_loss: 0.3570\n",
      "Epoch 168/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3775 - val_loss: 0.3569\n",
      "Epoch 169/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3774 - val_loss: 0.3568\n",
      "Epoch 170/200\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3772 - val_loss: 0.3567\n",
      "Epoch 171/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3771 - val_loss: 0.3566\n",
      "Epoch 172/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3769 - val_loss: 0.3565\n",
      "Epoch 173/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3768 - val_loss: 0.3564\n",
      "Epoch 174/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3767 - val_loss: 0.3563\n",
      "Epoch 175/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3765 - val_loss: 0.3562\n",
      "Epoch 176/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3764 - val_loss: 0.3561\n",
      "Epoch 177/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3762 - val_loss: 0.3560\n",
      "Epoch 178/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3761 - val_loss: 0.3559\n",
      "Epoch 179/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3759 - val_loss: 0.3558\n",
      "Epoch 180/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3758 - val_loss: 0.3557\n",
      "Epoch 181/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3757 - val_loss: 0.3556\n",
      "Epoch 182/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3755 - val_loss: 0.3555\n",
      "Epoch 183/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3754 - val_loss: 0.3555\n",
      "Epoch 184/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3753 - val_loss: 0.3554\n",
      "Epoch 185/200\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3751 - val_loss: 0.3553\n",
      "Epoch 186/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3750 - val_loss: 0.3552\n",
      "Epoch 187/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3749 - val_loss: 0.3551\n",
      "Epoch 188/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3747 - val_loss: 0.3550\n",
      "Epoch 189/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3746 - val_loss: 0.3549\n",
      "Epoch 190/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3745 - val_loss: 0.3549\n",
      "Epoch 191/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3743 - val_loss: 0.3548\n",
      "Epoch 192/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3742 - val_loss: 0.3547\n",
      "Epoch 193/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3741 - val_loss: 0.3546\n",
      "Epoch 194/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3740 - val_loss: 0.3545\n",
      "Epoch 195/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3738 - val_loss: 0.3545\n",
      "Epoch 196/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3737 - val_loss: 0.3544\n",
      "Epoch 197/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3736 - val_loss: 0.3543\n",
      "Epoch 198/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3735 - val_loss: 0.3542\n",
      "Epoch 199/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3733 - val_loss: 0.3541\n",
      "Epoch 200/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3732 - val_loss: 0.3541\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/200\n",
      "6012/6012 [==============================] - 2s 296us/sample - loss: 0.5088 - val_loss: 0.4017\n",
      "Epoch 2/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3898 - val_loss: 0.3698\n",
      "Epoch 3/200\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3653 - val_loss: 0.3558\n",
      "Epoch 4/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3521 - val_loss: 0.3500\n",
      "Epoch 5/200\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3436 - val_loss: 0.3410\n",
      "Epoch 6/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3360 - val_loss: 0.3368\n",
      "Epoch 7/200\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3297 - val_loss: 0.3323\n",
      "Epoch 8/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3244 - val_loss: 0.3281\n",
      "Epoch 9/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3197 - val_loss: 0.3256\n",
      "Epoch 10/200\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3151 - val_loss: 0.3218\n",
      "Epoch 11/200\n",
      "6012/6012 [==============================] - 0s 61us/sample - loss: 0.3113 - val_loss: 0.3197\n",
      "Epoch 12/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3076 - val_loss: 0.3178\n",
      "Epoch 13/200\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3046 - val_loss: 0.3175\n",
      "Epoch 14/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3010 - val_loss: 0.3149\n",
      "Epoch 15/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2987 - val_loss: 0.3141\n",
      "Epoch 16/200\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.2956 - val_loss: 0.3142\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/200\n",
      "6012/6012 [==============================] - 1s 182us/sample - loss: 0.5388 - val_loss: 0.4621\n",
      "Epoch 2/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.4435 - val_loss: 0.4203\n",
      "Epoch 3/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.4116 - val_loss: 0.4014\n",
      "Epoch 4/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3941 - val_loss: 0.3900\n",
      "Epoch 5/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3824 - val_loss: 0.3825\n",
      "Epoch 6/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3740 - val_loss: 0.3758\n",
      "Epoch 7/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3673 - val_loss: 0.3708\n",
      "Epoch 8/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3617 - val_loss: 0.3666\n",
      "Epoch 9/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3573 - val_loss: 0.3634\n",
      "Epoch 10/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3532 - val_loss: 0.3599\n",
      "Epoch 11/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3496 - val_loss: 0.3570\n",
      "Epoch 12/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3463 - val_loss: 0.3547\n",
      "Epoch 13/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3433 - val_loss: 0.3516\n",
      "Epoch 14/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3407 - val_loss: 0.3497\n",
      "Epoch 15/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3384 - val_loss: 0.3474\n",
      "Epoch 16/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3363 - val_loss: 0.3457\n",
      "Epoch 17/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3343 - val_loss: 0.3433\n",
      "Epoch 18/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3323 - val_loss: 0.3423\n",
      "Epoch 19/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3305 - val_loss: 0.3408\n",
      "Epoch 20/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3291 - val_loss: 0.3396\n",
      "Epoch 21/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3274 - val_loss: 0.3379\n",
      "Epoch 22/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3260 - val_loss: 0.3370\n",
      "Epoch 23/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3246 - val_loss: 0.3359\n",
      "Epoch 24/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3232 - val_loss: 0.3346\n",
      "Epoch 25/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3220 - val_loss: 0.3342\n",
      "Epoch 26/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3207 - val_loss: 0.3325\n",
      "Epoch 27/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3196 - val_loss: 0.3319\n",
      "Epoch 28/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3183 - val_loss: 0.3315\n",
      "Epoch 29/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3171 - val_loss: 0.3314\n",
      "Epoch 30/200\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3161 - val_loss: 0.3298\n",
      "Epoch 31/200\n",
      "6012/6012 [==============================] - 0s 52us/sample - loss: 0.3150 - val_loss: 0.3286\n",
      "Epoch 32/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3141 - val_loss: 0.3287\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/200\n",
      "6012/6012 [==============================] - 1s 185us/sample - loss: 0.4905 - val_loss: 0.4099\n",
      "Epoch 2/200\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3907 - val_loss: 0.3743\n",
      "Epoch 3/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3663 - val_loss: 0.3551\n",
      "Epoch 4/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3530 - val_loss: 0.3470\n",
      "Epoch 5/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3444 - val_loss: 0.3371\n",
      "Epoch 6/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3372 - val_loss: 0.3344\n",
      "Epoch 7/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3319 - val_loss: 0.3307\n",
      "Epoch 8/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3265 - val_loss: 0.3291\n",
      "Epoch 9/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3229 - val_loss: 0.3236\n",
      "Epoch 10/200\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3190 - val_loss: 0.3210\n",
      "Epoch 11/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3152 - val_loss: 0.3208\n",
      "Epoch 12/200\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3117 - val_loss: 0.3194\n",
      "Epoch 13/200\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3087 - val_loss: 0.3177\n",
      "Epoch 14/200\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3058 - val_loss: 0.3150\n",
      "Epoch 15/200\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3027 - val_loss: 0.3116\n",
      "Epoch 16/200\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3002 - val_loss: 0.3106\n",
      "Epoch 17/200\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.2979 - val_loss: 0.3094\n",
      "Epoch 18/200\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.2956 - val_loss: 0.3123\n"
     ]
    }
   ],
   "source": [
    "algorithms = ['Adagrad', 'Adam', 'Adamax', 'RMSprop']\n",
    "\n",
    "models_algorithms_log_loss = []\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    model = getMoldel(neurons, activation=best_activation, optimizer=algorithm)\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_validation,y_validation),\n",
    "                        epochs=200,\n",
    "                        batch_size=32,\n",
    "                        verbose=1,\n",
    "                        callbacks=[earlyStop])\n",
    "\n",
    "    models_algorithms_log_loss.append(history.history['val_loss'][-1])\n",
    "    \n",
    "best_algorithm_model_position = (models_algorithms_log_loss.index(min(models_algorithms_log_loss)))\n",
    "best_algorithm = algorithms[best_algorithm_model_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo con mejor rendimiento usó RMSprop como algoritmo de entrenamiento\n"
     ]
    }
   ],
   "source": [
    "plot_model_info(algorithms,models_algorithms_log_loss,'Algoritmos de aprendizaje')\n",
    "print(\"El modelo con mejor rendimiento usó %s como algoritmo de entrenamiento\" % best_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con los parámetros obtenidos anteriormente, se procede a variar la tasa de aprendizaje con el fin de escoger la que mejor se adapta al problema estudiado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 226us/sample - loss: 0.6849 - val_loss: 0.6676\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.6773 - val_loss: 0.6596\n",
      "Epoch 3/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.6699 - val_loss: 0.6519\n",
      "Epoch 4/300\n",
      "6012/6012 [==============================] - 0s 78us/sample - loss: 0.6626 - val_loss: 0.6443\n",
      "Epoch 5/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.6556 - val_loss: 0.6370\n",
      "Epoch 6/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.6488 - val_loss: 0.6299\n",
      "Epoch 7/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.6421 - val_loss: 0.6230\n",
      "Epoch 8/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.6357 - val_loss: 0.6164\n",
      "Epoch 9/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.6294 - val_loss: 0.6099\n",
      "Epoch 10/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.6234 - val_loss: 0.6036\n",
      "Epoch 11/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.6175 - val_loss: 0.5976\n",
      "Epoch 12/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.6117 - val_loss: 0.5917\n",
      "Epoch 13/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.6062 - val_loss: 0.5860\n",
      "Epoch 14/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.6008 - val_loss: 0.5804\n",
      "Epoch 15/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.5956 - val_loss: 0.5751\n",
      "Epoch 16/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.5904 - val_loss: 0.5699\n",
      "Epoch 17/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.5854 - val_loss: 0.5648\n",
      "Epoch 18/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.5806 - val_loss: 0.5599\n",
      "Epoch 19/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5759 - val_loss: 0.5551\n",
      "Epoch 20/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.5712 - val_loss: 0.5505\n",
      "Epoch 21/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5668 - val_loss: 0.5459\n",
      "Epoch 22/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5624 - val_loss: 0.5415\n",
      "Epoch 23/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5582 - val_loss: 0.5372\n",
      "Epoch 24/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5540 - val_loss: 0.5331\n",
      "Epoch 25/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5500 - val_loss: 0.5291\n",
      "Epoch 26/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5461 - val_loss: 0.5251\n",
      "Epoch 27/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5423 - val_loss: 0.5213\n",
      "Epoch 28/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5385 - val_loss: 0.5175\n",
      "Epoch 29/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5349 - val_loss: 0.5139\n",
      "Epoch 30/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5313 - val_loss: 0.5103\n",
      "Epoch 31/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5278 - val_loss: 0.5068\n",
      "Epoch 32/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5244 - val_loss: 0.5035\n",
      "Epoch 33/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5211 - val_loss: 0.5002\n",
      "Epoch 34/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5179 - val_loss: 0.4970\n",
      "Epoch 35/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.5148 - val_loss: 0.4939\n",
      "Epoch 36/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5117 - val_loss: 0.4909\n",
      "Epoch 37/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5087 - val_loss: 0.4879\n",
      "Epoch 38/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5057 - val_loss: 0.4850\n",
      "Epoch 39/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.5028 - val_loss: 0.4821\n",
      "Epoch 40/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.5000 - val_loss: 0.4794\n",
      "Epoch 41/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4973 - val_loss: 0.4767\n",
      "Epoch 42/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4946 - val_loss: 0.4741\n",
      "Epoch 43/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.4920 - val_loss: 0.4716\n",
      "Epoch 44/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4895 - val_loss: 0.4690\n",
      "Epoch 45/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4869 - val_loss: 0.4666\n",
      "Epoch 46/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4845 - val_loss: 0.4642\n",
      "Epoch 47/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4821 - val_loss: 0.4619\n",
      "Epoch 48/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4798 - val_loss: 0.4596\n",
      "Epoch 49/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4775 - val_loss: 0.4574\n",
      "Epoch 50/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4752 - val_loss: 0.4552\n",
      "Epoch 51/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4731 - val_loss: 0.4531\n",
      "Epoch 52/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4709 - val_loss: 0.4510\n",
      "Epoch 53/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.4688 - val_loss: 0.4490\n",
      "Epoch 54/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4668 - val_loss: 0.4471\n",
      "Epoch 55/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4648 - val_loss: 0.4452\n",
      "Epoch 56/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4628 - val_loss: 0.4433\n",
      "Epoch 57/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4609 - val_loss: 0.4415\n",
      "Epoch 58/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4591 - val_loss: 0.4397\n",
      "Epoch 59/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4572 - val_loss: 0.4380\n",
      "Epoch 60/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4554 - val_loss: 0.4362\n",
      "Epoch 61/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4537 - val_loss: 0.4346\n",
      "Epoch 62/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4520 - val_loss: 0.4330\n",
      "Epoch 63/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4503 - val_loss: 0.4314\n",
      "Epoch 64/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4487 - val_loss: 0.4299\n",
      "Epoch 65/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4470 - val_loss: 0.4284\n",
      "Epoch 66/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4455 - val_loss: 0.4269\n",
      "Epoch 67/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4439 - val_loss: 0.4255\n",
      "Epoch 68/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4424 - val_loss: 0.4241\n",
      "Epoch 69/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4409 - val_loss: 0.4228\n",
      "Epoch 70/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4395 - val_loss: 0.4215\n",
      "Epoch 71/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4381 - val_loss: 0.4202\n",
      "Epoch 72/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4367 - val_loss: 0.4189\n",
      "Epoch 73/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4354 - val_loss: 0.4177\n",
      "Epoch 74/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4340 - val_loss: 0.4165\n",
      "Epoch 75/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4327 - val_loss: 0.4153\n",
      "Epoch 76/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4315 - val_loss: 0.4142\n",
      "Epoch 77/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4302 - val_loss: 0.4131\n",
      "Epoch 78/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4290 - val_loss: 0.4121\n",
      "Epoch 79/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4278 - val_loss: 0.4110\n",
      "Epoch 80/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4267 - val_loss: 0.4100\n",
      "Epoch 81/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4255 - val_loss: 0.4090\n",
      "Epoch 82/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4244 - val_loss: 0.4081\n",
      "Epoch 83/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4233 - val_loss: 0.4072\n",
      "Epoch 84/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4222 - val_loss: 0.4063\n",
      "Epoch 85/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4212 - val_loss: 0.4054\n",
      "Epoch 86/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4202 - val_loss: 0.4045\n",
      "Epoch 87/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4192 - val_loss: 0.4037\n",
      "Epoch 88/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4182 - val_loss: 0.4029\n",
      "Epoch 89/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4172 - val_loss: 0.4020\n",
      "Epoch 90/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4162 - val_loss: 0.4012\n",
      "Epoch 91/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4153 - val_loss: 0.4005\n",
      "Epoch 92/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4144 - val_loss: 0.3997\n",
      "Epoch 93/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4135 - val_loss: 0.3990\n",
      "Epoch 94/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4126 - val_loss: 0.3983\n",
      "Epoch 95/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4118 - val_loss: 0.3976\n",
      "Epoch 96/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4109 - val_loss: 0.3969\n",
      "Epoch 97/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4101 - val_loss: 0.3962\n",
      "Epoch 98/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4093 - val_loss: 0.3956\n",
      "Epoch 99/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4085 - val_loss: 0.3950\n",
      "Epoch 100/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4077 - val_loss: 0.3943\n",
      "Epoch 101/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4070 - val_loss: 0.3937\n",
      "Epoch 102/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4063 - val_loss: 0.3932\n",
      "Epoch 103/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4055 - val_loss: 0.3926\n",
      "Epoch 104/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4048 - val_loss: 0.3920\n",
      "Epoch 105/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4041 - val_loss: 0.3914\n",
      "Epoch 106/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4034 - val_loss: 0.3909\n",
      "Epoch 107/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4028 - val_loss: 0.3904\n",
      "Epoch 108/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4021 - val_loss: 0.3898\n",
      "Epoch 109/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4014 - val_loss: 0.3893\n",
      "Epoch 110/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4008 - val_loss: 0.3888\n",
      "Epoch 111/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4002 - val_loss: 0.3884\n",
      "Epoch 112/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3996 - val_loss: 0.3879\n",
      "Epoch 113/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3990 - val_loss: 0.3874\n",
      "Epoch 114/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3984 - val_loss: 0.3870\n",
      "Epoch 115/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3978 - val_loss: 0.3865\n",
      "Epoch 116/300\n",
      "6012/6012 [==============================] - ETA: 0s - loss: 0.397 - 0s 60us/sample - loss: 0.3972 - val_loss: 0.3861\n",
      "Epoch 117/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3967 - val_loss: 0.3857\n",
      "Epoch 118/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3961 - val_loss: 0.3853\n",
      "Epoch 119/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3956 - val_loss: 0.3849\n",
      "Epoch 120/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3951 - val_loss: 0.3845\n",
      "Epoch 121/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3945 - val_loss: 0.3841\n",
      "Epoch 122/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3940 - val_loss: 0.3837\n",
      "Epoch 123/300\n",
      "6012/6012 [==============================] - 0s 61us/sample - loss: 0.3935 - val_loss: 0.3833\n",
      "Epoch 124/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3930 - val_loss: 0.3829\n",
      "Epoch 125/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3925 - val_loss: 0.3826\n",
      "Epoch 126/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3920 - val_loss: 0.3822\n",
      "Epoch 127/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3915 - val_loss: 0.3819\n",
      "Epoch 128/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3911 - val_loss: 0.3816\n",
      "Epoch 129/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3906 - val_loss: 0.3812\n",
      "Epoch 130/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3902 - val_loss: 0.3809\n",
      "Epoch 131/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3897 - val_loss: 0.3806\n",
      "Epoch 132/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3893 - val_loss: 0.3803\n",
      "Epoch 133/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3888 - val_loss: 0.3800\n",
      "Epoch 134/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3884 - val_loss: 0.3797\n",
      "Epoch 135/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3880 - val_loss: 0.3794\n",
      "Epoch 136/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3876 - val_loss: 0.3792\n",
      "Epoch 137/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3872 - val_loss: 0.3789\n",
      "Epoch 138/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3868 - val_loss: 0.3786\n",
      "Epoch 139/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3864 - val_loss: 0.3783\n",
      "Epoch 140/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3860 - val_loss: 0.3781\n",
      "Epoch 141/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3856 - val_loss: 0.3778\n",
      "Epoch 142/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3852 - val_loss: 0.3775\n",
      "Epoch 143/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3848 - val_loss: 0.3773\n",
      "Epoch 144/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3845 - val_loss: 0.3770\n",
      "Epoch 145/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3841 - val_loss: 0.3768\n",
      "Epoch 146/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3837 - val_loss: 0.3765\n",
      "Epoch 147/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3834 - val_loss: 0.3763\n",
      "Epoch 148/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3830 - val_loss: 0.3761\n",
      "Epoch 149/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3827 - val_loss: 0.3758\n",
      "Epoch 150/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3823 - val_loss: 0.3756\n",
      "Epoch 151/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3820 - val_loss: 0.3754\n",
      "Epoch 152/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3817 - val_loss: 0.3752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3813 - val_loss: 0.3750\n",
      "Epoch 154/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3810 - val_loss: 0.3747\n",
      "Epoch 155/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3807 - val_loss: 0.3745\n",
      "Epoch 156/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3803 - val_loss: 0.3743\n",
      "Epoch 157/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3800 - val_loss: 0.3741\n",
      "Epoch 158/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3797 - val_loss: 0.3739\n",
      "Epoch 159/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3794 - val_loss: 0.3737\n",
      "Epoch 160/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3791 - val_loss: 0.3735\n",
      "Epoch 161/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3788 - val_loss: 0.3733\n",
      "Epoch 162/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3785 - val_loss: 0.3731\n",
      "Epoch 163/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3782 - val_loss: 0.3729\n",
      "Epoch 164/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3779 - val_loss: 0.3727\n",
      "Epoch 165/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3776 - val_loss: 0.3726\n",
      "Epoch 166/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3773 - val_loss: 0.3724\n",
      "Epoch 167/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3771 - val_loss: 0.3722\n",
      "Epoch 168/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3768 - val_loss: 0.3720\n",
      "Epoch 169/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3765 - val_loss: 0.3719\n",
      "Epoch 170/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3762 - val_loss: 0.3717\n",
      "Epoch 171/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3760 - val_loss: 0.3715\n",
      "Epoch 172/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3757 - val_loss: 0.3714\n",
      "Epoch 173/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3754 - val_loss: 0.3712\n",
      "Epoch 174/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3752 - val_loss: 0.3710\n",
      "Epoch 175/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3749 - val_loss: 0.3709\n",
      "Epoch 176/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3747 - val_loss: 0.3707\n",
      "Epoch 177/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3744 - val_loss: 0.3706\n",
      "Epoch 178/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3741 - val_loss: 0.3704\n",
      "Epoch 179/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3739 - val_loss: 0.3702\n",
      "Epoch 180/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3736 - val_loss: 0.3701\n",
      "Epoch 181/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3734 - val_loss: 0.3699\n",
      "Epoch 182/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3732 - val_loss: 0.3698\n",
      "Epoch 183/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3729 - val_loss: 0.3696\n",
      "Epoch 184/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3727 - val_loss: 0.3695\n",
      "Epoch 185/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3724 - val_loss: 0.3693\n",
      "Epoch 186/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3722 - val_loss: 0.3692\n",
      "Epoch 187/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3720 - val_loss: 0.3690\n",
      "Epoch 188/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3717 - val_loss: 0.3689\n",
      "Epoch 189/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3715 - val_loss: 0.3687\n",
      "Epoch 190/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3713 - val_loss: 0.3686\n",
      "Epoch 191/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3710 - val_loss: 0.3684\n",
      "Epoch 192/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3708 - val_loss: 0.3683\n",
      "Epoch 193/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3706 - val_loss: 0.3682\n",
      "Epoch 194/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3704 - val_loss: 0.3680\n",
      "Epoch 195/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3702 - val_loss: 0.3679\n",
      "Epoch 196/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3699 - val_loss: 0.3678\n",
      "Epoch 197/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3697 - val_loss: 0.3676\n",
      "Epoch 198/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3695 - val_loss: 0.3675\n",
      "Epoch 199/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3693 - val_loss: 0.3674\n",
      "Epoch 200/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3691 - val_loss: 0.3672\n",
      "Epoch 201/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3689 - val_loss: 0.3671\n",
      "Epoch 202/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3687 - val_loss: 0.3670\n",
      "Epoch 203/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3685 - val_loss: 0.3669\n",
      "Epoch 204/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3683 - val_loss: 0.3667\n",
      "Epoch 205/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3681 - val_loss: 0.3666\n",
      "Epoch 206/300\n",
      "6012/6012 [==============================] - 0s 83us/sample - loss: 0.3679 - val_loss: 0.3665\n",
      "Epoch 207/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3677 - val_loss: 0.3664\n",
      "Epoch 208/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3675 - val_loss: 0.3663\n",
      "Epoch 209/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3673 - val_loss: 0.3661\n",
      "Epoch 210/300\n",
      "6012/6012 [==============================] - 0s 83us/sample - loss: 0.3671 - val_loss: 0.3660\n",
      "Epoch 211/300\n",
      "6012/6012 [==============================] - 0s 83us/sample - loss: 0.3670 - val_loss: 0.3659\n",
      "Epoch 212/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3668 - val_loss: 0.3658\n",
      "Epoch 213/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3666 - val_loss: 0.3657\n",
      "Epoch 214/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3664 - val_loss: 0.3656\n",
      "Epoch 215/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3662 - val_loss: 0.3655\n",
      "Epoch 216/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3660 - val_loss: 0.3654\n",
      "Epoch 217/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3659 - val_loss: 0.3652\n",
      "Epoch 218/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3657 - val_loss: 0.3651\n",
      "Epoch 219/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3655 - val_loss: 0.3650\n",
      "Epoch 220/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3653 - val_loss: 0.3649\n",
      "Epoch 221/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3652 - val_loss: 0.3648\n",
      "Epoch 222/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3650 - val_loss: 0.3647\n",
      "Epoch 223/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3648 - val_loss: 0.3646\n",
      "Epoch 224/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3646 - val_loss: 0.3645\n",
      "Epoch 225/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3645 - val_loss: 0.3644\n",
      "Epoch 226/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3643 - val_loss: 0.3643\n",
      "Epoch 227/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3641 - val_loss: 0.3642\n",
      "Epoch 228/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3640 - val_loss: 0.3641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3638 - val_loss: 0.3640\n",
      "Epoch 230/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3636 - val_loss: 0.3639\n",
      "Epoch 231/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3635 - val_loss: 0.3638\n",
      "Epoch 232/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3633 - val_loss: 0.3637\n",
      "Epoch 233/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3632 - val_loss: 0.3636\n",
      "Epoch 234/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3630 - val_loss: 0.3635\n",
      "Epoch 235/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3628 - val_loss: 0.3634\n",
      "Epoch 236/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3627 - val_loss: 0.3633\n",
      "Epoch 237/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3625 - val_loss: 0.3632\n",
      "Epoch 238/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3624 - val_loss: 0.3631\n",
      "Epoch 239/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3622 - val_loss: 0.3630\n",
      "Epoch 240/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3621 - val_loss: 0.3629\n",
      "Epoch 241/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3619 - val_loss: 0.3628\n",
      "Epoch 242/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3618 - val_loss: 0.3627\n",
      "Epoch 243/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3616 - val_loss: 0.3626\n",
      "Epoch 244/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3615 - val_loss: 0.3625\n",
      "Epoch 245/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3613 - val_loss: 0.3625\n",
      "Epoch 246/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3612 - val_loss: 0.3624\n",
      "Epoch 247/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3610 - val_loss: 0.3623\n",
      "Epoch 248/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3609 - val_loss: 0.3622\n",
      "Epoch 249/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3607 - val_loss: 0.3621\n",
      "Epoch 250/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3606 - val_loss: 0.3620\n",
      "Epoch 251/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3605 - val_loss: 0.3619\n",
      "Epoch 252/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3603 - val_loss: 0.3618\n",
      "Epoch 253/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3602 - val_loss: 0.3618\n",
      "Epoch 254/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3600 - val_loss: 0.3617\n",
      "Epoch 255/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3599 - val_loss: 0.3616\n",
      "Epoch 256/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3598 - val_loss: 0.3615\n",
      "Epoch 257/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3596 - val_loss: 0.3614\n",
      "Epoch 258/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3595 - val_loss: 0.3614\n",
      "Epoch 259/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3594 - val_loss: 0.3613\n",
      "Epoch 260/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3592 - val_loss: 0.3612\n",
      "Epoch 261/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3591 - val_loss: 0.3611\n",
      "Epoch 262/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3590 - val_loss: 0.3611\n",
      "Epoch 263/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3588 - val_loss: 0.3610\n",
      "Epoch 264/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3587 - val_loss: 0.3609\n",
      "Epoch 265/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3586 - val_loss: 0.3608\n",
      "Epoch 266/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3584 - val_loss: 0.3607\n",
      "Epoch 267/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3583 - val_loss: 0.3607\n",
      "Epoch 268/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3582 - val_loss: 0.3606\n",
      "Epoch 269/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3580 - val_loss: 0.3605\n",
      "Epoch 270/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3579 - val_loss: 0.3604\n",
      "Epoch 271/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3578 - val_loss: 0.3604\n",
      "Epoch 272/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3577 - val_loss: 0.3603\n",
      "Epoch 273/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3575 - val_loss: 0.3602\n",
      "Epoch 274/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3574 - val_loss: 0.3602\n",
      "Epoch 275/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3573 - val_loss: 0.3601\n",
      "Epoch 276/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3572 - val_loss: 0.3600\n",
      "Epoch 277/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3570 - val_loss: 0.3600\n",
      "Epoch 278/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3569 - val_loss: 0.3599\n",
      "Epoch 279/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3568 - val_loss: 0.3598\n",
      "Epoch 280/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3567 - val_loss: 0.3597\n",
      "Epoch 281/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3566 - val_loss: 0.3597\n",
      "Epoch 282/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3565 - val_loss: 0.3596\n",
      "Epoch 283/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3563 - val_loss: 0.3595\n",
      "Epoch 284/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3562 - val_loss: 0.3595\n",
      "Epoch 285/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3561 - val_loss: 0.3594\n",
      "Epoch 286/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3560 - val_loss: 0.3593\n",
      "Epoch 287/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3559 - val_loss: 0.3593\n",
      "Epoch 288/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3558 - val_loss: 0.3592\n",
      "Epoch 289/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3556 - val_loss: 0.3591\n",
      "Epoch 290/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3555 - val_loss: 0.3590\n",
      "Epoch 291/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3554 - val_loss: 0.3590\n",
      "Epoch 292/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3553 - val_loss: 0.3589\n",
      "Epoch 293/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3552 - val_loss: 0.3588\n",
      "Epoch 294/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3551 - val_loss: 0.3588\n",
      "Epoch 295/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3550 - val_loss: 0.3587\n",
      "Epoch 296/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3549 - val_loss: 0.3586\n",
      "Epoch 297/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3547 - val_loss: 0.3586\n",
      "Epoch 298/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3546 - val_loss: 0.3585\n",
      "Epoch 299/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3545 - val_loss: 0.3585\n",
      "Epoch 300/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3544 - val_loss: 0.3584\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 231us/sample - loss: 0.6649 - val_loss: 0.6408\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.6391 - val_loss: 0.6150\n",
      "Epoch 3/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.6163 - val_loss: 0.5922\n",
      "Epoch 4/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.5958 - val_loss: 0.5717\n",
      "Epoch 5/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.5770 - val_loss: 0.5532\n",
      "Epoch 6/300\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.5602 - val_loss: 0.5366\n",
      "Epoch 7/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.5450 - val_loss: 0.5215\n",
      "Epoch 8/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.5313 - val_loss: 0.5081\n",
      "Epoch 9/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.5189 - val_loss: 0.4962\n",
      "Epoch 10/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.5077 - val_loss: 0.4853\n",
      "Epoch 11/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4975 - val_loss: 0.4756\n",
      "Epoch 12/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4883 - val_loss: 0.4668\n",
      "Epoch 13/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4799 - val_loss: 0.4589\n",
      "Epoch 14/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4722 - val_loss: 0.4517\n",
      "Epoch 15/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4652 - val_loss: 0.4452\n",
      "Epoch 16/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4587 - val_loss: 0.4394\n",
      "Epoch 17/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4530 - val_loss: 0.4341\n",
      "Epoch 18/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4476 - val_loss: 0.4292\n",
      "Epoch 19/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4427 - val_loss: 0.4249\n",
      "Epoch 20/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4381 - val_loss: 0.4208\n",
      "Epoch 21/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4339 - val_loss: 0.4172\n",
      "Epoch 22/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4300 - val_loss: 0.4139\n",
      "Epoch 23/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4264 - val_loss: 0.4108\n",
      "Epoch 24/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4230 - val_loss: 0.4080\n",
      "Epoch 25/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4199 - val_loss: 0.4054\n",
      "Epoch 26/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4170 - val_loss: 0.4030\n",
      "Epoch 27/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4143 - val_loss: 0.4008\n",
      "Epoch 28/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4118 - val_loss: 0.3988\n",
      "Epoch 29/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4095 - val_loss: 0.3969\n",
      "Epoch 30/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4072 - val_loss: 0.3951\n",
      "Epoch 31/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4051 - val_loss: 0.3934\n",
      "Epoch 32/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4031 - val_loss: 0.3919\n",
      "Epoch 33/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4012 - val_loss: 0.3903\n",
      "Epoch 34/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3994 - val_loss: 0.3889\n",
      "Epoch 35/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3977 - val_loss: 0.3876\n",
      "Epoch 36/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3961 - val_loss: 0.3863\n",
      "Epoch 37/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3946 - val_loss: 0.3851\n",
      "Epoch 38/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3931 - val_loss: 0.3839\n",
      "Epoch 39/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3917 - val_loss: 0.3828\n",
      "Epoch 40/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3904 - val_loss: 0.3818\n",
      "Epoch 41/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3892 - val_loss: 0.3808\n",
      "Epoch 42/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3879 - val_loss: 0.3798\n",
      "Epoch 43/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3868 - val_loss: 0.3788\n",
      "Epoch 44/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3857 - val_loss: 0.3779\n",
      "Epoch 45/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3846 - val_loss: 0.3771\n",
      "Epoch 46/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3836 - val_loss: 0.3762\n",
      "Epoch 47/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3825 - val_loss: 0.3754\n",
      "Epoch 48/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3815 - val_loss: 0.3746\n",
      "Epoch 49/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3806 - val_loss: 0.3739\n",
      "Epoch 50/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3797 - val_loss: 0.3731\n",
      "Epoch 51/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3788 - val_loss: 0.3724\n",
      "Epoch 52/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3779 - val_loss: 0.3717\n",
      "Epoch 53/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3771 - val_loss: 0.3711\n",
      "Epoch 54/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3763 - val_loss: 0.3704\n",
      "Epoch 55/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3755 - val_loss: 0.3698\n",
      "Epoch 56/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3747 - val_loss: 0.3691\n",
      "Epoch 57/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3739 - val_loss: 0.3685\n",
      "Epoch 58/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3732 - val_loss: 0.3680\n",
      "Epoch 59/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3725 - val_loss: 0.3674\n",
      "Epoch 60/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3718 - val_loss: 0.3668\n",
      "Epoch 61/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3711 - val_loss: 0.3663\n",
      "Epoch 62/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3705 - val_loss: 0.3657\n",
      "Epoch 63/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3698 - val_loss: 0.3652\n",
      "Epoch 64/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3692 - val_loss: 0.3647\n",
      "Epoch 65/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3686 - val_loss: 0.3642\n",
      "Epoch 66/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3680 - val_loss: 0.3637\n",
      "Epoch 67/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3674 - val_loss: 0.3632\n",
      "Epoch 68/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3669 - val_loss: 0.3627\n",
      "Epoch 69/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3663 - val_loss: 0.3622\n",
      "Epoch 70/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3657 - val_loss: 0.3618\n",
      "Epoch 71/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3652 - val_loss: 0.3613\n",
      "Epoch 72/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3647 - val_loss: 0.3608\n",
      "Epoch 73/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3642 - val_loss: 0.3604\n",
      "Epoch 74/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3637 - val_loss: 0.3599\n",
      "Epoch 75/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3632 - val_loss: 0.3595\n",
      "Epoch 76/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3627 - val_loss: 0.3591\n",
      "Epoch 77/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3622 - val_loss: 0.3587\n",
      "Epoch 78/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3618 - val_loss: 0.3583\n",
      "Epoch 79/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3613 - val_loss: 0.3579\n",
      "Epoch 80/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3608 - val_loss: 0.3575\n",
      "Epoch 81/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3604 - val_loss: 0.3571\n",
      "Epoch 82/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3599 - val_loss: 0.3567\n",
      "Epoch 83/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3595 - val_loss: 0.3564\n",
      "Epoch 84/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3591 - val_loss: 0.3560\n",
      "Epoch 85/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3587 - val_loss: 0.3556\n",
      "Epoch 86/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3583 - val_loss: 0.3553\n",
      "Epoch 87/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3579 - val_loss: 0.3550\n",
      "Epoch 88/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3575 - val_loss: 0.3547\n",
      "Epoch 89/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3571 - val_loss: 0.3544\n",
      "Epoch 90/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3568 - val_loss: 0.3541\n",
      "Epoch 91/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3564 - val_loss: 0.3538\n",
      "Epoch 92/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3560 - val_loss: 0.3534\n",
      "Epoch 93/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3557 - val_loss: 0.3532\n",
      "Epoch 94/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3553 - val_loss: 0.3529\n",
      "Epoch 95/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3550 - val_loss: 0.3526\n",
      "Epoch 96/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3546 - val_loss: 0.3523\n",
      "Epoch 97/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3543 - val_loss: 0.3520\n",
      "Epoch 98/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3539 - val_loss: 0.3517\n",
      "Epoch 99/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3536 - val_loss: 0.3514\n",
      "Epoch 100/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3533 - val_loss: 0.3511\n",
      "Epoch 101/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3529 - val_loss: 0.3509\n",
      "Epoch 102/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3526 - val_loss: 0.3506\n",
      "Epoch 103/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3523 - val_loss: 0.3504\n",
      "Epoch 104/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3520 - val_loss: 0.3501\n",
      "Epoch 105/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3517 - val_loss: 0.3498\n",
      "Epoch 106/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3514 - val_loss: 0.3496\n",
      "Epoch 107/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3511 - val_loss: 0.3493\n",
      "Epoch 108/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3508 - val_loss: 0.3490\n",
      "Epoch 109/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3505 - val_loss: 0.3488\n",
      "Epoch 110/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3502 - val_loss: 0.3486\n",
      "Epoch 111/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3499 - val_loss: 0.3483\n",
      "Epoch 112/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3496 - val_loss: 0.3481\n",
      "Epoch 113/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3493 - val_loss: 0.3478\n",
      "Epoch 114/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3490 - val_loss: 0.3476\n",
      "Epoch 115/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3488 - val_loss: 0.3474\n",
      "Epoch 116/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3485 - val_loss: 0.3472\n",
      "Epoch 117/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3482 - val_loss: 0.3470\n",
      "Epoch 118/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3480 - val_loss: 0.3468\n",
      "Epoch 119/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3477 - val_loss: 0.3465\n",
      "Epoch 120/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3474 - val_loss: 0.3463\n",
      "Epoch 121/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3472 - val_loss: 0.3462\n",
      "Epoch 122/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3469 - val_loss: 0.3460\n",
      "Epoch 123/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3467 - val_loss: 0.3458\n",
      "Epoch 124/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3464 - val_loss: 0.3456\n",
      "Epoch 125/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3461 - val_loss: 0.3454\n",
      "Epoch 126/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3459 - val_loss: 0.3452\n",
      "Epoch 127/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3456 - val_loss: 0.3451\n",
      "Epoch 128/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3454 - val_loss: 0.3449\n",
      "Epoch 129/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3451 - val_loss: 0.3447\n",
      "Epoch 130/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3449 - val_loss: 0.3445\n",
      "Epoch 131/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3446 - val_loss: 0.3443\n",
      "Epoch 132/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3444 - val_loss: 0.3441\n",
      "Epoch 133/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3442 - val_loss: 0.3440\n",
      "Epoch 134/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3439 - val_loss: 0.3438\n",
      "Epoch 135/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3437 - val_loss: 0.3436\n",
      "Epoch 136/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3434 - val_loss: 0.3435\n",
      "Epoch 137/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3432 - val_loss: 0.3434\n",
      "Epoch 138/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3430 - val_loss: 0.3432\n",
      "Epoch 139/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3428 - val_loss: 0.3431\n",
      "Epoch 140/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3426 - val_loss: 0.3429\n",
      "Epoch 141/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3423 - val_loss: 0.3428\n",
      "Epoch 142/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3421 - val_loss: 0.3426\n",
      "Epoch 143/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3419 - val_loss: 0.3425\n",
      "Epoch 144/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3417 - val_loss: 0.3423\n",
      "Epoch 145/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3415 - val_loss: 0.3421\n",
      "Epoch 146/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3413 - val_loss: 0.3420\n",
      "Epoch 147/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3411 - val_loss: 0.3419\n",
      "Epoch 148/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3409 - val_loss: 0.3417\n",
      "Epoch 149/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3407 - val_loss: 0.3416\n",
      "Epoch 150/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3405 - val_loss: 0.3415\n",
      "Epoch 151/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3403 - val_loss: 0.3414\n",
      "Epoch 152/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3401 - val_loss: 0.3413\n",
      "Epoch 153/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3399 - val_loss: 0.3411\n",
      "Epoch 154/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3397 - val_loss: 0.3410\n",
      "Epoch 155/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3395 - val_loss: 0.3409\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3393 - val_loss: 0.3408\n",
      "Epoch 157/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3391 - val_loss: 0.3406\n",
      "Epoch 158/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3389 - val_loss: 0.3405\n",
      "Epoch 159/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3387 - val_loss: 0.3404\n",
      "Epoch 160/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3385 - val_loss: 0.3403\n",
      "Epoch 161/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3383 - val_loss: 0.3402\n",
      "Epoch 162/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3381 - val_loss: 0.3401\n",
      "Epoch 163/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3379 - val_loss: 0.3400\n",
      "Epoch 164/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3377 - val_loss: 0.3399\n",
      "Epoch 165/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3375 - val_loss: 0.3398\n",
      "Epoch 166/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3374 - val_loss: 0.3397\n",
      "Epoch 167/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3372 - val_loss: 0.3396\n",
      "Epoch 168/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3370 - val_loss: 0.3395\n",
      "Epoch 169/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3368 - val_loss: 0.3394\n",
      "Epoch 170/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3366 - val_loss: 0.3393\n",
      "Epoch 171/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3364 - val_loss: 0.3392\n",
      "Epoch 172/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3363 - val_loss: 0.3391\n",
      "Epoch 173/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3361 - val_loss: 0.3390\n",
      "Epoch 174/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3359 - val_loss: 0.3389\n",
      "Epoch 175/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3357 - val_loss: 0.3388\n",
      "Epoch 176/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3355 - val_loss: 0.3387\n",
      "Epoch 177/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3354 - val_loss: 0.3386\n",
      "Epoch 178/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3352 - val_loss: 0.3385\n",
      "Epoch 179/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3350 - val_loss: 0.3383\n",
      "Epoch 180/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3348 - val_loss: 0.3383\n",
      "Epoch 181/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3347 - val_loss: 0.3382\n",
      "Epoch 182/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3345 - val_loss: 0.3381\n",
      "Epoch 183/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3343 - val_loss: 0.3380\n",
      "Epoch 184/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3342 - val_loss: 0.3379\n",
      "Epoch 185/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3340 - val_loss: 0.3378\n",
      "Epoch 186/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3339 - val_loss: 0.3377\n",
      "Epoch 187/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3337 - val_loss: 0.3376\n",
      "Epoch 188/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3335 - val_loss: 0.3376\n",
      "Epoch 189/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3333 - val_loss: 0.3375\n",
      "Epoch 190/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3332 - val_loss: 0.3374\n",
      "Epoch 191/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3330 - val_loss: 0.3373\n",
      "Epoch 192/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3328 - val_loss: 0.3373\n",
      "Epoch 193/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3327 - val_loss: 0.3372\n",
      "Epoch 194/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3325 - val_loss: 0.3371\n",
      "Epoch 195/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3324 - val_loss: 0.3370\n",
      "Epoch 196/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3322 - val_loss: 0.3369\n",
      "Epoch 197/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3321 - val_loss: 0.3368\n",
      "Epoch 198/300\n",
      "6012/6012 [==============================] - 0s 81us/sample - loss: 0.3319 - val_loss: 0.3367\n",
      "Epoch 199/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3318 - val_loss: 0.3367\n",
      "Epoch 200/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3316 - val_loss: 0.3366\n",
      "Epoch 201/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3314 - val_loss: 0.3365\n",
      "Epoch 202/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3313 - val_loss: 0.3364\n",
      "Epoch 203/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3311 - val_loss: 0.3363\n",
      "Epoch 204/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3310 - val_loss: 0.3362\n",
      "Epoch 205/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3308 - val_loss: 0.3362\n",
      "Epoch 206/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3307 - val_loss: 0.3361\n",
      "Epoch 207/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3305 - val_loss: 0.3361\n",
      "Epoch 208/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3304 - val_loss: 0.3360\n",
      "Epoch 209/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3302 - val_loss: 0.3359\n",
      "Epoch 210/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3301 - val_loss: 0.3358\n",
      "Epoch 211/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3299 - val_loss: 0.3358\n",
      "Epoch 212/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3298 - val_loss: 0.3357\n",
      "Epoch 213/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3296 - val_loss: 0.3357\n",
      "Epoch 214/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3295 - val_loss: 0.3356\n",
      "Epoch 215/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3294 - val_loss: 0.3356\n",
      "Epoch 216/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3292 - val_loss: 0.3355\n",
      "Epoch 217/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3291 - val_loss: 0.3355\n",
      "Epoch 218/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3289 - val_loss: 0.3354\n",
      "Epoch 219/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3288 - val_loss: 0.3354\n",
      "Epoch 220/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3287 - val_loss: 0.3353\n",
      "Epoch 221/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3285 - val_loss: 0.3352\n",
      "Epoch 222/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3284 - val_loss: 0.3352\n",
      "Epoch 223/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3282 - val_loss: 0.3351\n",
      "Epoch 224/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3281 - val_loss: 0.3351\n",
      "Epoch 225/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3280 - val_loss: 0.3350\n",
      "Epoch 226/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3278 - val_loss: 0.3350\n",
      "Epoch 227/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3277 - val_loss: 0.3349\n",
      "Epoch 228/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3275 - val_loss: 0.3349\n",
      "Epoch 229/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3274 - val_loss: 0.3348\n",
      "Epoch 230/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3273 - val_loss: 0.3348\n",
      "Epoch 231/300\n",
      "6012/6012 [==============================] - 0s 78us/sample - loss: 0.3271 - val_loss: 0.3347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3270 - val_loss: 0.3347\n",
      "Epoch 233/300\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3269 - val_loss: 0.3346\n",
      "Epoch 234/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3267 - val_loss: 0.3346\n",
      "Epoch 235/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3266 - val_loss: 0.3346\n",
      "Epoch 236/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3265 - val_loss: 0.3345\n",
      "Epoch 237/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3264 - val_loss: 0.3345\n",
      "Epoch 238/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3262 - val_loss: 0.3345\n",
      "Epoch 239/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3261 - val_loss: 0.3345\n",
      "Epoch 240/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3260 - val_loss: 0.3344\n",
      "Epoch 241/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3258 - val_loss: 0.3344\n",
      "Epoch 242/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3257 - val_loss: 0.3343\n",
      "Epoch 243/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3256 - val_loss: 0.3343\n",
      "Epoch 244/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3255 - val_loss: 0.3342\n",
      "Epoch 245/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3253 - val_loss: 0.3342\n",
      "Epoch 246/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3252 - val_loss: 0.3342\n",
      "Epoch 247/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3251 - val_loss: 0.3342\n",
      "Epoch 248/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3249 - val_loss: 0.3341\n",
      "Epoch 249/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3248 - val_loss: 0.3341\n",
      "Epoch 250/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3247 - val_loss: 0.3340\n",
      "Epoch 251/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3246 - val_loss: 0.3340\n",
      "Epoch 252/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3244 - val_loss: 0.3339\n",
      "Epoch 253/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3243 - val_loss: 0.3339\n",
      "Epoch 254/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3242 - val_loss: 0.3338\n",
      "Epoch 255/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3241 - val_loss: 0.3338\n",
      "Epoch 256/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3239 - val_loss: 0.3338\n",
      "Epoch 257/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3238 - val_loss: 0.3337\n",
      "Epoch 258/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3237 - val_loss: 0.3337\n",
      "Epoch 259/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3235 - val_loss: 0.3336\n",
      "Epoch 260/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3234 - val_loss: 0.3336\n",
      "Epoch 261/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3233 - val_loss: 0.3336\n",
      "Epoch 262/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3232 - val_loss: 0.3335\n",
      "Epoch 263/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3230 - val_loss: 0.3335\n",
      "Epoch 264/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3229 - val_loss: 0.3334\n",
      "Epoch 265/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3228 - val_loss: 0.3333\n",
      "Epoch 266/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3227 - val_loss: 0.3333\n",
      "Epoch 267/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3226 - val_loss: 0.3333\n",
      "Epoch 268/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3225 - val_loss: 0.3332\n",
      "Epoch 269/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3223 - val_loss: 0.3332\n",
      "Epoch 270/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3222 - val_loss: 0.3332\n",
      "Epoch 271/300\n",
      "6012/6012 [==============================] - 0s 55us/sample - loss: 0.3221 - val_loss: 0.3331\n",
      "Epoch 272/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3220 - val_loss: 0.3331\n",
      "Epoch 273/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3218 - val_loss: 0.3330\n",
      "Epoch 274/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3217 - val_loss: 0.3330\n",
      "Epoch 275/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3216 - val_loss: 0.3329\n",
      "Epoch 276/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3215 - val_loss: 0.3329\n",
      "Epoch 277/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3214 - val_loss: 0.3328\n",
      "Epoch 278/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3213 - val_loss: 0.3328\n",
      "Epoch 279/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3211 - val_loss: 0.3327\n",
      "Epoch 280/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3210 - val_loss: 0.3327\n",
      "Epoch 281/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3209 - val_loss: 0.3327\n",
      "Epoch 282/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3208 - val_loss: 0.3326\n",
      "Epoch 283/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3206 - val_loss: 0.3326\n",
      "Epoch 284/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3205 - val_loss: 0.3326\n",
      "Epoch 285/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3204 - val_loss: 0.3326\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 213us/sample - loss: 0.7139 - val_loss: 0.6364\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.6234 - val_loss: 0.5625\n",
      "Epoch 3/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.5612 - val_loss: 0.5112\n",
      "Epoch 4/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.5171 - val_loss: 0.4747\n",
      "Epoch 5/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4851 - val_loss: 0.4479\n",
      "Epoch 6/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4608 - val_loss: 0.4276\n",
      "Epoch 7/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4422 - val_loss: 0.4122\n",
      "Epoch 8/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4276 - val_loss: 0.4006\n",
      "Epoch 9/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4161 - val_loss: 0.3918\n",
      "Epoch 10/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.4068 - val_loss: 0.3849\n",
      "Epoch 11/300\n",
      "6012/6012 [==============================] - 0s 58us/sample - loss: 0.3991 - val_loss: 0.3795\n",
      "Epoch 12/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3927 - val_loss: 0.3752\n",
      "Epoch 13/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3873 - val_loss: 0.3714\n",
      "Epoch 14/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3826 - val_loss: 0.3681\n",
      "Epoch 15/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3786 - val_loss: 0.3653\n",
      "Epoch 16/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3750 - val_loss: 0.3628\n",
      "Epoch 17/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3718 - val_loss: 0.3608\n",
      "Epoch 18/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3691 - val_loss: 0.3591\n",
      "Epoch 19/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3666 - val_loss: 0.3575\n",
      "Epoch 20/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3643 - val_loss: 0.3560\n",
      "Epoch 21/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3621 - val_loss: 0.3546\n",
      "Epoch 22/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3602 - val_loss: 0.3533\n",
      "Epoch 23/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3584 - val_loss: 0.3521\n",
      "Epoch 24/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3568 - val_loss: 0.3510\n",
      "Epoch 25/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3552 - val_loss: 0.3498\n",
      "Epoch 26/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3537 - val_loss: 0.3487\n",
      "Epoch 27/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3523 - val_loss: 0.3477\n",
      "Epoch 28/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3511 - val_loss: 0.3467\n",
      "Epoch 29/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3498 - val_loss: 0.3457\n",
      "Epoch 30/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3487 - val_loss: 0.3448\n",
      "Epoch 31/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3476 - val_loss: 0.3438\n",
      "Epoch 32/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3465 - val_loss: 0.3428\n",
      "Epoch 33/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3455 - val_loss: 0.3419\n",
      "Epoch 34/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3445 - val_loss: 0.3412\n",
      "Epoch 35/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3436 - val_loss: 0.3403\n",
      "Epoch 36/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3427 - val_loss: 0.3399\n",
      "Epoch 37/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3418 - val_loss: 0.3391\n",
      "Epoch 38/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3410 - val_loss: 0.3384\n",
      "Epoch 39/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3402 - val_loss: 0.3378\n",
      "Epoch 40/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3393 - val_loss: 0.3372\n",
      "Epoch 41/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3386 - val_loss: 0.3365\n",
      "Epoch 42/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3378 - val_loss: 0.3358\n",
      "Epoch 43/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3371 - val_loss: 0.3353\n",
      "Epoch 44/300\n",
      "6012/6012 [==============================] - 0s 78us/sample - loss: 0.3363 - val_loss: 0.3346\n",
      "Epoch 45/300\n",
      "6012/6012 [==============================] - 0s 81us/sample - loss: 0.3356 - val_loss: 0.3343\n",
      "Epoch 46/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3349 - val_loss: 0.3339\n",
      "Epoch 47/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3342 - val_loss: 0.3332\n",
      "Epoch 48/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3335 - val_loss: 0.3327\n",
      "Epoch 49/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3329 - val_loss: 0.3322\n",
      "Epoch 50/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3322 - val_loss: 0.3317\n",
      "Epoch 51/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3316 - val_loss: 0.3313\n",
      "Epoch 52/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3310 - val_loss: 0.3307\n",
      "Epoch 53/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3303 - val_loss: 0.3302\n",
      "Epoch 54/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3298 - val_loss: 0.3297\n",
      "Epoch 55/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3291 - val_loss: 0.3292\n",
      "Epoch 56/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3285 - val_loss: 0.3287\n",
      "Epoch 57/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3280 - val_loss: 0.3283\n",
      "Epoch 58/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3274 - val_loss: 0.3278\n",
      "Epoch 59/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3269 - val_loss: 0.3274\n",
      "Epoch 60/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3263 - val_loss: 0.3270\n",
      "Epoch 61/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3257 - val_loss: 0.3266\n",
      "Epoch 62/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3251 - val_loss: 0.3260\n",
      "Epoch 63/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3246 - val_loss: 0.3256\n",
      "Epoch 64/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3240 - val_loss: 0.3251\n",
      "Epoch 65/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3235 - val_loss: 0.3250\n",
      "Epoch 66/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3229 - val_loss: 0.3247\n",
      "Epoch 67/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3224 - val_loss: 0.3242\n",
      "Epoch 68/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3219 - val_loss: 0.3238\n",
      "Epoch 69/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3214 - val_loss: 0.3234\n",
      "Epoch 70/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3209 - val_loss: 0.3229\n",
      "Epoch 71/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3204 - val_loss: 0.3225\n",
      "Epoch 72/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3199 - val_loss: 0.3221\n",
      "Epoch 73/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3194 - val_loss: 0.3219\n",
      "Epoch 74/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3189 - val_loss: 0.3215\n",
      "Epoch 75/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3185 - val_loss: 0.3211\n",
      "Epoch 76/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3180 - val_loss: 0.3208\n",
      "Epoch 77/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3175 - val_loss: 0.3205\n",
      "Epoch 78/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3170 - val_loss: 0.3201\n",
      "Epoch 79/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3165 - val_loss: 0.3199\n",
      "Epoch 80/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3160 - val_loss: 0.3195\n",
      "Epoch 81/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3156 - val_loss: 0.3192\n",
      "Epoch 82/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3152 - val_loss: 0.3190\n",
      "Epoch 83/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3147 - val_loss: 0.3190\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 249us/sample - loss: 0.8997 - val_loss: 0.7149\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 78us/sample - loss: 0.6208 - val_loss: 0.5336\n",
      "Epoch 3/300\n",
      "6012/6012 [==============================] - 0s 81us/sample - loss: 0.4994 - val_loss: 0.4560\n",
      "Epoch 4/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4408 - val_loss: 0.4169\n",
      "Epoch 5/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.4103 - val_loss: 0.3960\n",
      "Epoch 6/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3923 - val_loss: 0.3834\n",
      "Epoch 7/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3806 - val_loss: 0.3750\n",
      "Epoch 8/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3725 - val_loss: 0.3682\n",
      "Epoch 9/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3661 - val_loss: 0.3626\n",
      "Epoch 10/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3611 - val_loss: 0.3590\n",
      "Epoch 11/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3568 - val_loss: 0.3556\n",
      "Epoch 12/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3532 - val_loss: 0.3516\n",
      "Epoch 13/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3499 - val_loss: 0.3485\n",
      "Epoch 14/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3467 - val_loss: 0.3453\n",
      "Epoch 15/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3438 - val_loss: 0.3431\n",
      "Epoch 16/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3412 - val_loss: 0.3398\n",
      "Epoch 17/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3386 - val_loss: 0.3372\n",
      "Epoch 18/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3364 - val_loss: 0.3349\n",
      "Epoch 19/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3343 - val_loss: 0.3332\n",
      "Epoch 20/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3325 - val_loss: 0.3310\n",
      "Epoch 21/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3305 - val_loss: 0.3290\n",
      "Epoch 22/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3287 - val_loss: 0.3268\n",
      "Epoch 23/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3270 - val_loss: 0.3255\n",
      "Epoch 24/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3256 - val_loss: 0.3241\n",
      "Epoch 25/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3239 - val_loss: 0.3222\n",
      "Epoch 26/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3227 - val_loss: 0.3213\n",
      "Epoch 27/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3211 - val_loss: 0.3195\n",
      "Epoch 28/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3201 - val_loss: 0.3184\n",
      "Epoch 29/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3187 - val_loss: 0.3177\n",
      "Epoch 30/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3176 - val_loss: 0.3164\n",
      "Epoch 31/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3163 - val_loss: 0.3157\n",
      "Epoch 32/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3152 - val_loss: 0.3138\n",
      "Epoch 33/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3143 - val_loss: 0.3126\n",
      "Epoch 34/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.3131 - val_loss: 0.3122\n",
      "Epoch 35/300\n",
      "6012/6012 [==============================] - 0s 75us/sample - loss: 0.3121 - val_loss: 0.3108\n",
      "Epoch 36/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3111 - val_loss: 0.3099\n",
      "Epoch 37/300\n",
      "6012/6012 [==============================] - 0s 73us/sample - loss: 0.3104 - val_loss: 0.3092\n",
      "Epoch 38/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 0.3096 - val_loss: 0.3088\n",
      "Epoch 39/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3086 - val_loss: 0.3084\n",
      "Epoch 40/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3079 - val_loss: 0.3078\n",
      "Epoch 41/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3070 - val_loss: 0.3081\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 211us/sample - loss: 0.5007 - val_loss: 0.3860\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3838 - val_loss: 0.3614\n",
      "Epoch 3/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3641 - val_loss: 0.3503\n",
      "Epoch 4/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3533 - val_loss: 0.3428\n",
      "Epoch 5/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3443 - val_loss: 0.3402\n",
      "Epoch 6/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3375 - val_loss: 0.3358\n",
      "Epoch 7/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3308 - val_loss: 0.3299\n",
      "Epoch 8/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3260 - val_loss: 0.3290\n",
      "Epoch 9/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3212 - val_loss: 0.3277\n",
      "Epoch 10/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3165 - val_loss: 0.3237\n",
      "Epoch 11/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3127 - val_loss: 0.3261\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 221us/sample - loss: 0.4169 - val_loss: 0.3727\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3511 - val_loss: 0.3452\n",
      "Epoch 3/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3337 - val_loss: 0.3350\n",
      "Epoch 4/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3217 - val_loss: 0.3338\n",
      "Epoch 5/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3140 - val_loss: 0.3292\n",
      "Epoch 6/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3057 - val_loss: 0.3184\n",
      "Epoch 7/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3002 - val_loss: 0.3161\n",
      "Epoch 8/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.2962 - val_loss: 0.3131\n",
      "Epoch 9/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.2924 - val_loss: 0.3130\n",
      "Epoch 10/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 0.2888 - val_loss: 0.3152\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 236us/sample - loss: 0.3738 - val_loss: 0.3374\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 65us/sample - loss: 0.3316 - val_loss: 0.3432\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 247us/sample - loss: 0.3848 - val_loss: 0.3327\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.3445 - val_loss: 0.3313\n",
      "Epoch 3/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3301 - val_loss: 0.3839\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 249us/sample - loss: 0.4556 - val_loss: 0.4352\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.4118 - val_loss: 0.4324\n",
      "Epoch 3/300\n",
      "6012/6012 [==============================] - 0s 62us/sample - loss: 0.3942 - val_loss: 0.4217\n",
      "Epoch 4/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.4000 - val_loss: 0.3927\n",
      "Epoch 5/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.3904 - val_loss: 0.4810\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 218us/sample - loss: 0.9457 - val_loss: 1.0001\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.7190 - val_loss: 0.7301\n",
      "Epoch 3/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 0.5978 - val_loss: 0.6551\n",
      "Epoch 4/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 0.6207 - val_loss: 0.8571\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 216us/sample - loss: 6.4698 - val_loss: 3.7109\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 1.9792 - val_loss: 2.1481\n",
      "Epoch 3/300\n",
      "6012/6012 [==============================] - 0s 68us/sample - loss: 1.5975 - val_loss: 3.3717\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/300\n",
      "6012/6012 [==============================] - 1s 218us/sample - loss: 55.8399 - val_loss: 9.5015\n",
      "Epoch 2/300\n",
      "6012/6012 [==============================] - 0s 81us/sample - loss: 10.4536 - val_loss: 9.4134\n",
      "Epoch 3/300\n",
      "6012/6012 [==============================] - 0s 70us/sample - loss: 5.8682 - val_loss: 6.6026\n",
      "Epoch 4/300\n",
      "6012/6012 [==============================] - 0s 57us/sample - loss: 4.4405 - val_loss: 2.2455\n",
      "Epoch 5/300\n",
      "6012/6012 [==============================] - 0s 60us/sample - loss: 2.0206 - val_loss: 17.8152\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import get\n",
    "\n",
    "learning_rates = [0.00001,0.00003,\n",
    "                 0.0001, 0.0003,\n",
    "                 0.001, 0.003,\n",
    "                 0.01, 0.03,\n",
    "                 0.1, 0.3,\n",
    "                 1, 3]\n",
    "\n",
    "algorithm = best_algorithm\n",
    "models_lr_log_loss = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    \n",
    "    optimizer = get(algorithm).from_config({'learning_rate':learning_rate})\n",
    "    model = getMoldel(neurons, activation=best_activation, optimizer=optimizer)\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_validation,y_validation),\n",
    "                        epochs=300,\n",
    "                        batch_size=32,\n",
    "                        verbose=1,\n",
    "                        callbacks=[earlyStop])\n",
    "\n",
    "    models_lr_log_loss.append(history.history['val_loss'][-1])\n",
    "    \n",
    "best_lr_model_position = (models_lr_log_loss.index(min(models_lr_log_loss)))\n",
    "best_lr = learning_rates[best_lr_model_position]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se grafica cada una de las tasas de aprendizaje usadas y se escoge la que obtuvo menores pérdidas con un número máximo de iteraciones igual a 300. El entrenamiento se suspendia si se observaba un aumento en la función de error o un mejoramiento menor a $10^{-9}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo con mejor rendimiento usó 0.000300 como tasa de aprendizaje\n"
     ]
    }
   ],
   "source": [
    "pl.semilogx(learning_rates, \n",
    "        models_lr_log_loss,\n",
    "        linewidth=2,\n",
    "        label='Valor del error')\n",
    "\n",
    "pl.semilogx(learning_rates,\n",
    "        [models_lr_log_loss[best_lr_model_position]] * len(learning_rates),\n",
    "        linestyle = '--',\n",
    "        linewidth = 1,\n",
    "        color = 'red',\n",
    "        label = 'Mínimo valor de error')\n",
    "\n",
    "pl.xlabel('Funciones de activacion')\n",
    "pl.ylabel('Tasa de aprendizaje')\n",
    "pl.title('Comportamiento de la función de error')\n",
    "pl.legend()\n",
    "pl.xlim(min(learning_rates), max(learning_rates))\n",
    "pl.show()\n",
    "print(\"El modelo con mejor rendimiento usó %f como tasa de aprendizaje\" % best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con la información obtenida, se entrena el modelo con el cojunto de entrenamiento completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples\n",
      "Epoch 1/500\n",
      "6680/6680 - 1s - loss: 0.4998\n",
      "Epoch 2/500\n",
      "6680/6680 - 0s - loss: 0.3876\n",
      "Epoch 3/500\n",
      "6680/6680 - 0s - loss: 0.3649\n",
      "Epoch 4/500\n",
      "6680/6680 - 0s - loss: 0.3528\n",
      "Epoch 5/500\n",
      "6680/6680 - 0s - loss: 0.3445\n",
      "Epoch 6/500\n",
      "6680/6680 - 0s - loss: 0.3388\n",
      "Epoch 7/500\n",
      "6680/6680 - 0s - loss: 0.3337\n",
      "Epoch 8/500\n",
      "6680/6680 - 0s - loss: 0.3296\n",
      "Epoch 9/500\n",
      "6680/6680 - 0s - loss: 0.3255\n",
      "Epoch 10/500\n",
      "6680/6680 - 0s - loss: 0.3220\n",
      "Epoch 11/500\n",
      "6680/6680 - 0s - loss: 0.3189\n",
      "Epoch 12/500\n",
      "6680/6680 - 0s - loss: 0.3161\n",
      "Epoch 13/500\n",
      "6680/6680 - 0s - loss: 0.3129\n",
      "Epoch 14/500\n",
      "6680/6680 - 0s - loss: 0.3101\n",
      "Epoch 15/500\n",
      "6680/6680 - 0s - loss: 0.3076\n",
      "Epoch 16/500\n",
      "6680/6680 - 0s - loss: 0.3052\n",
      "Epoch 17/500\n",
      "6680/6680 - 0s - loss: 0.3030\n",
      "Epoch 18/500\n",
      "6680/6680 - 0s - loss: 0.3005\n",
      "Epoch 19/500\n",
      "6680/6680 - 0s - loss: 0.2984\n",
      "Epoch 20/500\n",
      "6680/6680 - 0s - loss: 0.2965\n",
      "Epoch 21/500\n",
      "6680/6680 - 0s - loss: 0.2948\n",
      "Epoch 22/500\n",
      "6680/6680 - 0s - loss: 0.2925\n",
      "Epoch 23/500\n",
      "6680/6680 - 0s - loss: 0.2913\n",
      "Epoch 24/500\n",
      "6680/6680 - 0s - loss: 0.2897\n",
      "Epoch 25/500\n",
      "6680/6680 - 0s - loss: 0.2878\n",
      "Epoch 26/500\n",
      "6680/6680 - 0s - loss: 0.2865\n",
      "Epoch 27/500\n",
      "6680/6680 - 0s - loss: 0.2848\n",
      "Epoch 28/500\n",
      "6680/6680 - 0s - loss: 0.2832\n",
      "Epoch 29/500\n",
      "6680/6680 - 0s - loss: 0.2823\n",
      "Epoch 30/500\n",
      "6680/6680 - 0s - loss: 0.2806\n",
      "Epoch 31/500\n",
      "6680/6680 - 0s - loss: 0.2794\n",
      "Epoch 32/500\n",
      "6680/6680 - 0s - loss: 0.2782\n",
      "Epoch 33/500\n",
      "6680/6680 - 0s - loss: 0.2772\n",
      "Epoch 34/500\n",
      "6680/6680 - 0s - loss: 0.2762\n",
      "Epoch 35/500\n",
      "6680/6680 - 0s - loss: 0.2749\n",
      "Epoch 36/500\n",
      "6680/6680 - 0s - loss: 0.2731\n",
      "Epoch 37/500\n",
      "6680/6680 - 0s - loss: 0.2732\n",
      "Epoch 38/500\n",
      "6680/6680 - 0s - loss: 0.2718\n",
      "Epoch 39/500\n",
      "6680/6680 - 0s - loss: 0.2711\n",
      "Epoch 40/500\n",
      "6680/6680 - 0s - loss: 0.2702\n",
      "Epoch 41/500\n",
      "6680/6680 - 0s - loss: 0.2692\n",
      "Epoch 42/500\n",
      "6680/6680 - 0s - loss: 0.2689\n",
      "Epoch 43/500\n",
      "6680/6680 - 0s - loss: 0.2676\n",
      "Epoch 44/500\n",
      "6680/6680 - 0s - loss: 0.2669\n",
      "Epoch 45/500\n",
      "6680/6680 - 0s - loss: 0.2657\n",
      "Epoch 46/500\n",
      "6680/6680 - 0s - loss: 0.2653\n",
      "Epoch 47/500\n",
      "6680/6680 - 0s - loss: 0.2647\n",
      "Epoch 48/500\n",
      "6680/6680 - 0s - loss: 0.2636\n",
      "Epoch 49/500\n",
      "6680/6680 - 0s - loss: 0.2636\n",
      "Epoch 50/500\n",
      "6680/6680 - 0s - loss: 0.2626\n",
      "Epoch 51/500\n",
      "6680/6680 - 0s - loss: 0.2619\n",
      "Epoch 52/500\n",
      "6680/6680 - 0s - loss: 0.2617\n",
      "Epoch 53/500\n",
      "6680/6680 - 0s - loss: 0.2612\n",
      "Epoch 54/500\n",
      "6680/6680 - 0s - loss: 0.2600\n",
      "Epoch 55/500\n",
      "6680/6680 - 0s - loss: 0.2598\n",
      "Epoch 56/500\n",
      "6680/6680 - 0s - loss: 0.2593\n",
      "Epoch 57/500\n",
      "6680/6680 - 0s - loss: 0.2585\n",
      "Epoch 58/500\n",
      "6680/6680 - 0s - loss: 0.2577\n",
      "Epoch 59/500\n",
      "6680/6680 - 0s - loss: 0.2570\n",
      "Epoch 60/500\n",
      "6680/6680 - 0s - loss: 0.2568\n",
      "Epoch 61/500\n",
      "6680/6680 - 0s - loss: 0.2565\n",
      "Epoch 62/500\n",
      "6680/6680 - 0s - loss: 0.2564\n",
      "Epoch 63/500\n",
      "6680/6680 - 0s - loss: 0.2556\n",
      "Epoch 64/500\n",
      "6680/6680 - 0s - loss: 0.2550\n",
      "Epoch 65/500\n",
      "6680/6680 - 0s - loss: 0.2546\n",
      "Epoch 66/500\n",
      "6680/6680 - 0s - loss: 0.2538\n",
      "Epoch 67/500\n",
      "6680/6680 - 0s - loss: 0.2531\n",
      "Epoch 68/500\n",
      "6680/6680 - 0s - loss: 0.2526\n",
      "Epoch 69/500\n",
      "6680/6680 - 0s - loss: 0.2526\n",
      "Epoch 70/500\n",
      "6680/6680 - 0s - loss: 0.2518\n",
      "Epoch 71/500\n",
      "6680/6680 - 0s - loss: 0.2517\n",
      "Epoch 72/500\n",
      "6680/6680 - 0s - loss: 0.2514\n",
      "Epoch 73/500\n",
      "6680/6680 - 1s - loss: 0.2507\n",
      "Epoch 74/500\n",
      "6680/6680 - 0s - loss: 0.2503\n",
      "Epoch 75/500\n",
      "6680/6680 - 0s - loss: 0.2499\n",
      "Epoch 76/500\n",
      "6680/6680 - 0s - loss: 0.2495\n",
      "Epoch 77/500\n",
      "6680/6680 - 0s - loss: 0.2488\n",
      "Epoch 78/500\n",
      "6680/6680 - 0s - loss: 0.2489\n",
      "Epoch 79/500\n",
      "6680/6680 - 0s - loss: 0.2479\n",
      "Epoch 80/500\n",
      "6680/6680 - 0s - loss: 0.2475\n",
      "Epoch 81/500\n",
      "6680/6680 - 0s - loss: 0.2477\n",
      "Epoch 82/500\n",
      "6680/6680 - 0s - loss: 0.2468\n",
      "Epoch 83/500\n",
      "6680/6680 - 0s - loss: 0.2464\n",
      "Epoch 84/500\n",
      "6680/6680 - 0s - loss: 0.2463\n",
      "Epoch 85/500\n",
      "6680/6680 - 0s - loss: 0.2458\n",
      "Epoch 86/500\n",
      "6680/6680 - 0s - loss: 0.2453\n",
      "Epoch 87/500\n",
      "6680/6680 - 0s - loss: 0.2452\n",
      "Epoch 88/500\n",
      "6680/6680 - 0s - loss: 0.2440\n",
      "Epoch 89/500\n",
      "6680/6680 - 0s - loss: 0.2446\n",
      "Epoch 90/500\n",
      "6680/6680 - 0s - loss: 0.2433\n",
      "Epoch 91/500\n",
      "6680/6680 - 0s - loss: 0.2436\n",
      "Epoch 92/500\n",
      "6680/6680 - 0s - loss: 0.2436\n",
      "Epoch 93/500\n",
      "6680/6680 - 0s - loss: 0.2430\n",
      "Epoch 94/500\n",
      "6680/6680 - 0s - loss: 0.2429\n",
      "Epoch 95/500\n",
      "6680/6680 - 0s - loss: 0.2424\n",
      "Epoch 96/500\n",
      "6680/6680 - 0s - loss: 0.2425\n",
      "Epoch 97/500\n",
      "6680/6680 - 0s - loss: 0.2414\n",
      "Epoch 98/500\n",
      "6680/6680 - 0s - loss: 0.2419\n",
      "Epoch 99/500\n",
      "6680/6680 - 0s - loss: 0.2416\n",
      "Epoch 100/500\n",
      "6680/6680 - 0s - loss: 0.2414\n",
      "Epoch 101/500\n",
      "6680/6680 - 0s - loss: 0.2405\n",
      "Epoch 102/500\n",
      "6680/6680 - 0s - loss: 0.2401\n",
      "Epoch 103/500\n",
      "6680/6680 - 0s - loss: 0.2403\n",
      "Epoch 104/500\n",
      "6680/6680 - 0s - loss: 0.2406\n",
      "Epoch 105/500\n",
      "6680/6680 - 0s - loss: 0.2396\n",
      "Epoch 106/500\n",
      "6680/6680 - 0s - loss: 0.2394\n",
      "Epoch 107/500\n",
      "6680/6680 - 0s - loss: 0.2394\n",
      "Epoch 108/500\n",
      "6680/6680 - 0s - loss: 0.2394\n",
      "Epoch 109/500\n",
      "6680/6680 - 0s - loss: 0.2387\n",
      "Epoch 110/500\n",
      "6680/6680 - 0s - loss: 0.2383\n",
      "Epoch 111/500\n",
      "6680/6680 - 0s - loss: 0.2389\n",
      "Epoch 112/500\n",
      "6680/6680 - 0s - loss: 0.2385\n",
      "Epoch 113/500\n",
      "6680/6680 - 0s - loss: 0.2385\n",
      "Epoch 114/500\n",
      "6680/6680 - 0s - loss: 0.2383\n",
      "Epoch 115/500\n",
      "6680/6680 - 0s - loss: 0.2379\n",
      "Epoch 116/500\n",
      "6680/6680 - 0s - loss: 0.2372\n",
      "Epoch 117/500\n",
      "6680/6680 - 0s - loss: 0.2374\n",
      "Epoch 118/500\n",
      "6680/6680 - 0s - loss: 0.2367\n",
      "Epoch 119/500\n",
      "6680/6680 - 0s - loss: 0.2369\n",
      "Epoch 120/500\n",
      "6680/6680 - 0s - loss: 0.2365\n",
      "Epoch 121/500\n",
      "6680/6680 - 0s - loss: 0.2366\n",
      "Epoch 122/500\n",
      "6680/6680 - 0s - loss: 0.2359\n",
      "Epoch 123/500\n",
      "6680/6680 - 0s - loss: 0.2361\n",
      "Epoch 124/500\n",
      "6680/6680 - 0s - loss: 0.2357\n",
      "Epoch 125/500\n",
      "6680/6680 - 0s - loss: 0.2351\n",
      "Epoch 126/500\n",
      "6680/6680 - 0s - loss: 0.2357\n",
      "Epoch 127/500\n",
      "6680/6680 - 0s - loss: 0.2356\n",
      "Epoch 128/500\n",
      "6680/6680 - 0s - loss: 0.2350\n",
      "Epoch 129/500\n",
      "6680/6680 - 0s - loss: 0.2348\n",
      "Epoch 130/500\n",
      "6680/6680 - 0s - loss: 0.2345\n",
      "Epoch 131/500\n",
      "6680/6680 - 0s - loss: 0.2341\n",
      "Epoch 132/500\n",
      "6680/6680 - 0s - loss: 0.2343\n",
      "Epoch 133/500\n",
      "6680/6680 - 0s - loss: 0.2337\n",
      "Epoch 134/500\n",
      "6680/6680 - 0s - loss: 0.2337\n",
      "Epoch 135/500\n",
      "6680/6680 - 0s - loss: 0.2329\n",
      "Epoch 136/500\n",
      "6680/6680 - 0s - loss: 0.2334\n",
      "Epoch 137/500\n",
      "6680/6680 - 0s - loss: 0.2340\n",
      "Epoch 138/500\n",
      "6680/6680 - 0s - loss: 0.2331\n",
      "Epoch 139/500\n",
      "6680/6680 - 0s - loss: 0.2333\n",
      "Epoch 140/500\n",
      "6680/6680 - 0s - loss: 0.2323\n",
      "Epoch 141/500\n",
      "6680/6680 - 0s - loss: 0.2322\n",
      "Epoch 142/500\n",
      "6680/6680 - 0s - loss: 0.2322\n",
      "Epoch 143/500\n",
      "6680/6680 - 0s - loss: 0.2324\n",
      "Epoch 144/500\n",
      "6680/6680 - 0s - loss: 0.2322\n",
      "Epoch 145/500\n",
      "6680/6680 - 0s - loss: 0.2313\n",
      "Epoch 146/500\n",
      "6680/6680 - 0s - loss: 0.2314\n",
      "Epoch 147/500\n",
      "6680/6680 - 0s - loss: 0.2321\n",
      "Epoch 148/500\n",
      "6680/6680 - 0s - loss: 0.2314\n",
      "Epoch 149/500\n",
      "6680/6680 - 0s - loss: 0.2311\n",
      "Epoch 150/500\n",
      "6680/6680 - 0s - loss: 0.2314\n",
      "Epoch 151/500\n",
      "6680/6680 - 0s - loss: 0.2312\n",
      "Epoch 152/500\n",
      "6680/6680 - 0s - loss: 0.2304\n",
      "Epoch 153/500\n",
      "6680/6680 - 0s - loss: 0.2307\n",
      "Epoch 154/500\n",
      "6680/6680 - 0s - loss: 0.2305\n",
      "Epoch 155/500\n",
      "6680/6680 - 0s - loss: 0.2306\n",
      "Epoch 156/500\n",
      "6680/6680 - 0s - loss: 0.2308\n",
      "Epoch 157/500\n",
      "6680/6680 - 0s - loss: 0.2302\n",
      "Epoch 158/500\n",
      "6680/6680 - 0s - loss: 0.2298\n",
      "Epoch 159/500\n",
      "6680/6680 - 0s - loss: 0.2304\n",
      "Epoch 160/500\n",
      "6680/6680 - 0s - loss: 0.2305\n",
      "Epoch 161/500\n",
      "6680/6680 - 0s - loss: 0.2299\n",
      "Epoch 162/500\n",
      "6680/6680 - 0s - loss: 0.2294\n",
      "Epoch 163/500\n",
      "6680/6680 - 0s - loss: 0.2299\n",
      "Epoch 164/500\n",
      "6680/6680 - 0s - loss: 0.2293\n",
      "Epoch 165/500\n",
      "6680/6680 - 0s - loss: 0.2290\n",
      "Epoch 166/500\n",
      "6680/6680 - 0s - loss: 0.2286\n",
      "Epoch 167/500\n",
      "6680/6680 - 0s - loss: 0.2294\n",
      "Epoch 168/500\n",
      "6680/6680 - 0s - loss: 0.2288\n",
      "Epoch 169/500\n",
      "6680/6680 - 0s - loss: 0.2288\n",
      "Epoch 170/500\n",
      "6680/6680 - 0s - loss: 0.2286\n",
      "Epoch 171/500\n",
      "6680/6680 - 0s - loss: 0.2281\n",
      "Epoch 172/500\n",
      "6680/6680 - 0s - loss: 0.2286\n",
      "Epoch 173/500\n",
      "6680/6680 - 0s - loss: 0.2276\n",
      "Epoch 174/500\n",
      "6680/6680 - 0s - loss: 0.2288\n",
      "Epoch 175/500\n",
      "6680/6680 - 0s - loss: 0.2274\n",
      "Epoch 176/500\n",
      "6680/6680 - 0s - loss: 0.2278\n",
      "Epoch 177/500\n",
      "6680/6680 - 0s - loss: 0.2269\n",
      "Epoch 178/500\n",
      "6680/6680 - 0s - loss: 0.2275\n",
      "Epoch 179/500\n",
      "6680/6680 - 0s - loss: 0.2273\n",
      "Epoch 180/500\n",
      "6680/6680 - 0s - loss: 0.2265\n",
      "Epoch 181/500\n",
      "6680/6680 - 0s - loss: 0.2271\n",
      "Epoch 182/500\n",
      "6680/6680 - 0s - loss: 0.2266\n",
      "Epoch 183/500\n",
      "6680/6680 - 0s - loss: 0.2270\n",
      "Epoch 184/500\n",
      "6680/6680 - 0s - loss: 0.2267\n",
      "Epoch 185/500\n",
      "6680/6680 - 0s - loss: 0.2265\n",
      "Epoch 186/500\n",
      "6680/6680 - 0s - loss: 0.2267\n",
      "Epoch 187/500\n",
      "6680/6680 - 0s - loss: 0.2267\n",
      "Epoch 188/500\n",
      "6680/6680 - 0s - loss: 0.2270\n",
      "Epoch 189/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 - 0s - loss: 0.2265\n",
      "Epoch 190/500\n",
      "6680/6680 - 0s - loss: 0.2261\n",
      "Epoch 191/500\n",
      "6680/6680 - 0s - loss: 0.2264\n",
      "Epoch 192/500\n",
      "6680/6680 - 0s - loss: 0.2258\n",
      "Epoch 193/500\n",
      "6680/6680 - 0s - loss: 0.2264\n",
      "Epoch 194/500\n",
      "6680/6680 - 0s - loss: 0.2259\n",
      "Epoch 195/500\n",
      "6680/6680 - 0s - loss: 0.2254\n",
      "Epoch 196/500\n",
      "6680/6680 - 0s - loss: 0.2264\n",
      "Epoch 197/500\n",
      "6680/6680 - 0s - loss: 0.2258\n",
      "Epoch 198/500\n",
      "6680/6680 - 0s - loss: 0.2259\n",
      "Epoch 199/500\n",
      "6680/6680 - 0s - loss: 0.2248\n",
      "Epoch 200/500\n",
      "6680/6680 - 0s - loss: 0.2256\n",
      "Epoch 201/500\n",
      "6680/6680 - 0s - loss: 0.2249\n",
      "Epoch 202/500\n",
      "6680/6680 - 0s - loss: 0.2254\n",
      "Epoch 203/500\n",
      "6680/6680 - 0s - loss: 0.2253\n",
      "Epoch 204/500\n",
      "6680/6680 - 0s - loss: 0.2248\n",
      "Epoch 205/500\n",
      "6680/6680 - 0s - loss: 0.2248\n",
      "Epoch 206/500\n",
      "6680/6680 - 0s - loss: 0.2240\n",
      "Epoch 207/500\n",
      "6680/6680 - 0s - loss: 0.2247\n",
      "Epoch 208/500\n",
      "6680/6680 - 0s - loss: 0.2247\n",
      "Epoch 209/500\n",
      "6680/6680 - 0s - loss: 0.2242\n",
      "Epoch 210/500\n",
      "6680/6680 - 0s - loss: 0.2239\n",
      "Epoch 211/500\n",
      "6680/6680 - 0s - loss: 0.2235\n",
      "Epoch 212/500\n",
      "6680/6680 - 0s - loss: 0.2239\n",
      "Epoch 213/500\n",
      "6680/6680 - 0s - loss: 0.2243\n",
      "Epoch 214/500\n",
      "6680/6680 - 0s - loss: 0.2238\n",
      "Epoch 215/500\n",
      "6680/6680 - 0s - loss: 0.2244\n",
      "Epoch 216/500\n",
      "6680/6680 - 0s - loss: 0.2236\n",
      "Epoch 217/500\n",
      "6680/6680 - 0s - loss: 0.2232\n",
      "Epoch 218/500\n",
      "6680/6680 - 0s - loss: 0.2229\n",
      "Epoch 219/500\n",
      "6680/6680 - 0s - loss: 0.2233\n",
      "Epoch 220/500\n",
      "6680/6680 - 0s - loss: 0.2234\n",
      "Epoch 221/500\n",
      "6680/6680 - 0s - loss: 0.2229\n",
      "Epoch 222/500\n",
      "6680/6680 - 0s - loss: 0.2229\n",
      "Epoch 223/500\n",
      "6680/6680 - 0s - loss: 0.2225\n",
      "Epoch 224/500\n",
      "6680/6680 - 0s - loss: 0.2231\n",
      "Epoch 225/500\n",
      "6680/6680 - 0s - loss: 0.2227\n",
      "Epoch 226/500\n",
      "6680/6680 - 0s - loss: 0.2224\n",
      "Epoch 227/500\n",
      "6680/6680 - 0s - loss: 0.2223\n",
      "Epoch 228/500\n",
      "6680/6680 - 0s - loss: 0.2226\n",
      "Epoch 229/500\n",
      "6680/6680 - 0s - loss: 0.2224\n",
      "Epoch 230/500\n",
      "6680/6680 - 0s - loss: 0.2216\n",
      "Epoch 231/500\n",
      "6680/6680 - 0s - loss: 0.2216\n",
      "Epoch 232/500\n",
      "6680/6680 - 0s - loss: 0.2221\n",
      "Epoch 233/500\n",
      "6680/6680 - 0s - loss: 0.2218\n",
      "Epoch 234/500\n",
      "6680/6680 - 0s - loss: 0.2211\n",
      "Epoch 235/500\n",
      "6680/6680 - 0s - loss: 0.2214\n",
      "Epoch 236/500\n",
      "6680/6680 - 0s - loss: 0.2217\n",
      "Epoch 237/500\n",
      "6680/6680 - 0s - loss: 0.2214\n",
      "Epoch 238/500\n",
      "6680/6680 - 0s - loss: 0.2211\n",
      "Epoch 239/500\n",
      "6680/6680 - 0s - loss: 0.2210\n",
      "Epoch 240/500\n",
      "6680/6680 - 0s - loss: 0.2206\n",
      "Epoch 241/500\n",
      "6680/6680 - 0s - loss: 0.2211\n",
      "Epoch 242/500\n",
      "6680/6680 - 0s - loss: 0.2205\n",
      "Epoch 243/500\n",
      "6680/6680 - 0s - loss: 0.2211\n",
      "Epoch 244/500\n",
      "6680/6680 - 0s - loss: 0.2208\n",
      "Epoch 245/500\n",
      "6680/6680 - 0s - loss: 0.2212\n",
      "Epoch 246/500\n",
      "6680/6680 - 0s - loss: 0.2208\n",
      "Epoch 247/500\n",
      "6680/6680 - 0s - loss: 0.2207\n",
      "Epoch 248/500\n",
      "6680/6680 - 0s - loss: 0.2206\n",
      "Epoch 249/500\n",
      "6680/6680 - 0s - loss: 0.2202\n",
      "Epoch 250/500\n",
      "6680/6680 - 0s - loss: 0.2194\n",
      "Epoch 251/500\n",
      "6680/6680 - 0s - loss: 0.2202\n",
      "Epoch 252/500\n",
      "6680/6680 - 0s - loss: 0.2206\n",
      "Epoch 253/500\n",
      "6680/6680 - 0s - loss: 0.2194\n",
      "Epoch 254/500\n",
      "6680/6680 - 0s - loss: 0.2199\n",
      "Epoch 255/500\n",
      "6680/6680 - 0s - loss: 0.2203\n",
      "Epoch 256/500\n",
      "6680/6680 - 0s - loss: 0.2198\n",
      "Epoch 257/500\n",
      "6680/6680 - 0s - loss: 0.2199\n",
      "Epoch 258/500\n",
      "6680/6680 - 0s - loss: 0.2193\n",
      "Epoch 259/500\n",
      "6680/6680 - 0s - loss: 0.2197\n",
      "Epoch 260/500\n",
      "6680/6680 - 0s - loss: 0.2200\n",
      "Epoch 261/500\n",
      "6680/6680 - 0s - loss: 0.2191\n",
      "Epoch 262/500\n",
      "6680/6680 - 0s - loss: 0.2194\n",
      "Epoch 263/500\n",
      "6680/6680 - 0s - loss: 0.2198\n",
      "Epoch 264/500\n",
      "6680/6680 - 0s - loss: 0.2195\n",
      "Epoch 265/500\n",
      "6680/6680 - 0s - loss: 0.2195\n",
      "Epoch 266/500\n",
      "6680/6680 - 0s - loss: 0.2194\n",
      "Epoch 267/500\n",
      "6680/6680 - 0s - loss: 0.2190\n",
      "Epoch 268/500\n",
      "6680/6680 - 0s - loss: 0.2183\n",
      "Epoch 269/500\n",
      "6680/6680 - 0s - loss: 0.2198\n",
      "Epoch 270/500\n",
      "6680/6680 - 0s - loss: 0.2193\n",
      "Epoch 271/500\n",
      "6680/6680 - 0s - loss: 0.2191\n",
      "Epoch 272/500\n",
      "6680/6680 - 0s - loss: 0.2189\n",
      "Epoch 273/500\n",
      "6680/6680 - 0s - loss: 0.2188\n",
      "Epoch 274/500\n",
      "6680/6680 - 0s - loss: 0.2184\n",
      "Epoch 275/500\n",
      "6680/6680 - 0s - loss: 0.2187\n",
      "Epoch 276/500\n",
      "6680/6680 - 0s - loss: 0.2185\n",
      "Epoch 277/500\n",
      "6680/6680 - 0s - loss: 0.2184\n",
      "Epoch 278/500\n",
      "6680/6680 - 0s - loss: 0.2186\n",
      "Epoch 279/500\n",
      "6680/6680 - 0s - loss: 0.2181\n",
      "Epoch 280/500\n",
      "6680/6680 - 0s - loss: 0.2180\n",
      "Epoch 281/500\n",
      "6680/6680 - 0s - loss: 0.2185\n",
      "Epoch 282/500\n",
      "6680/6680 - 0s - loss: 0.2180\n",
      "Epoch 283/500\n",
      "6680/6680 - 0s - loss: 0.2186\n",
      "Epoch 284/500\n",
      "6680/6680 - 0s - loss: 0.2179\n",
      "Epoch 285/500\n",
      "6680/6680 - 0s - loss: 0.2180\n",
      "Epoch 286/500\n",
      "6680/6680 - 0s - loss: 0.2180\n",
      "Epoch 287/500\n",
      "6680/6680 - 0s - loss: 0.2174\n",
      "Epoch 288/500\n",
      "6680/6680 - 0s - loss: 0.2180\n",
      "Epoch 289/500\n",
      "6680/6680 - 0s - loss: 0.2178\n",
      "Epoch 290/500\n",
      "6680/6680 - 0s - loss: 0.2179\n",
      "Epoch 291/500\n",
      "6680/6680 - 0s - loss: 0.2180\n",
      "Epoch 292/500\n",
      "6680/6680 - 0s - loss: 0.2165\n",
      "Epoch 293/500\n",
      "6680/6680 - 0s - loss: 0.2177\n",
      "Epoch 294/500\n",
      "6680/6680 - 0s - loss: 0.2176\n",
      "Epoch 295/500\n",
      "6680/6680 - 0s - loss: 0.2172\n",
      "Epoch 296/500\n",
      "6680/6680 - 0s - loss: 0.2173\n",
      "Epoch 297/500\n",
      "6680/6680 - 0s - loss: 0.2177\n",
      "Epoch 298/500\n",
      "6680/6680 - 0s - loss: 0.2173\n",
      "Epoch 299/500\n",
      "6680/6680 - 0s - loss: 0.2171\n",
      "Epoch 300/500\n",
      "6680/6680 - 0s - loss: 0.2174\n",
      "Epoch 301/500\n",
      "6680/6680 - 0s - loss: 0.2168\n",
      "Epoch 302/500\n",
      "6680/6680 - 0s - loss: 0.2171\n",
      "Epoch 303/500\n",
      "6680/6680 - 0s - loss: 0.2174\n",
      "Epoch 304/500\n",
      "6680/6680 - 0s - loss: 0.2174\n",
      "Epoch 305/500\n",
      "6680/6680 - 0s - loss: 0.2170\n",
      "Epoch 306/500\n",
      "6680/6680 - 0s - loss: 0.2168\n",
      "Epoch 307/500\n",
      "6680/6680 - 0s - loss: 0.2164\n",
      "Epoch 308/500\n",
      "6680/6680 - 0s - loss: 0.2162\n",
      "Epoch 309/500\n",
      "6680/6680 - 0s - loss: 0.2171\n",
      "Epoch 310/500\n",
      "6680/6680 - 0s - loss: 0.2165\n",
      "Epoch 311/500\n",
      "6680/6680 - 0s - loss: 0.2164\n",
      "Epoch 312/500\n",
      "6680/6680 - 0s - loss: 0.2157\n",
      "Epoch 313/500\n",
      "6680/6680 - 0s - loss: 0.2170\n",
      "Epoch 314/500\n",
      "6680/6680 - 0s - loss: 0.2158\n",
      "Epoch 315/500\n",
      "6680/6680 - 0s - loss: 0.2166\n",
      "Epoch 316/500\n",
      "6680/6680 - 0s - loss: 0.2163\n",
      "Epoch 317/500\n",
      "6680/6680 - 0s - loss: 0.2163\n",
      "Epoch 318/500\n",
      "6680/6680 - 0s - loss: 0.2164\n",
      "Epoch 319/500\n",
      "6680/6680 - 0s - loss: 0.2165\n",
      "Epoch 320/500\n",
      "6680/6680 - 0s - loss: 0.2163\n",
      "Epoch 321/500\n",
      "6680/6680 - 0s - loss: 0.2161\n",
      "Epoch 322/500\n",
      "6680/6680 - 0s - loss: 0.2154\n",
      "Epoch 323/500\n",
      "6680/6680 - 0s - loss: 0.2162\n",
      "Epoch 324/500\n",
      "6680/6680 - 0s - loss: 0.2162\n",
      "Epoch 325/500\n",
      "6680/6680 - 0s - loss: 0.2155\n",
      "Epoch 326/500\n",
      "6680/6680 - 0s - loss: 0.2159\n",
      "Epoch 327/500\n",
      "6680/6680 - 0s - loss: 0.2152\n",
      "Epoch 328/500\n",
      "6680/6680 - 0s - loss: 0.2159\n",
      "Epoch 329/500\n",
      "6680/6680 - 0s - loss: 0.2150\n",
      "Epoch 330/500\n",
      "6680/6680 - 0s - loss: 0.2156\n",
      "Epoch 331/500\n",
      "6680/6680 - 0s - loss: 0.2162\n",
      "Epoch 332/500\n",
      "6680/6680 - 0s - loss: 0.2151\n",
      "Epoch 333/500\n",
      "6680/6680 - 0s - loss: 0.2153\n",
      "Epoch 334/500\n",
      "6680/6680 - 0s - loss: 0.2152\n",
      "Epoch 335/500\n",
      "6680/6680 - 0s - loss: 0.2154\n",
      "Epoch 336/500\n",
      "6680/6680 - 0s - loss: 0.2151\n",
      "Epoch 337/500\n",
      "6680/6680 - 0s - loss: 0.2153\n",
      "Epoch 338/500\n",
      "6680/6680 - 0s - loss: 0.2147\n",
      "Epoch 339/500\n",
      "6680/6680 - 0s - loss: 0.2146\n",
      "Epoch 340/500\n",
      "6680/6680 - 0s - loss: 0.2145\n",
      "Epoch 341/500\n",
      "6680/6680 - 0s - loss: 0.2151\n",
      "Epoch 342/500\n",
      "6680/6680 - 0s - loss: 0.2149\n",
      "Epoch 343/500\n",
      "6680/6680 - 0s - loss: 0.2147\n",
      "Epoch 344/500\n",
      "6680/6680 - 0s - loss: 0.2146\n",
      "Epoch 345/500\n",
      "6680/6680 - 0s - loss: 0.2149\n",
      "Epoch 346/500\n",
      "6680/6680 - 0s - loss: 0.2143\n",
      "Epoch 347/500\n",
      "6680/6680 - 0s - loss: 0.2146\n",
      "Epoch 348/500\n",
      "6680/6680 - 0s - loss: 0.2139\n",
      "Epoch 349/500\n",
      "6680/6680 - 0s - loss: 0.2145\n",
      "Epoch 350/500\n",
      "6680/6680 - 0s - loss: 0.2138\n",
      "Epoch 351/500\n",
      "6680/6680 - 0s - loss: 0.2147\n",
      "Epoch 352/500\n",
      "6680/6680 - 0s - loss: 0.2140\n",
      "Epoch 353/500\n",
      "6680/6680 - 0s - loss: 0.2137\n",
      "Epoch 354/500\n",
      "6680/6680 - 0s - loss: 0.2139\n",
      "Epoch 355/500\n",
      "6680/6680 - 0s - loss: 0.2138\n",
      "Epoch 356/500\n",
      "6680/6680 - 0s - loss: 0.2140\n",
      "Epoch 357/500\n",
      "6680/6680 - 0s - loss: 0.2130\n",
      "Epoch 358/500\n",
      "6680/6680 - 0s - loss: 0.2143\n",
      "Epoch 359/500\n",
      "6680/6680 - 0s - loss: 0.2137\n",
      "Epoch 360/500\n",
      "6680/6680 - 0s - loss: 0.2129\n",
      "Epoch 361/500\n",
      "6680/6680 - 0s - loss: 0.2132\n",
      "Epoch 362/500\n",
      "6680/6680 - 0s - loss: 0.2136\n",
      "Epoch 363/500\n",
      "6680/6680 - 0s - loss: 0.2134\n",
      "Epoch 364/500\n",
      "6680/6680 - 0s - loss: 0.2137\n",
      "Epoch 365/500\n",
      "6680/6680 - 0s - loss: 0.2130\n",
      "Epoch 366/500\n",
      "6680/6680 - 0s - loss: 0.2125\n",
      "Epoch 367/500\n",
      "6680/6680 - 0s - loss: 0.2134\n",
      "Epoch 368/500\n",
      "6680/6680 - 0s - loss: 0.2132\n",
      "Epoch 369/500\n",
      "6680/6680 - 0s - loss: 0.2128\n",
      "Epoch 370/500\n",
      "6680/6680 - 0s - loss: 0.2126\n",
      "Epoch 371/500\n",
      "6680/6680 - 0s - loss: 0.2130\n",
      "Epoch 372/500\n",
      "6680/6680 - 0s - loss: 0.2129\n",
      "Epoch 373/500\n",
      "6680/6680 - 0s - loss: 0.2133\n",
      "Epoch 374/500\n",
      "6680/6680 - 0s - loss: 0.2135\n",
      "Epoch 375/500\n",
      "6680/6680 - 0s - loss: 0.2124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376/500\n",
      "6680/6680 - 0s - loss: 0.2122\n",
      "Epoch 377/500\n",
      "6680/6680 - 0s - loss: 0.2117\n",
      "Epoch 378/500\n",
      "6680/6680 - 0s - loss: 0.2126\n",
      "Epoch 379/500\n",
      "6680/6680 - 0s - loss: 0.2126\n",
      "Epoch 380/500\n",
      "6680/6680 - 0s - loss: 0.2130\n",
      "Epoch 381/500\n",
      "6680/6680 - 0s - loss: 0.2127\n",
      "Epoch 382/500\n",
      "6680/6680 - 0s - loss: 0.2122\n",
      "Epoch 383/500\n",
      "6680/6680 - 0s - loss: 0.2120\n",
      "Epoch 384/500\n",
      "6680/6680 - 0s - loss: 0.2122\n",
      "Epoch 385/500\n",
      "6680/6680 - 0s - loss: 0.2124\n",
      "Epoch 386/500\n",
      "6680/6680 - 0s - loss: 0.2125\n",
      "Epoch 387/500\n",
      "6680/6680 - 0s - loss: 0.2122\n",
      "Epoch 388/500\n",
      "6680/6680 - 0s - loss: 0.2125\n",
      "Epoch 389/500\n",
      "6680/6680 - 0s - loss: 0.2116\n",
      "Epoch 390/500\n",
      "6680/6680 - 0s - loss: 0.2118\n",
      "Epoch 391/500\n",
      "6680/6680 - 0s - loss: 0.2112\n",
      "Epoch 392/500\n",
      "6680/6680 - 0s - loss: 0.2123\n",
      "Epoch 393/500\n",
      "6680/6680 - 0s - loss: 0.2120\n",
      "Epoch 394/500\n",
      "6680/6680 - 0s - loss: 0.2111\n",
      "Epoch 395/500\n",
      "6680/6680 - 0s - loss: 0.2119\n",
      "Epoch 396/500\n",
      "6680/6680 - 0s - loss: 0.2114\n",
      "Epoch 397/500\n",
      "6680/6680 - 0s - loss: 0.2116\n",
      "Epoch 398/500\n",
      "6680/6680 - 0s - loss: 0.2118\n",
      "Epoch 399/500\n",
      "6680/6680 - 0s - loss: 0.2115\n",
      "Epoch 400/500\n",
      "6680/6680 - 0s - loss: 0.2121\n",
      "Epoch 401/500\n",
      "6680/6680 - 0s - loss: 0.2113\n",
      "Epoch 402/500\n",
      "6680/6680 - 0s - loss: 0.2114\n",
      "Epoch 403/500\n",
      "6680/6680 - 0s - loss: 0.2111\n",
      "Epoch 404/500\n",
      "6680/6680 - 0s - loss: 0.2117\n",
      "Epoch 405/500\n",
      "6680/6680 - 0s - loss: 0.2113\n",
      "Epoch 406/500\n",
      "6680/6680 - 0s - loss: 0.2109\n",
      "Epoch 407/500\n",
      "6680/6680 - 0s - loss: 0.2113\n",
      "Epoch 408/500\n",
      "6680/6680 - 0s - loss: 0.2105\n",
      "Epoch 409/500\n",
      "6680/6680 - 0s - loss: 0.2117\n",
      "Epoch 410/500\n",
      "6680/6680 - 0s - loss: 0.2108\n",
      "Epoch 411/500\n",
      "6680/6680 - 0s - loss: 0.2113\n",
      "Epoch 412/500\n",
      "6680/6680 - 0s - loss: 0.2112\n",
      "Epoch 413/500\n",
      "6680/6680 - 0s - loss: 0.2110\n",
      "Epoch 414/500\n",
      "6680/6680 - 0s - loss: 0.2108\n",
      "Epoch 415/500\n",
      "6680/6680 - 0s - loss: 0.2112\n",
      "Epoch 416/500\n",
      "6680/6680 - 0s - loss: 0.2106\n",
      "Epoch 417/500\n",
      "6680/6680 - 0s - loss: 0.2105\n",
      "Epoch 418/500\n",
      "6680/6680 - 0s - loss: 0.2112\n",
      "Epoch 419/500\n",
      "6680/6680 - 0s - loss: 0.2104\n",
      "Epoch 420/500\n",
      "6680/6680 - 0s - loss: 0.2107\n",
      "Epoch 421/500\n",
      "6680/6680 - 0s - loss: 0.2107\n",
      "Epoch 422/500\n",
      "6680/6680 - 0s - loss: 0.2105\n",
      "Epoch 423/500\n",
      "6680/6680 - 0s - loss: 0.2106\n",
      "Epoch 424/500\n",
      "6680/6680 - 0s - loss: 0.2103\n",
      "Epoch 425/500\n",
      "6680/6680 - 0s - loss: 0.2102\n",
      "Epoch 426/500\n",
      "6680/6680 - 0s - loss: 0.2112\n",
      "Epoch 427/500\n",
      "6680/6680 - 0s - loss: 0.2105\n",
      "Epoch 428/500\n",
      "6680/6680 - 0s - loss: 0.2107\n",
      "Epoch 429/500\n",
      "6680/6680 - 0s - loss: 0.2100\n",
      "Epoch 430/500\n",
      "6680/6680 - 0s - loss: 0.2111\n",
      "Epoch 431/500\n",
      "6680/6680 - 0s - loss: 0.2105\n",
      "Epoch 432/500\n",
      "6680/6680 - 0s - loss: 0.2101\n",
      "Epoch 433/500\n",
      "6680/6680 - 0s - loss: 0.2104\n",
      "Epoch 434/500\n",
      "6680/6680 - 0s - loss: 0.2097\n",
      "Epoch 435/500\n",
      "6680/6680 - 0s - loss: 0.2099\n",
      "Epoch 436/500\n",
      "6680/6680 - 0s - loss: 0.2104\n",
      "Epoch 437/500\n",
      "6680/6680 - 0s - loss: 0.2104\n",
      "Epoch 438/500\n",
      "6680/6680 - 0s - loss: 0.2103\n",
      "Epoch 439/500\n",
      "6680/6680 - 0s - loss: 0.2099\n",
      "Epoch 440/500\n",
      "6680/6680 - 0s - loss: 0.2104\n",
      "Epoch 441/500\n",
      "6680/6680 - 0s - loss: 0.2106\n",
      "Epoch 442/500\n",
      "6680/6680 - 0s - loss: 0.2098\n",
      "Epoch 443/500\n",
      "6680/6680 - 0s - loss: 0.2098\n",
      "Epoch 444/500\n",
      "6680/6680 - 0s - loss: 0.2094\n",
      "Epoch 445/500\n",
      "6680/6680 - 0s - loss: 0.2096\n",
      "Epoch 446/500\n",
      "6680/6680 - 0s - loss: 0.2096\n",
      "Epoch 447/500\n",
      "6680/6680 - 0s - loss: 0.2105\n",
      "Epoch 448/500\n",
      "6680/6680 - 0s - loss: 0.2095\n",
      "Epoch 449/500\n",
      "6680/6680 - 0s - loss: 0.2099\n",
      "Epoch 450/500\n",
      "6680/6680 - 0s - loss: 0.2098\n",
      "Epoch 451/500\n",
      "6680/6680 - 0s - loss: 0.2088\n",
      "Epoch 452/500\n",
      "6680/6680 - 0s - loss: 0.2093\n",
      "Epoch 453/500\n",
      "6680/6680 - 0s - loss: 0.2094\n",
      "Epoch 454/500\n",
      "6680/6680 - 0s - loss: 0.2094\n",
      "Epoch 455/500\n",
      "6680/6680 - 0s - loss: 0.2096\n",
      "Epoch 456/500\n",
      "6680/6680 - 0s - loss: 0.2088\n",
      "Epoch 457/500\n",
      "6680/6680 - 0s - loss: 0.2098\n",
      "Epoch 458/500\n",
      "6680/6680 - 0s - loss: 0.2090\n",
      "Epoch 459/500\n",
      "6680/6680 - 0s - loss: 0.2099\n",
      "Epoch 460/500\n",
      "6680/6680 - 0s - loss: 0.2094\n",
      "Epoch 461/500\n",
      "6680/6680 - 0s - loss: 0.2101\n",
      "Epoch 462/500\n",
      "6680/6680 - 0s - loss: 0.2091\n",
      "Epoch 463/500\n",
      "6680/6680 - 0s - loss: 0.2091\n",
      "Epoch 464/500\n",
      "6680/6680 - 0s - loss: 0.2096\n",
      "Epoch 465/500\n",
      "6680/6680 - 0s - loss: 0.2088\n",
      "Epoch 466/500\n",
      "6680/6680 - 0s - loss: 0.2095\n",
      "Epoch 467/500\n",
      "6680/6680 - 0s - loss: 0.2096\n",
      "Epoch 468/500\n",
      "6680/6680 - 0s - loss: 0.2086\n",
      "Epoch 469/500\n",
      "6680/6680 - 0s - loss: 0.2090\n",
      "Epoch 470/500\n",
      "6680/6680 - 0s - loss: 0.2089\n",
      "Epoch 471/500\n",
      "6680/6680 - 0s - loss: 0.2096\n",
      "Epoch 472/500\n",
      "6680/6680 - 0s - loss: 0.2087\n",
      "Epoch 473/500\n",
      "6680/6680 - 0s - loss: 0.2089\n",
      "Epoch 474/500\n",
      "6680/6680 - 0s - loss: 0.2088\n",
      "Epoch 475/500\n",
      "6680/6680 - 0s - loss: 0.2092\n",
      "Epoch 476/500\n",
      "6680/6680 - 0s - loss: 0.2085\n",
      "Epoch 477/500\n",
      "6680/6680 - 0s - loss: 0.2084\n",
      "Epoch 478/500\n",
      "6680/6680 - 0s - loss: 0.2087\n",
      "Epoch 479/500\n",
      "6680/6680 - 0s - loss: 0.2081\n",
      "Epoch 480/500\n",
      "6680/6680 - 0s - loss: 0.2089\n",
      "Epoch 481/500\n",
      "6680/6680 - 0s - loss: 0.2087\n",
      "Epoch 482/500\n",
      "6680/6680 - 0s - loss: 0.2089\n",
      "Epoch 483/500\n",
      "6680/6680 - 0s - loss: 0.2087\n",
      "Epoch 484/500\n",
      "6680/6680 - 0s - loss: 0.2084\n",
      "Epoch 485/500\n",
      "6680/6680 - 0s - loss: 0.2089\n",
      "Epoch 486/500\n",
      "6680/6680 - 0s - loss: 0.2084\n",
      "Epoch 487/500\n",
      "6680/6680 - 0s - loss: 0.2093\n",
      "Epoch 488/500\n",
      "6680/6680 - 0s - loss: 0.2086\n",
      "Epoch 489/500\n",
      "6680/6680 - 0s - loss: 0.2085\n",
      "Epoch 490/500\n",
      "6680/6680 - 0s - loss: 0.2080\n",
      "Epoch 491/500\n",
      "6680/6680 - 0s - loss: 0.2084\n",
      "Epoch 492/500\n",
      "6680/6680 - 0s - loss: 0.2083\n",
      "Epoch 493/500\n",
      "6680/6680 - 0s - loss: 0.2085\n",
      "Epoch 494/500\n",
      "6680/6680 - 0s - loss: 0.2083\n",
      "Epoch 495/500\n",
      "6680/6680 - 0s - loss: 0.2081\n",
      "Epoch 496/500\n",
      "6680/6680 - 0s - loss: 0.2083\n",
      "Epoch 497/500\n",
      "6680/6680 - 0s - loss: 0.2080\n",
      "Epoch 498/500\n",
      "6680/6680 - 0s - loss: 0.2082\n",
      "Epoch 499/500\n",
      "6680/6680 - 0s - loss: 0.2083\n",
      "Epoch 500/500\n",
      "6680/6680 - 0s - loss: 0.2082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f474ce9fc8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow import squeeze\n",
    "\n",
    "optimizer = get(algorithm).from_config({'learning_rate':best_lr})\n",
    "model = getMoldel(neurons, activation=best_activation, optimizer=optimizer)\n",
    "model.fit(X, y,\n",
    "          epochs=500,\n",
    "          batch_size=32,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión alcanzada por el modelo en los datos de prueba es de 84.67%\n",
      "Esta estimación posee un 3.4% de precisión con una confianza del 95.79%\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "binAccuracy = binary_accuracy(y_test, squeeze(model.predict(X_test)))\n",
    "print('La precisión alcanzada por el modelo en los datos de prueba es de %.2f%%' % (binAccuracy*100))\n",
    "\n",
    "precision = 0.034\n",
    "confianza = (1 - (2*math.exp(-2*(precision**2)*total_test_data))) *100\n",
    "\n",
    "print('Esta estimación posee un %.1f%% de precisión con una confianza del %.2f%%' % (precision*100, confianza))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con los parámetros anteriormente descritos, se construye un modelo multicapa para mejorar la precisión del modelo contruido hasta ahora. Con este objetivo en mente, se entrenan diversos moldelos variando la cantidad de neuronas en la capa siguiente y escoge la arquitectura con mejor rendimiento en los datos de validación. Se repite este proceso para un total de cuatro capas escondidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStop = EarlyStopping(min_delta=1e-9, patience=1)\n",
    "\n",
    "def getMultiLayerMoldel(neurons=[neurons], activation=best_activation, optimizer=algorithm, learning_rate=best_lr):\n",
    "    model = Sequential()\n",
    "    units = neurons.pop(0)\n",
    "    model.add(Dense(units=units, activation=activation, input_dim=X.shape[1]))\n",
    "    \n",
    "    optimizer = get(optimizer).from_config({'learning_rate':learning_rate})\n",
    "    \n",
    "    for units in neurons:\n",
    "        model.add(Dense(units=units, activation=activation))\n",
    "        \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 2s - loss: 0.5901 - val_loss: 0.5260\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.4978 - val_loss: 0.4878\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.4599 - val_loss: 0.4604\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.4316 - val_loss: 0.4367\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.4082 - val_loss: 0.4160\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3889 - val_loss: 0.4089\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3737 - val_loss: 0.3905\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3615 - val_loss: 0.3833\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3505 - val_loss: 0.3749\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3424 - val_loss: 0.3695\n",
      "Epoch 11/2000\n",
      "6012/6012 - 0s - loss: 0.3351 - val_loss: 0.3626\n",
      "Epoch 12/2000\n",
      "6012/6012 - 0s - loss: 0.3293 - val_loss: 0.3591\n",
      "Epoch 13/2000\n",
      "6012/6012 - 0s - loss: 0.3237 - val_loss: 0.3590\n",
      "Epoch 14/2000\n",
      "6012/6012 - 0s - loss: 0.3194 - val_loss: 0.3533\n",
      "Epoch 15/2000\n",
      "6012/6012 - 0s - loss: 0.3145 - val_loss: 0.3538\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 2s - loss: 0.5999 - val_loss: 0.5207\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.4945 - val_loss: 0.4344\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.4113 - val_loss: 0.3843\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3766 - val_loss: 0.3680\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3613 - val_loss: 0.3568\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3509 - val_loss: 0.3472\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3421 - val_loss: 0.3412\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3354 - val_loss: 0.3364\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3289 - val_loss: 0.3335\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3229 - val_loss: 0.3277\n",
      "Epoch 11/2000\n",
      "6012/6012 - 0s - loss: 0.3191 - val_loss: 0.3291\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 2s - loss: 0.5020 - val_loss: 0.3952\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3979 - val_loss: 0.3718\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3732 - val_loss: 0.3636\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3599 - val_loss: 0.3533\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3496 - val_loss: 0.3489\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3409 - val_loss: 0.3436\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3347 - val_loss: 0.3467\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5310 - val_loss: 0.4162\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.4032 - val_loss: 0.3736\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3717 - val_loss: 0.3553\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3570 - val_loss: 0.3474\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3470 - val_loss: 0.3447\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3386 - val_loss: 0.3380\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3317 - val_loss: 0.3357\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3253 - val_loss: 0.3301\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3195 - val_loss: 0.3290\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3152 - val_loss: 0.3294\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.6069 - val_loss: 0.4839\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.4167 - val_loss: 0.3752\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3707 - val_loss: 0.3592\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3560 - val_loss: 0.3564\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3461 - val_loss: 0.3538\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3384 - val_loss: 0.3509\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3322 - val_loss: 0.3490\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3265 - val_loss: 0.3484\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3218 - val_loss: 0.3475\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3173 - val_loss: 0.3452\n",
      "Epoch 11/2000\n",
      "6012/6012 - 0s - loss: 0.3130 - val_loss: 0.3403\n",
      "Epoch 12/2000\n",
      "6012/6012 - 0s - loss: 0.3087 - val_loss: 0.3411\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5396 - val_loss: 0.4088\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3919 - val_loss: 0.3748\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3626 - val_loss: 0.3649\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3468 - val_loss: 0.3582\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3360 - val_loss: 0.3543\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3278 - val_loss: 0.3445\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3210 - val_loss: 0.3418\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3150 - val_loss: 0.3367\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3102 - val_loss: 0.3344\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3051 - val_loss: 0.3343\n",
      "Epoch 11/2000\n",
      "6012/6012 - 0s - loss: 0.3004 - val_loss: 0.3279\n",
      "Epoch 12/2000\n",
      "6012/6012 - 0s - loss: 0.2963 - val_loss: 0.3371\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5181 - val_loss: 0.4259\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3995 - val_loss: 0.3664\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3688 - val_loss: 0.3501\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3532 - val_loss: 0.3432\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3427 - val_loss: 0.3350\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3336 - val_loss: 0.3286\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3263 - val_loss: 0.3245\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3199 - val_loss: 0.3245\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.4863 - val_loss: 0.3852\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3861 - val_loss: 0.3644\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3647 - val_loss: 0.3535\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3514 - val_loss: 0.3495\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3409 - val_loss: 0.3506\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.4759 - val_loss: 0.3862\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3736 - val_loss: 0.3586\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3536 - val_loss: 0.3443\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3416 - val_loss: 0.3381\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3333 - val_loss: 0.3293\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3268 - val_loss: 0.3276\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3203 - val_loss: 0.3238\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3155 - val_loss: 0.3256\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.4898 - val_loss: 0.4046\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3925 - val_loss: 0.3770\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3690 - val_loss: 0.3690\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3557 - val_loss: 0.3615\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3461 - val_loss: 0.3535\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3377 - val_loss: 0.3482\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3311 - val_loss: 0.3436\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3250 - val_loss: 0.3409\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3190 - val_loss: 0.3376\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3132 - val_loss: 0.3382\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 2s - loss: 0.5039 - val_loss: 0.3822\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3782 - val_loss: 0.3616\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3553 - val_loss: 0.3477\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3426 - val_loss: 0.3401\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3331 - val_loss: 0.3331\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3266 - val_loss: 0.3307\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3198 - val_loss: 0.3284\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3147 - val_loss: 0.3258\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3094 - val_loss: 0.3245\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3053 - val_loss: 0.3236\n",
      "Epoch 11/2000\n",
      "6012/6012 - 0s - loss: 0.3006 - val_loss: 0.3237\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.4681 - val_loss: 0.4019\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3831 - val_loss: 0.3615\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3571 - val_loss: 0.3409\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3427 - val_loss: 0.3283\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3340 - val_loss: 0.3259\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3272 - val_loss: 0.3168\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3206 - val_loss: 0.3153\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3144 - val_loss: 0.3186\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5237 - val_loss: 0.3998\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3846 - val_loss: 0.3649\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3624 - val_loss: 0.3525\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3504 - val_loss: 0.3450\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3422 - val_loss: 0.3422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3351 - val_loss: 0.3379\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3296 - val_loss: 0.3421\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5016 - val_loss: 0.3858\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3790 - val_loss: 0.3610\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3563 - val_loss: 0.3500\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3440 - val_loss: 0.3430\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3339 - val_loss: 0.3397\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3269 - val_loss: 0.3339\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3192 - val_loss: 0.3308\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3121 - val_loss: 0.3254\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3057 - val_loss: 0.3216\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3011 - val_loss: 0.3211\n",
      "Epoch 11/2000\n",
      "6012/6012 - 0s - loss: 0.2964 - val_loss: 0.3215\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.4982 - val_loss: 0.3961\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3733 - val_loss: 0.3603\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3507 - val_loss: 0.3476\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3375 - val_loss: 0.3382\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3285 - val_loss: 0.3337\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3211 - val_loss: 0.3291\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3144 - val_loss: 0.3250\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3097 - val_loss: 0.3242\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3045 - val_loss: 0.3227\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3000 - val_loss: 0.3177\n",
      "Epoch 11/2000\n",
      "6012/6012 - 0s - loss: 0.2955 - val_loss: 0.3216\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.4964 - val_loss: 0.4146\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3821 - val_loss: 0.3768\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3588 - val_loss: 0.3649\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3462 - val_loss: 0.3616\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3361 - val_loss: 0.3568\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3286 - val_loss: 0.3504\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3218 - val_loss: 0.3481\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3151 - val_loss: 0.3494\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5000 - val_loss: 0.4024\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3785 - val_loss: 0.3703\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3565 - val_loss: 0.3536\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3435 - val_loss: 0.3422\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3336 - val_loss: 0.3337\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3264 - val_loss: 0.3298\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3187 - val_loss: 0.3302\n",
      "El modelo con mejor desempeño tiene 26 neuronas en la segunda capa oculta\n"
     ]
    }
   ],
   "source": [
    "multi_models_log_loss_valid = []\n",
    "\n",
    "for units in range(2,neurons):\n",
    "\n",
    "    model = getMultiLayerMoldel(neurons = [neurons, units])    \n",
    "    history_data = model.fit(X_train, y_train,\n",
    "                             validation_data=(X_validation,y_validation),\n",
    "                             epochs=2000,\n",
    "                             batch_size=32,\n",
    "                             verbose=2,\n",
    "                             callbacks=[earlyStop])\n",
    "\n",
    "    multi_models_log_loss_valid.append(history_data.history['val_loss'][-1])\n",
    "\n",
    "best_model_multi_position = multi_models_log_loss_valid.index(min(multi_models_log_loss_valid))\n",
    "best_model_multi_neurons = best_model_multi_position + 2\n",
    "print(\"El modelo con mejor desempeño tiene %d neuronas en la segunda capa oculta\" % best_model_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo con mejor desempeño tiene 13 neuronas en la segunda capa oculta\n"
     ]
    }
   ],
   "source": [
    "plot_model_info(range(2,neurons),multi_models_log_loss_valid,'Neuronas en la segunda capa')\n",
    "print(\"El modelo con mejor desempeño tiene %d neuronas en la segunda capa oculta\" % best_model_multi_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5443 - val_loss: 0.4162\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3947 - val_loss: 0.3867\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3702 - val_loss: 0.3737\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3560 - val_loss: 0.3597\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3453 - val_loss: 0.3545\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3350 - val_loss: 0.3497\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3284 - val_loss: 0.3461\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3207 - val_loss: 0.3446\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3149 - val_loss: 0.3432\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3086 - val_loss: 0.3355\n",
      "Epoch 11/2000\n",
      "6012/6012 - 0s - loss: 0.3044 - val_loss: 0.3404\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5837 - val_loss: 0.4432\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3951 - val_loss: 0.3984\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3674 - val_loss: 0.3768\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3518 - val_loss: 0.3661\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3407 - val_loss: 0.3577\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3313 - val_loss: 0.3538\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3258 - val_loss: 0.3454\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3200 - val_loss: 0.3429\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3146 - val_loss: 0.3463\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5570 - val_loss: 0.4291\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.4096 - val_loss: 0.3805\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3778 - val_loss: 0.3617\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3620 - val_loss: 0.3516\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3501 - val_loss: 0.3432\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3418 - val_loss: 0.3431\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3338 - val_loss: 0.3357\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3263 - val_loss: 0.3424\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.6320 - val_loss: 0.4974\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.4192 - val_loss: 0.3771\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3684 - val_loss: 0.3556\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3512 - val_loss: 0.3453\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3403 - val_loss: 0.3354\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3306 - val_loss: 0.3368\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5594 - val_loss: 0.4202\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3907 - val_loss: 0.3748\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3642 - val_loss: 0.3633\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3515 - val_loss: 0.3569\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3430 - val_loss: 0.3506\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3346 - val_loss: 0.3448\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3278 - val_loss: 0.3434\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3212 - val_loss: 0.3414\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3163 - val_loss: 0.3379\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3098 - val_loss: 0.3360\n",
      "Epoch 11/2000\n",
      "6012/6012 - 0s - loss: 0.3060 - val_loss: 0.3396\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.4648 - val_loss: 0.3710\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3728 - val_loss: 0.3429\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3523 - val_loss: 0.3317\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3378 - val_loss: 0.3258\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3272 - val_loss: 0.3169\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3189 - val_loss: 0.3182\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 2s - loss: 0.5371 - val_loss: 0.3863\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3739 - val_loss: 0.3640\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3550 - val_loss: 0.3580\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3429 - val_loss: 0.3520\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3348 - val_loss: 0.3544\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5048 - val_loss: 0.3821\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3897 - val_loss: 0.3586\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3673 - val_loss: 0.3499\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3541 - val_loss: 0.3464\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3431 - val_loss: 0.3418\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3344 - val_loss: 0.3400\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3267 - val_loss: 0.3365\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3221 - val_loss: 0.3322\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3161 - val_loss: 0.3348\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5104 - val_loss: 0.4220\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3885 - val_loss: 0.3713\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3605 - val_loss: 0.3566\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3440 - val_loss: 0.3574\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.4949 - val_loss: 0.3862\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3919 - val_loss: 0.3602\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3658 - val_loss: 0.3511\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3496 - val_loss: 0.3399\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3380 - val_loss: 0.3319\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3286 - val_loss: 0.3304\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3216 - val_loss: 0.3278\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3140 - val_loss: 0.3247\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3079 - val_loss: 0.3214\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3031 - val_loss: 0.3229\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 2s - loss: 0.4859 - val_loss: 0.3869\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3854 - val_loss: 0.3609\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3638 - val_loss: 0.3446\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3513 - val_loss: 0.3417\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3422 - val_loss: 0.3359\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3352 - val_loss: 0.3314\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3283 - val_loss: 0.3258\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3220 - val_loss: 0.3274\n",
      "El modelo con mejor desempeño tiene 7 neuronas en la segunda capa oculta\n"
     ]
    }
   ],
   "source": [
    "trilayered_models_log_loss_valid = []\n",
    "\n",
    "for units in range(2, best_model_multi_neurons):\n",
    "\n",
    "    model = getMultiLayerMoldel(neurons = [neurons, best_model_multi_neurons, units])    \n",
    "    history_data = model.fit(X_train, y_train,\n",
    "                             validation_data=(X_validation,y_validation),\n",
    "                             epochs=2000,\n",
    "                             batch_size=32,\n",
    "                             verbose=2,\n",
    "                             callbacks=[earlyStop])\n",
    "\n",
    "    trilayered_models_log_loss_valid.append(history_data.history['val_loss'][-1])\n",
    "\n",
    "best_model_tri_position = trilayered_models_log_loss_valid.index(min(trilayered_models_log_loss_valid))\n",
    "best_model_tri_neurons = best_model_tri_position + 2\n",
    "print(\"El modelo con mejor desempeño tiene %d neuronas en la tercera capa oculta\" % best_model_tri_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo con mejor desempeño tiene 7 neuronas en la tercera capa oculta\n"
     ]
    }
   ],
   "source": [
    "plot_model_info(range(2,best_model_multi_neurons),trilayered_models_log_loss_valid,'Neuronas en la segunda capa')\n",
    "print(\"El modelo con mejor desempeño tiene %d neuronas en la tercera capa oculta\" % best_model_tri_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 2s - loss: 0.5931 - val_loss: 0.5070\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.4388 - val_loss: 0.3794\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3750 - val_loss: 0.3571\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3588 - val_loss: 0.3465\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3484 - val_loss: 0.3445\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3403 - val_loss: 0.3399\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3348 - val_loss: 0.3343\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3296 - val_loss: 0.3359\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 2s - loss: 0.6374 - val_loss: 0.5470\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.5340 - val_loss: 0.4907\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.4927 - val_loss: 0.4612\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.4651 - val_loss: 0.4401\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.4414 - val_loss: 0.4234\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.4226 - val_loss: 0.4076\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.4066 - val_loss: 0.4023\n",
      "Epoch 8/2000\n",
      "6012/6012 - 0s - loss: 0.3933 - val_loss: 0.3868\n",
      "Epoch 9/2000\n",
      "6012/6012 - 0s - loss: 0.3815 - val_loss: 0.3799\n",
      "Epoch 10/2000\n",
      "6012/6012 - 0s - loss: 0.3721 - val_loss: 0.3768\n",
      "Epoch 11/2000\n",
      "6012/6012 - 0s - loss: 0.3632 - val_loss: 0.3630\n",
      "Epoch 12/2000\n",
      "6012/6012 - 0s - loss: 0.3546 - val_loss: 0.3630\n",
      "Epoch 13/2000\n",
      "6012/6012 - 0s - loss: 0.3474 - val_loss: 0.3594\n",
      "Epoch 14/2000\n",
      "6012/6012 - 0s - loss: 0.3392 - val_loss: 0.3522\n",
      "Epoch 15/2000\n",
      "6012/6012 - 0s - loss: 0.3340 - val_loss: 0.3431\n",
      "Epoch 16/2000\n",
      "6012/6012 - 0s - loss: 0.3274 - val_loss: 0.3379\n",
      "Epoch 17/2000\n",
      "6012/6012 - 0s - loss: 0.3224 - val_loss: 0.3299\n",
      "Epoch 18/2000\n",
      "6012/6012 - 0s - loss: 0.3167 - val_loss: 0.3290\n",
      "Epoch 19/2000\n",
      "6012/6012 - 0s - loss: 0.3124 - val_loss: 0.3243\n",
      "Epoch 20/2000\n",
      "6012/6012 - 0s - loss: 0.3084 - val_loss: 0.3269\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 1s - loss: 0.5326 - val_loss: 0.4255\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3899 - val_loss: 0.3764\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3627 - val_loss: 0.3640\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3494 - val_loss: 0.3633\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3382 - val_loss: 0.3488\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3310 - val_loss: 0.3519\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 3s - loss: 0.5453 - val_loss: 0.4020\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3969 - val_loss: 0.3646\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3655 - val_loss: 0.3510\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3491 - val_loss: 0.3478\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3379 - val_loss: 0.3348\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3284 - val_loss: 0.3322\n",
      "Epoch 7/2000\n",
      "6012/6012 - 0s - loss: 0.3212 - val_loss: 0.3327\n",
      "Train on 6012 samples, validate on 668 samples\n",
      "Epoch 1/2000\n",
      "6012/6012 - 2s - loss: 0.5341 - val_loss: 0.4064\n",
      "Epoch 2/2000\n",
      "6012/6012 - 0s - loss: 0.3876 - val_loss: 0.3667\n",
      "Epoch 3/2000\n",
      "6012/6012 - 0s - loss: 0.3604 - val_loss: 0.3537\n",
      "Epoch 4/2000\n",
      "6012/6012 - 0s - loss: 0.3463 - val_loss: 0.3500\n",
      "Epoch 5/2000\n",
      "6012/6012 - 0s - loss: 0.3362 - val_loss: 0.3419\n",
      "Epoch 6/2000\n",
      "6012/6012 - 0s - loss: 0.3288 - val_loss: 0.3422\n",
      "El modelo con mejor desempeño tiene 3 neuronas en la cuarta capa oculta\n"
     ]
    }
   ],
   "source": [
    "four_layered_models_log_loss_valid = []\n",
    "\n",
    "for units in range(2, best_model_tri_neurons):\n",
    "\n",
    "    model = getMultiLayerMoldel(neurons = [neurons, best_model_multi_neurons, best_model_tri_neurons, units])    \n",
    "    history_data = model.fit(X_train, y_train,\n",
    "                             validation_data=(X_validation,y_validation),\n",
    "                             epochs=2000,\n",
    "                             batch_size=32,\n",
    "                             verbose=2,\n",
    "                             callbacks=[earlyStop])\n",
    "\n",
    "    four_layered_models_log_loss_valid.append(history_data.history['val_loss'][-1])\n",
    "\n",
    "best_model_four_position = four_layered_models_log_loss_valid.index(min(four_layered_models_log_loss_valid))\n",
    "best_model_four_neurons = best_model_four_position + 2\n",
    "print(\"El modelo con mejor desempeño tiene %d neuronas en la cuarta capa oculta\" % best_model_four_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo con mejor desempeño tiene 3 neuronas en la cuarta capa oculta\n"
     ]
    }
   ],
   "source": [
    "plot_model_info(range(2,best_model_tri_neurons),four_layered_models_log_loss_valid,'Neuronas en la segunda capa')\n",
    "print(\"El modelo con mejor desempeño tiene %d neuronas en la cuarta capa oculta\" % best_model_four_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Una vez obtenidas las neuronas en cada capa que poseen el mejor el rendimiento, se evalua una red neuronal entrenada en la totalidad de datos para este fin y se prueba cada arquitectura (dos, tres y cuatro capaz) en los datos separados para prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples\n",
      "Epoch 1/70\n",
      "6680/6680 [==============================] - 2s 269us/sample - loss: 0.5099\n",
      "Epoch 2/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.3866\n",
      "Epoch 3/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.3619\n",
      "Epoch 4/70\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.3490\n",
      "Epoch 5/70\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.3388\n",
      "Epoch 6/70\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.3300\n",
      "Epoch 7/70\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.3227\n",
      "Epoch 8/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.3170\n",
      "Epoch 9/70\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.3129\n",
      "Epoch 10/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.3079\n",
      "Epoch 11/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.3035\n",
      "Epoch 12/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.3010\n",
      "Epoch 13/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2965\n",
      "Epoch 14/70\n",
      "6680/6680 [==============================] - 0s 75us/sample - loss: 0.2921\n",
      "Epoch 15/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2896\n",
      "Epoch 16/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2874\n",
      "Epoch 17/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2836\n",
      "Epoch 18/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2809\n",
      "Epoch 19/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2784\n",
      "Epoch 20/70\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2760\n",
      "Epoch 21/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2734\n",
      "Epoch 22/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2713\n",
      "Epoch 23/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2696\n",
      "Epoch 24/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2666\n",
      "Epoch 25/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2640\n",
      "Epoch 26/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2623\n",
      "Epoch 27/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2610\n",
      "Epoch 28/70\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2588\n",
      "Epoch 29/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2572\n",
      "Epoch 30/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2547\n",
      "Epoch 31/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2540\n",
      "Epoch 32/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2521\n",
      "Epoch 33/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2500\n",
      "Epoch 34/70\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2495\n",
      "Epoch 35/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2477\n",
      "Epoch 36/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2454\n",
      "Epoch 37/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2458\n",
      "Epoch 38/70\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2432\n",
      "Epoch 39/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2418\n",
      "Epoch 40/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2414\n",
      "Epoch 41/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2395\n",
      "Epoch 42/70\n",
      "6680/6680 [==============================] - 1s 77us/sample - loss: 0.2394\n",
      "Epoch 43/70\n",
      "6680/6680 [==============================] - 1s 77us/sample - loss: 0.2379\n",
      "Epoch 44/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2364\n",
      "Epoch 45/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2355\n",
      "Epoch 46/70\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2345\n",
      "Epoch 47/70\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2336\n",
      "Epoch 48/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2317\n",
      "Epoch 49/70\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2320\n",
      "Epoch 50/70\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.2297\n",
      "Epoch 51/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2296\n",
      "Epoch 52/70\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2289\n",
      "Epoch 53/70\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2275\n",
      "Epoch 54/70\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.2262\n",
      "Epoch 55/70\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.2260\n",
      "Epoch 56/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2257\n",
      "Epoch 57/70\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.2234\n",
      "Epoch 58/70\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2229\n",
      "Epoch 59/70\n",
      "6680/6680 [==============================] - 1s 80us/sample - loss: 0.2214\n",
      "Epoch 60/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2204\n",
      "Epoch 61/70\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2196\n",
      "Epoch 62/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2192\n",
      "Epoch 63/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2181\n",
      "Epoch 64/70\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2170\n",
      "Epoch 65/70\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2166\n",
      "Epoch 66/70\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.2165\n",
      "Epoch 67/70\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2158\n",
      "Epoch 68/70\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2145\n",
      "Epoch 69/70\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2139\n",
      "Epoch 70/70\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f5219160c8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_four_layers = getMultiLayerMoldel(neurons = [neurons, \n",
    "                                       best_model_multi_neurons, \n",
    "                                       best_model_tri_neurons, \n",
    "                                       best_model_four_neurons])\n",
    "model_four_layers.fit(X, y,\n",
    "                      epochs=70,\n",
    "                      batch_size=32,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples\n",
      "Epoch 1/60\n",
      "6680/6680 [==============================] - 2s 236us/sample - loss: 0.6016\n",
      "Epoch 2/60\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.3932\n",
      "Epoch 3/60\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.3644\n",
      "Epoch 4/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.3489\n",
      "Epoch 5/60\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.3377\n",
      "Epoch 6/60\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.3304\n",
      "Epoch 7/60\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.3243\n",
      "Epoch 8/60\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.3173\n",
      "Epoch 9/60\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.3131\n",
      "Epoch 10/60\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.3086\n",
      "Epoch 11/60\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.3043\n",
      "Epoch 12/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.3001\n",
      "Epoch 13/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2961\n",
      "Epoch 14/60\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2926\n",
      "Epoch 15/60\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2904\n",
      "Epoch 16/60\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2868\n",
      "Epoch 17/60\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2832\n",
      "Epoch 18/60\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2813\n",
      "Epoch 19/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2787\n",
      "Epoch 20/60\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2760\n",
      "Epoch 21/60\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2738\n",
      "Epoch 22/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2709\n",
      "Epoch 23/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2696\n",
      "Epoch 24/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2676\n",
      "Epoch 25/60\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2659\n",
      "Epoch 26/60\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2640\n",
      "Epoch 27/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2624\n",
      "Epoch 28/60\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2603\n",
      "Epoch 29/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2576\n",
      "Epoch 30/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2571\n",
      "Epoch 31/60\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2554\n",
      "Epoch 32/60\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2535\n",
      "Epoch 33/60\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2521\n",
      "Epoch 34/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2510\n",
      "Epoch 35/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2500\n",
      "Epoch 36/60\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2491\n",
      "Epoch 37/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2471\n",
      "Epoch 38/60\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2458\n",
      "Epoch 39/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2451\n",
      "Epoch 40/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2433\n",
      "Epoch 41/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2421\n",
      "Epoch 42/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2411\n",
      "Epoch 43/60\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2412\n",
      "Epoch 44/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2388\n",
      "Epoch 45/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2378\n",
      "Epoch 46/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2367\n",
      "Epoch 47/60\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2349\n",
      "Epoch 48/60\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.2362\n",
      "Epoch 49/60\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2342\n",
      "Epoch 50/60\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2331\n",
      "Epoch 51/60\n",
      "6680/6680 [==============================] - 0s 75us/sample - loss: 0.2325\n",
      "Epoch 52/60\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2311\n",
      "Epoch 53/60\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2312\n",
      "Epoch 54/60\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2301\n",
      "Epoch 55/60\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2287\n",
      "Epoch 56/60\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2276\n",
      "Epoch 57/60\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2267\n",
      "Epoch 58/60\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2259\n",
      "Epoch 59/60\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2243\n",
      "Epoch 60/60\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f5222eed48>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_three_layers = getMultiLayerMoldel(neurons = [neurons, \n",
    "                                       best_model_multi_neurons, \n",
    "                                       best_model_tri_neurons])\n",
    "model_three_layers.fit(X, y,\n",
    "                      epochs=60,\n",
    "                      batch_size=32,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples\n",
      "Epoch 1/120\n",
      "6680/6680 [==============================] - 1s 196us/sample - loss: 0.5586\n",
      "Epoch 2/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.3973\n",
      "Epoch 3/120\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.3713\n",
      "Epoch 4/120\n",
      "6680/6680 [==============================] - 0s 54us/sample - loss: 0.3573\n",
      "Epoch 5/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.3464\n",
      "Epoch 6/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.3374\n",
      "Epoch 7/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.3303\n",
      "Epoch 8/120\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.3228\n",
      "Epoch 9/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.3168\n",
      "Epoch 10/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.3118\n",
      "Epoch 11/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.3067\n",
      "Epoch 12/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.3024\n",
      "Epoch 13/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2993\n",
      "Epoch 14/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2955\n",
      "Epoch 15/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2924\n",
      "Epoch 16/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2893\n",
      "Epoch 17/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2866\n",
      "Epoch 18/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2838\n",
      "Epoch 19/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2821\n",
      "Epoch 20/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2798\n",
      "Epoch 21/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2769\n",
      "Epoch 22/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2753\n",
      "Epoch 23/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2740\n",
      "Epoch 24/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2717\n",
      "Epoch 25/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2690\n",
      "Epoch 26/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2675\n",
      "Epoch 27/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2668\n",
      "Epoch 28/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2649\n",
      "Epoch 29/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2635\n",
      "Epoch 30/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2619\n",
      "Epoch 31/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2605\n",
      "Epoch 32/120\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2594\n",
      "Epoch 33/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2575\n",
      "Epoch 34/120\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.2566\n",
      "Epoch 35/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2559\n",
      "Epoch 36/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2546\n",
      "Epoch 37/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2533\n",
      "Epoch 38/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2520\n",
      "Epoch 39/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2516\n",
      "Epoch 40/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2499\n",
      "Epoch 41/120\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2503\n",
      "Epoch 42/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2485\n",
      "Epoch 43/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2467\n",
      "Epoch 44/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2457\n",
      "Epoch 45/120\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2456\n",
      "Epoch 46/120\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2448\n",
      "Epoch 47/120\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.2436\n",
      "Epoch 48/120\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2430\n",
      "Epoch 49/120\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2422\n",
      "Epoch 50/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2409\n",
      "Epoch 51/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2401\n",
      "Epoch 52/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2396\n",
      "Epoch 53/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2384\n",
      "Epoch 54/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2383\n",
      "Epoch 55/120\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2368\n",
      "Epoch 56/120\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2360\n",
      "Epoch 57/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2355\n",
      "Epoch 58/120\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2341\n",
      "Epoch 59/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2339\n",
      "Epoch 60/120\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2339\n",
      "Epoch 61/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2324\n",
      "Epoch 62/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2313\n",
      "Epoch 63/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2314\n",
      "Epoch 64/120\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2308\n",
      "Epoch 65/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2291\n",
      "Epoch 66/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2294\n",
      "Epoch 67/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2291\n",
      "Epoch 68/120\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2279\n",
      "Epoch 69/120\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2279\n",
      "Epoch 70/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2270\n",
      "Epoch 71/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2266\n",
      "Epoch 72/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2259\n",
      "Epoch 73/120\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.2259\n",
      "Epoch 74/120\n",
      "6680/6680 [==============================] - 0s 75us/sample - loss: 0.2237\n",
      "Epoch 75/120\n",
      "6680/6680 [==============================] - 0s 70us/sample - loss: 0.2249\n",
      "Epoch 76/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2233\n",
      "Epoch 77/120\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2228\n",
      "Epoch 78/120\n",
      "6680/6680 [==============================] - 0s 73us/sample - loss: 0.2212\n",
      "Epoch 79/120\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2221\n",
      "Epoch 80/120\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2217\n",
      "Epoch 81/120\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2203\n",
      "Epoch 82/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2192\n",
      "Epoch 83/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2194\n",
      "Epoch 84/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2178\n",
      "Epoch 85/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2186\n",
      "Epoch 86/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2184\n",
      "Epoch 87/120\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2173\n",
      "Epoch 88/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2165\n",
      "Epoch 89/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2173\n",
      "Epoch 90/120\n",
      "6680/6680 [==============================] - 0s 68us/sample - loss: 0.2170\n",
      "Epoch 91/120\n",
      "6680/6680 [==============================] - 0s 65us/sample - loss: 0.2160\n",
      "Epoch 92/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2148\n",
      "Epoch 93/120\n",
      "6680/6680 [==============================] - 0s 63us/sample - loss: 0.2152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/120\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2144\n",
      "Epoch 95/120\n",
      "6680/6680 [==============================] - 0s 54us/sample - loss: 0.2134\n",
      "Epoch 96/120\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2130\n",
      "Epoch 97/120\n",
      "6680/6680 [==============================] - 0s 61us/sample - loss: 0.2138\n",
      "Epoch 98/120\n",
      "6680/6680 [==============================] - 0s 55us/sample - loss: 0.2112\n",
      "Epoch 99/120\n",
      "6680/6680 [==============================] - 0s 54us/sample - loss: 0.2119\n",
      "Epoch 100/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2115\n",
      "Epoch 101/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2111\n",
      "Epoch 102/120\n",
      "6680/6680 [==============================] - 0s 54us/sample - loss: 0.2099\n",
      "Epoch 103/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2086\n",
      "Epoch 104/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2094\n",
      "Epoch 105/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2092\n",
      "Epoch 106/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2079\n",
      "Epoch 107/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2080\n",
      "Epoch 108/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2074\n",
      "Epoch 109/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2064\n",
      "Epoch 110/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2068\n",
      "Epoch 111/120\n",
      "6680/6680 [==============================] - 0s 54us/sample - loss: 0.2064\n",
      "Epoch 112/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2062\n",
      "Epoch 113/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2055\n",
      "Epoch 114/120\n",
      "6680/6680 [==============================] - 0s 58us/sample - loss: 0.2057\n",
      "Epoch 115/120\n",
      "6680/6680 [==============================] - 0s 54us/sample - loss: 0.2049\n",
      "Epoch 116/120\n",
      "6680/6680 [==============================] - 0s 54us/sample - loss: 0.2048\n",
      "Epoch 117/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2040\n",
      "Epoch 118/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2032\n",
      "Epoch 119/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2028\n",
      "Epoch 120/120\n",
      "6680/6680 [==============================] - 0s 56us/sample - loss: 0.2031\n",
      "tf.Tensor(0.83892214, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model_two_layers = getMultiLayerMoldel(neurons = [neurons, \n",
    "                                       best_model_multi_neurons])\n",
    "model_two_layers.fit(X, y,\n",
    "          epochs=120,\n",
    "          batch_size=32,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión alcanzada por el modelo con cuatro capas escondidas en los datos de prueba es de 84.31%\n",
      "La precisión alcanzada por el modelo con tres capas escondidas en los datos de prueba es de 84.55%\n",
      "La precisión alcanzada por el modelo con dos capas escondidas en los datos de prueba es de 85.03%\n",
      "Esta estimación posee un 3.4% de precisión con una confianza del 95.79%\n"
     ]
    }
   ],
   "source": [
    "binAccuracy = binary_accuracy(y_test, squeeze(model_four_layers.predict(X_test)))\n",
    "print('La precisión alcanzada por el modelo con cuatro capas escondidas en los datos de prueba es de %.2f%%' % (binAccuracy*100))\n",
    "\n",
    "binAccuracy = binary_accuracy(y_test, squeeze(model_three_layers.predict(X_test)))\n",
    "print('La precisión alcanzada por el modelo con tres capas escondidas en los datos de prueba es de %.2f%%' % (binAccuracy*100))\n",
    "\n",
    "binAccuracy = binary_accuracy(y_test, squeeze(model_two_layers.predict(X_test)))\n",
    "print('La precisión alcanzada por el modelo con dos capas escondidas en los datos de prueba es de %.2f%%' % (binAccuracy*100))\n",
    "\n",
    "precision = 0.034\n",
    "confianza = (1 - (2*math.exp(-2*(precision**2)*total_test_data))) *100\n",
    "\n",
    "print('Esta estimación posee un %.1f%% de precisión con una confianza del %.2f%%' % (precision*100, confianza))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De los resultados obtenidos, se evidencia que las arquitecturas con más de dos capas ocultas tienen un peor rendimiento en el conjunto de prueba comparado con el modelo con solo una capa. Por otro lado, al analizar la red con dos capas ocultas, es posible observar que este apenas mejora el desempeño del modelo. Se imprimen la cantidad total de parámetros de este último modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_280\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_741 (Dense)            (None, 19)                589       \n",
      "_________________________________________________________________\n",
      "dense_742 (Dense)            (None, 13)                260       \n",
      "_________________________________________________________________\n",
      "dense_743 (Dense)            (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 863\n",
      "Trainable params: 863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_two_layers.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De la información mostrada, se evidencia que la cantidad de parámetros a entrenar es muy alta si se compara con el total de datos. Por lo que es posible concluir que, si se desea mejorar el desempeño de la red neural con dos o más capas, es necesario aumentar significativamente la cantidad de datos. Es importante aclarar que las épocas de entrenamiento para cada red se escogieron variando dicho parámetro con el fin de evitar sobreajuste en los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones:\n",
    "- Fue posible construir un modelo de perceptrón multicapa para clasificación de canciones. Sin embargo, no fue posible crear una red muy profunda debido a la poca cantidad de datos disponibles.\n",
    "- Se evidencia la importancia del conjunto de validación en el diseño de modelos complejos ya que éste permite escoger los hiperparámetros adecuadamente.\n",
    "- Se observó la dificultad que existe a la hora de crear modelos de aprendizaje automático, debido a que necesario de entrenar gran cantidad de modelos antes de elegir una arquitectura definitiva.\n",
    "- Los tiempo de ejecución del este Notebook puede acercarse a la hora, debido a que necesario entrenar numerosos modelos durante una gran cantidad de épocas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
