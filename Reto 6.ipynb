{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de canciones con redes neuronales (implementación propia de backpropagation)\n",
    "## Javier Andres Tellez Ortiz 201617861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se descarga el conjunto de datos y se descomprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in d:\\programas\\anaconda3\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................................] 12656044 / 12656044"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "from zipfile import ZipFile\n",
    "\n",
    "##Se descarga el archivo del repositorio \n",
    "file = wget.download(\"http://millionsongdataset.com/sites/default/files/AdditionalFiles/msd_genre_dataset.zip\")\n",
    "\n",
    "##Se abre el archivo y se descomprime\n",
    "zpFile = ZipFile(file)\n",
    "zpFile.extractall()\n",
    "zpFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%genre</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>title</th>\n",
       "      <th>loudness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>key</th>\n",
       "      <th>mode</th>\n",
       "      <th>duration</th>\n",
       "      <th>...</th>\n",
       "      <th>var_timbre3</th>\n",
       "      <th>var_timbre4</th>\n",
       "      <th>var_timbre5</th>\n",
       "      <th>var_timbre6</th>\n",
       "      <th>var_timbre7</th>\n",
       "      <th>var_timbre8</th>\n",
       "      <th>var_timbre9</th>\n",
       "      <th>var_timbre10</th>\n",
       "      <th>var_timbre11</th>\n",
       "      <th>var_timbre12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>classic pop and rock</td>\n",
       "      <td>TRFCOOU128F427AEC0</td>\n",
       "      <td>Blue Oyster Cult</td>\n",
       "      <td>Mes Dames Sarat</td>\n",
       "      <td>-8.697</td>\n",
       "      <td>155.007</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>246.33424</td>\n",
       "      <td>...</td>\n",
       "      <td>1255.514569</td>\n",
       "      <td>580.030472</td>\n",
       "      <td>598.485223</td>\n",
       "      <td>575.337671</td>\n",
       "      <td>322.068603</td>\n",
       "      <td>321.726029</td>\n",
       "      <td>232.700609</td>\n",
       "      <td>186.805303</td>\n",
       "      <td>181.938688</td>\n",
       "      <td>151.508011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic pop and rock</td>\n",
       "      <td>TRNJTPB128F427AE9F</td>\n",
       "      <td>Blue Oyster Cult</td>\n",
       "      <td>Screams</td>\n",
       "      <td>-10.659</td>\n",
       "      <td>148.462</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>189.80526</td>\n",
       "      <td>...</td>\n",
       "      <td>2007.653070</td>\n",
       "      <td>1043.474073</td>\n",
       "      <td>585.694981</td>\n",
       "      <td>564.013736</td>\n",
       "      <td>510.177022</td>\n",
       "      <td>400.200186</td>\n",
       "      <td>365.119588</td>\n",
       "      <td>238.099708</td>\n",
       "      <td>197.933757</td>\n",
       "      <td>251.577525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic pop and rock</td>\n",
       "      <td>TRLFJHA128F427AEEA</td>\n",
       "      <td>Blue Oyster Cult</td>\n",
       "      <td>Dance The Night Away</td>\n",
       "      <td>-13.494</td>\n",
       "      <td>112.909</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>158.19710</td>\n",
       "      <td>...</td>\n",
       "      <td>1204.856777</td>\n",
       "      <td>2736.520024</td>\n",
       "      <td>730.233239</td>\n",
       "      <td>665.203452</td>\n",
       "      <td>535.775111</td>\n",
       "      <td>439.335059</td>\n",
       "      <td>486.822970</td>\n",
       "      <td>265.333860</td>\n",
       "      <td>447.097987</td>\n",
       "      <td>251.880724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>classic pop and rock</td>\n",
       "      <td>TRCQZAG128F427DB97</td>\n",
       "      <td>Blue Oyster Cult</td>\n",
       "      <td>Debbie Denise</td>\n",
       "      <td>-12.786</td>\n",
       "      <td>117.429</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>250.22649</td>\n",
       "      <td>...</td>\n",
       "      <td>809.755802</td>\n",
       "      <td>563.908070</td>\n",
       "      <td>492.803819</td>\n",
       "      <td>378.382799</td>\n",
       "      <td>372.875044</td>\n",
       "      <td>231.941957</td>\n",
       "      <td>246.313305</td>\n",
       "      <td>168.400152</td>\n",
       "      <td>85.282462</td>\n",
       "      <td>339.897173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>classic pop and rock</td>\n",
       "      <td>TRNXMNM128F427DB8C</td>\n",
       "      <td>Blue Oyster Cult</td>\n",
       "      <td>(Don't Fear) The Reaper</td>\n",
       "      <td>-14.093</td>\n",
       "      <td>141.536</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>307.06893</td>\n",
       "      <td>...</td>\n",
       "      <td>1093.684935</td>\n",
       "      <td>343.556047</td>\n",
       "      <td>889.163314</td>\n",
       "      <td>218.111796</td>\n",
       "      <td>304.862864</td>\n",
       "      <td>178.352161</td>\n",
       "      <td>440.478867</td>\n",
       "      <td>142.669283</td>\n",
       "      <td>81.061326</td>\n",
       "      <td>208.355152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 %genre            track_id       artist_name  \\\n",
       "0  classic pop and rock  TRFCOOU128F427AEC0  Blue Oyster Cult   \n",
       "1  classic pop and rock  TRNJTPB128F427AE9F  Blue Oyster Cult   \n",
       "2  classic pop and rock  TRLFJHA128F427AEEA  Blue Oyster Cult   \n",
       "3  classic pop and rock  TRCQZAG128F427DB97  Blue Oyster Cult   \n",
       "4  classic pop and rock  TRNXMNM128F427DB8C  Blue Oyster Cult   \n",
       "\n",
       "                     title  loudness    tempo  time_signature  key  mode  \\\n",
       "0          Mes Dames Sarat    -8.697  155.007               1    9     1   \n",
       "1                  Screams   -10.659  148.462               1    4     0   \n",
       "2     Dance The Night Away   -13.494  112.909               1   10     0   \n",
       "3            Debbie Denise   -12.786  117.429               4    7     1   \n",
       "4  (Don't Fear) The Reaper   -14.093  141.536               4    9     0   \n",
       "\n",
       "    duration  ...  var_timbre3  var_timbre4  var_timbre5  var_timbre6  \\\n",
       "0  246.33424  ...  1255.514569   580.030472   598.485223   575.337671   \n",
       "1  189.80526  ...  2007.653070  1043.474073   585.694981   564.013736   \n",
       "2  158.19710  ...  1204.856777  2736.520024   730.233239   665.203452   \n",
       "3  250.22649  ...   809.755802   563.908070   492.803819   378.382799   \n",
       "4  307.06893  ...  1093.684935   343.556047   889.163314   218.111796   \n",
       "\n",
       "   var_timbre7  var_timbre8  var_timbre9  var_timbre10  var_timbre11  \\\n",
       "0   322.068603   321.726029   232.700609    186.805303    181.938688   \n",
       "1   510.177022   400.200186   365.119588    238.099708    197.933757   \n",
       "2   535.775111   439.335059   486.822970    265.333860    447.097987   \n",
       "3   372.875044   231.941957   246.313305    168.400152     85.282462   \n",
       "4   304.862864   178.352161   440.478867    142.669283     81.061326   \n",
       "\n",
       "   var_timbre12  \n",
       "0    151.508011  \n",
       "1    251.577525  \n",
       "2    251.880724  \n",
       "3    339.897173  \n",
       "4    208.355152  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = pd.read_csv(\"msd_genre_dataset.txt\", skiprows = range(9))\n",
    "\n",
    "dataset.dropna()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns = [\"track_id\", \"artist_name\", \"title\"])\n",
    "features = dataset.columns.tolist()\n",
    "dataset = dataset[(dataset[\"%genre\"] == \"jazz and blues\") | (dataset[\"%genre\"] == \"soul and reggae\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se obtienen los datos de los géneros de interés y se muestra la cantidad de cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jazz and blues</th>\n",
       "      <td>4334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soul and reggae</th>\n",
       "      <td>4016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 %genre\n",
       "jazz and blues     4334\n",
       "soul and reggae    4016"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset[\"%genre\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De acuerdo con trabajos anteriores, se estandarizan los datos y se convierten las etiquetas a valores numéricos. Se agrega un vector de unos que representa el bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "y = dataset[\"%genre\"].values\n",
    "X = dataset.values[:,1:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "lblEncoder = LabelEncoder()\n",
    "\n",
    "##Se convierten las etiquetas en datos binarios\n",
    "y = lblEncoder.fit(np.unique(y)).transform(y)\n",
    "\n",
    "##Se estandarizan los datos provenientes del archivo\n",
    "X = scaler.fit(X).transform(X)\n",
    "data_rows = X.shape[0]\n",
    "ones_vector =  np.ones([data_rows,1])\n",
    "\n",
    "##Se agrega un vector de 1's que representan el bias\n",
    "X = np.concatenate((ones_vector , X), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se dividen los datos en entrenamiento y prueba. Los datos de validación no son necesarios ya que se usarán los modelos encontrados en el reto anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de prueba clase 0 (jazz and blues): 879\n",
      "Datos de prueba clase 1 (soul and reggae): 791\n",
      "Total datos prueba: 1670\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##Se dividen los datos en entrenamiento y prueba\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size = 0.2, random_state = 7861)\n",
    "X, X_test = X.T, X_test.T\n",
    "total_test_data = X_test.shape[1]\n",
    "\n",
    "##Se agrega una dimension a los vectores de las etiquetas para garantizar compatibilidad\n",
    "y = np.expand_dims(y, axis = 0)\n",
    "y_test = np.expand_dims(y_test, axis = 0)\n",
    "\n",
    "genres, cantidad = np.unique(y_test, return_counts = True)\n",
    "total_test_data = sum(cantidad)\n",
    "labels = lblEncoder.inverse_transform(genres)\n",
    "print(\"Datos de prueba clase %s (%s): %d\" % (genres[0],labels[0],cantidad[0]))\n",
    "print(\"Datos de prueba clase %s (%s): %d\" % (genres[1],labels[1],cantidad[1]))\n",
    "print(\"Total datos prueba: %d\" % total_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se definen las funciones de activacion a usar y sus derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(a):\n",
    "    mask = a > 0\n",
    "    mask = mask.astype(float)\n",
    "    return np.multiply(a, mask)\n",
    "\n",
    "def ReLU_grad(a):\n",
    "    mask = a > 0\n",
    "    mask = mask.astype(float)\n",
    "    return mask\n",
    "\n",
    "def sigmoid(X):\n",
    "    y = 1/(1 + np.exp(-X))\n",
    "    return y\n",
    "\n",
    "def sigmoid_grad(X):\n",
    "    value = sigmoid(X)\n",
    "    return value * (1-value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se define la clase que representa una red neuronal. En ella se defienen a su vez las funciones para inicializar pesos (Xavier initializer), para hacer propagación de un conjunto de datos y su respectiva predicción y para realizar el procedimiento de retopropagación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "import scipy.stats as stats\n",
    "\n",
    "class Neural_Network():\n",
    "    \n",
    "    def __init__(self, layers, activations, activations_grad):\n",
    "        assert len(layers)-1 == len(activations), \"Se debe ingresar un número consistente de activaciones y de capas\"\n",
    "        assert len(activations_grad) == len(activations), \"Se debe ingresar el mismo número de activaciones y de derivadas\"\n",
    "        \n",
    "        self.activations = activations\n",
    "        self.activations_grad = activations_grad\n",
    "        \n",
    "        self.delta = []\n",
    "        self.z = []\n",
    "        self.a = []\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.z.append(np.ones((1,1)))\n",
    "            self.delta.append(np.ones((1,1)))\n",
    "            self.a.append(np.ones((1,1)))\n",
    "            \n",
    "        self.W = self.initialize_weights(layers)\n",
    "        \n",
    "    def initialize_weights(self, layers):\n",
    "        W = []\n",
    "        \n",
    "        for i in range(len(layers) - 1):\n",
    "            fan_in = layers[i] + 1\n",
    "            fan_out = layers[i + 1]\n",
    "            total_neurons = fan_in + fan_out\n",
    "            \n",
    "            variance = math.sqrt(2/(total_neurons))\n",
    "            distribution = stats.truncnorm(-2, 2, loc=0, scale=variance)\n",
    "            \n",
    "            w = distribution.rvs((fan_out,fan_in))\n",
    "            W.append(w)\n",
    "        \n",
    "        return W\n",
    "        \n",
    "    def predict_probs(self, X):\n",
    "        self.a[0] = np.matmul(self.W[0],X)\n",
    "        ones = np.ones((1,self.a[0].shape[1]))\n",
    "        self.a[0] = np.append(ones, self.a[0], axis=0)\n",
    "        \n",
    "        self.z[0] = np.apply_along_axis(self.activations[0],0,self.a[0])\n",
    "        \n",
    "        layers = len(self.W)\n",
    "        \n",
    "        for i in range(1,layers):\n",
    "            self.a[i] = np.matmul(self.W[i],self.z[i-1])\n",
    "            \n",
    "            if i != layers-1:\n",
    "                ones = np.ones((1,self.a[i].shape[1]))\n",
    "                self.a[i] = np.append(ones, self.a[i], axis=0)\n",
    "            \n",
    "            self.z[i] = np.apply_along_axis(self.activations[i],0,self.a[i])\n",
    "        return self.z[-1]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob = self.predict_probs(X)\n",
    "        return (prob > 0.5).astype(float)\n",
    "    \n",
    "    def back_prop(self, y):\n",
    "        self.delta[-1] = self.z[-1] - y\n",
    "        for i in range(len(self.delta)-2, -1, -1):\n",
    "            derivate = np.apply_along_axis(self.activations_grad[i],0,self.a[i][1:,:])\n",
    "            weigth_error = np.matmul(self.W[i+1][:,1:].T, self.delta[i+1])\n",
    "            self.delta[i] = np.multiply(derivate, weigth_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se define la clase para el optimizador Adam el cual puede actualizar los pesos del modelo que recibe por parámetro usando los resultados obtenidos por una propagación y una retropropagación con un conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam_optimizer():\n",
    "    def __init__(self, model, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07):\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.t = 0\n",
    "        \n",
    "        self.momentum = []\n",
    "        self.variance = []\n",
    "        \n",
    "        for element in self.model.W:\n",
    "            shape = element.shape\n",
    "            self.momentum.append(np.zeros(shape))\n",
    "            self.variance.append(np.zeros(shape))\n",
    "        \n",
    "    def update_weigths(self, X, y):\n",
    "        self.t = self.t + 1 \n",
    "        \n",
    "        gradients = []\n",
    "        \n",
    "        self.model.predict_probs(X)\n",
    "        self.model.back_prop(y)\n",
    "        \n",
    "        z = [X] + self.model.z\n",
    "        deltas = self.model.delta\n",
    "        \n",
    "        for i in range(len(self.momentum)):\n",
    "            gradient = np.matmul(deltas[i], z[i].T)\n",
    "            momentum = (self.beta_1*self.momentum[i]) + ((1 - self.beta_1)*gradient)\n",
    "            variance = (self.beta_2*self.variance[i]) + ((1 - self.beta_2)*np.power(gradient,2))\n",
    "\n",
    "            corrected_momentum = momentum/(1 - (self.beta_1**self.t))\n",
    "            corrected_variance = variance/(1 - (self.beta_2**self.t))\n",
    "\n",
    "            gradient = self.learning_rate * corrected_momentum / (np.sqrt(corrected_variance) + self.epsilon)\n",
    "            self.model.W[i] = self.model.W[i] - gradient \n",
    "            \n",
    "            gradients.append(gradient)\n",
    "            self.momentum[i] = momentum\n",
    "            self.variance[i] = variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se define la función que calcula las pérdidas a partir de la entropía cruzada binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(model, X, y):\n",
    "    y_hat = model.predict_probs(X)\n",
    "    error_vector = np.multiply(y, np.log(y_hat)) + np.multiply(1-y, np.log(1-y_hat))\n",
    "    error = np.sum(error_vector)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se define la función que calcula la precisión binaria de un modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, X, y):\n",
    "    y_hat = model.predict(X)\n",
    "    acc = (y_hat == y).astype(float)\n",
    "    acc = np.sum(acc)\n",
    "    \n",
    "    return acc/y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se define la función que permite dividir los datos en lotes, barajándolos previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitBatches(X, y, batch_size, seed=None):\n",
    "    ## Se obtiene el número total de datos\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    ##Se calcula la cantidad total de lotes\n",
    "    batches = math.ceil(m/batch_size)\n",
    "    \n",
    "    ##Se mezclan aleatoriamente los datos\n",
    "    np.random.seed(seed)\n",
    "    data = np.append(X,y, axis=0)\n",
    "    np.random.shuffle(data.T)\n",
    "\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    for i in range(batches):\n",
    "        \n",
    "        ##Se toman subconjuntos de datos con el tamaño especificado \n",
    "        X_batch = data[:-1,0:batch_size]\n",
    "        X_batches.append(X_batch)\n",
    "                \n",
    "        y_batch = data[-1,0:batch_size]\n",
    "        y_batches.append(y_batch)\n",
    "        \n",
    "        data = data[:,batch_size:]\n",
    "        \n",
    "    return X_batches, y_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para realizar el entremaniento de un modelo, indica pérdidas y exactud al final de cada época"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(optimizer, X, y, epochs=200, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "        ##Se dividen los datos en lotes al inicio de cada época\n",
    "        ##se realiza este procedimiento antes de cada época para \n",
    "        ##garantizar que los datos estén barajados de forma distinta\n",
    "        ##en cada recorrido\n",
    "        X_batches, y_batches = splitBatches(X, y, batch_size)\n",
    "\n",
    "        for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "            optimizer.update_weigths(X_batch, y_batch)\n",
    "\n",
    "        error = cross_entropy(optimizer.model, X, y)\n",
    "        accuracy = calculate_accuracy(optimizer.model, X, y)\n",
    "\n",
    "        print(\"Epoch: %d, train_loss: %.2f, train_accuracy: %.3f\" % (epoch+1, error, accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se entrena el modelo con una capa escondida con los parámetros encontrados en reto anterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss: -4473.79, train_accuracy: 0.586\n",
      "Epoch: 2, train_loss: -4370.09, train_accuracy: 0.609\n",
      "Epoch: 3, train_loss: -4279.06, train_accuracy: 0.626\n",
      "Epoch: 4, train_loss: -4194.68, train_accuracy: 0.641\n",
      "Epoch: 5, train_loss: -4114.88, train_accuracy: 0.661\n",
      "Epoch: 6, train_loss: -4039.69, train_accuracy: 0.679\n",
      "Epoch: 7, train_loss: -3966.40, train_accuracy: 0.694\n",
      "Epoch: 8, train_loss: -3898.32, train_accuracy: 0.705\n",
      "Epoch: 9, train_loss: -3831.65, train_accuracy: 0.716\n",
      "Epoch: 10, train_loss: -3772.30, train_accuracy: 0.726\n",
      "Epoch: 11, train_loss: -3715.31, train_accuracy: 0.734\n",
      "Epoch: 12, train_loss: -3659.67, train_accuracy: 0.744\n",
      "Epoch: 13, train_loss: -3607.86, train_accuracy: 0.751\n",
      "Epoch: 14, train_loss: -3558.67, train_accuracy: 0.756\n",
      "Epoch: 15, train_loss: -3511.21, train_accuracy: 0.762\n",
      "Epoch: 16, train_loss: -3465.68, train_accuracy: 0.766\n",
      "Epoch: 17, train_loss: -3423.54, train_accuracy: 0.770\n",
      "Epoch: 18, train_loss: -3382.01, train_accuracy: 0.776\n",
      "Epoch: 19, train_loss: -3342.85, train_accuracy: 0.780\n",
      "Epoch: 20, train_loss: -3304.79, train_accuracy: 0.784\n",
      "Epoch: 21, train_loss: -3268.86, train_accuracy: 0.786\n",
      "Epoch: 22, train_loss: -3234.08, train_accuracy: 0.788\n",
      "Epoch: 23, train_loss: -3201.14, train_accuracy: 0.789\n",
      "Epoch: 24, train_loss: -3169.13, train_accuracy: 0.790\n",
      "Epoch: 25, train_loss: -3139.73, train_accuracy: 0.792\n",
      "Epoch: 26, train_loss: -3111.66, train_accuracy: 0.793\n",
      "Epoch: 27, train_loss: -3084.20, train_accuracy: 0.794\n",
      "Epoch: 28, train_loss: -3058.97, train_accuracy: 0.794\n",
      "Epoch: 29, train_loss: -3034.83, train_accuracy: 0.795\n",
      "Epoch: 30, train_loss: -3012.39, train_accuracy: 0.797\n",
      "Epoch: 31, train_loss: -2990.11, train_accuracy: 0.799\n",
      "Epoch: 32, train_loss: -2968.78, train_accuracy: 0.800\n",
      "Epoch: 33, train_loss: -2948.76, train_accuracy: 0.801\n",
      "Epoch: 34, train_loss: -2929.18, train_accuracy: 0.802\n",
      "Epoch: 35, train_loss: -2911.26, train_accuracy: 0.803\n",
      "Epoch: 36, train_loss: -2893.56, train_accuracy: 0.804\n",
      "Epoch: 37, train_loss: -2877.18, train_accuracy: 0.806\n",
      "Epoch: 38, train_loss: -2861.21, train_accuracy: 0.807\n",
      "Epoch: 39, train_loss: -2845.78, train_accuracy: 0.808\n",
      "Epoch: 40, train_loss: -2831.05, train_accuracy: 0.809\n",
      "Epoch: 41, train_loss: -2816.86, train_accuracy: 0.810\n",
      "Epoch: 42, train_loss: -2803.22, train_accuracy: 0.810\n",
      "Epoch: 43, train_loss: -2789.73, train_accuracy: 0.810\n",
      "Epoch: 44, train_loss: -2777.23, train_accuracy: 0.812\n",
      "Epoch: 45, train_loss: -2765.22, train_accuracy: 0.814\n",
      "Epoch: 46, train_loss: -2753.66, train_accuracy: 0.814\n",
      "Epoch: 47, train_loss: -2742.34, train_accuracy: 0.815\n",
      "Epoch: 48, train_loss: -2730.78, train_accuracy: 0.816\n",
      "Epoch: 49, train_loss: -2720.95, train_accuracy: 0.816\n",
      "Epoch: 50, train_loss: -2711.74, train_accuracy: 0.816\n",
      "Epoch: 51, train_loss: -2702.90, train_accuracy: 0.817\n",
      "Epoch: 52, train_loss: -2694.93, train_accuracy: 0.817\n",
      "Epoch: 53, train_loss: -2686.81, train_accuracy: 0.817\n",
      "Epoch: 54, train_loss: -2679.53, train_accuracy: 0.817\n",
      "Epoch: 55, train_loss: -2671.71, train_accuracy: 0.818\n",
      "Epoch: 56, train_loss: -2664.14, train_accuracy: 0.819\n",
      "Epoch: 57, train_loss: -2657.08, train_accuracy: 0.819\n",
      "Epoch: 58, train_loss: -2651.18, train_accuracy: 0.820\n",
      "Epoch: 59, train_loss: -2645.86, train_accuracy: 0.819\n",
      "Epoch: 60, train_loss: -2641.65, train_accuracy: 0.819\n",
      "Epoch: 61, train_loss: -2637.70, train_accuracy: 0.820\n",
      "Epoch: 62, train_loss: -2634.23, train_accuracy: 0.821\n",
      "Epoch: 63, train_loss: -2630.22, train_accuracy: 0.821\n",
      "Epoch: 64, train_loss: -2626.19, train_accuracy: 0.821\n",
      "Epoch: 65, train_loss: -2620.73, train_accuracy: 0.821\n",
      "Epoch: 66, train_loss: -2613.98, train_accuracy: 0.821\n",
      "Epoch: 67, train_loss: -2607.99, train_accuracy: 0.821\n",
      "Epoch: 68, train_loss: -2602.59, train_accuracy: 0.821\n",
      "Epoch: 69, train_loss: -2597.61, train_accuracy: 0.822\n",
      "Epoch: 70, train_loss: -2592.02, train_accuracy: 0.822\n",
      "Epoch: 71, train_loss: -2586.35, train_accuracy: 0.822\n",
      "Epoch: 72, train_loss: -2580.66, train_accuracy: 0.822\n",
      "Epoch: 73, train_loss: -2574.70, train_accuracy: 0.823\n",
      "Epoch: 74, train_loss: -2569.50, train_accuracy: 0.822\n",
      "Epoch: 75, train_loss: -2565.17, train_accuracy: 0.823\n",
      "Epoch: 76, train_loss: -2560.78, train_accuracy: 0.824\n",
      "Epoch: 77, train_loss: -2557.45, train_accuracy: 0.824\n",
      "Epoch: 78, train_loss: -2553.53, train_accuracy: 0.824\n",
      "Epoch: 79, train_loss: -2549.31, train_accuracy: 0.824\n",
      "Epoch: 80, train_loss: -2543.49, train_accuracy: 0.825\n",
      "Epoch: 81, train_loss: -2537.61, train_accuracy: 0.826\n",
      "Epoch: 82, train_loss: -2531.60, train_accuracy: 0.826\n",
      "Epoch: 83, train_loss: -2525.65, train_accuracy: 0.827\n",
      "Epoch: 84, train_loss: -2520.31, train_accuracy: 0.827\n",
      "Epoch: 85, train_loss: -2514.79, train_accuracy: 0.827\n",
      "Epoch: 86, train_loss: -2509.92, train_accuracy: 0.828\n",
      "Epoch: 87, train_loss: -2505.63, train_accuracy: 0.827\n",
      "Epoch: 88, train_loss: -2502.33, train_accuracy: 0.827\n",
      "Epoch: 89, train_loss: -2499.29, train_accuracy: 0.827\n",
      "Epoch: 90, train_loss: -2496.14, train_accuracy: 0.828\n",
      "Epoch: 91, train_loss: -2492.54, train_accuracy: 0.827\n",
      "Epoch: 92, train_loss: -2489.31, train_accuracy: 0.827\n",
      "Epoch: 93, train_loss: -2486.52, train_accuracy: 0.827\n",
      "Epoch: 94, train_loss: -2483.65, train_accuracy: 0.827\n",
      "Epoch: 95, train_loss: -2480.11, train_accuracy: 0.828\n",
      "Epoch: 96, train_loss: -2476.58, train_accuracy: 0.828\n",
      "Epoch: 97, train_loss: -2473.57, train_accuracy: 0.828\n",
      "Epoch: 98, train_loss: -2470.67, train_accuracy: 0.829\n",
      "Epoch: 99, train_loss: -2467.86, train_accuracy: 0.829\n",
      "Epoch: 100, train_loss: -2465.54, train_accuracy: 0.829\n",
      "Epoch: 101, train_loss: -2462.47, train_accuracy: 0.830\n",
      "Epoch: 102, train_loss: -2459.51, train_accuracy: 0.830\n",
      "Epoch: 103, train_loss: -2456.64, train_accuracy: 0.831\n",
      "Epoch: 104, train_loss: -2454.06, train_accuracy: 0.831\n",
      "Epoch: 105, train_loss: -2451.55, train_accuracy: 0.831\n",
      "Epoch: 106, train_loss: -2449.25, train_accuracy: 0.831\n",
      "Epoch: 107, train_loss: -2447.27, train_accuracy: 0.832\n",
      "Epoch: 108, train_loss: -2445.60, train_accuracy: 0.831\n",
      "Epoch: 109, train_loss: -2443.45, train_accuracy: 0.832\n",
      "Epoch: 110, train_loss: -2441.27, train_accuracy: 0.833\n",
      "Epoch: 111, train_loss: -2439.00, train_accuracy: 0.832\n",
      "Epoch: 112, train_loss: -2436.85, train_accuracy: 0.833\n",
      "Epoch: 113, train_loss: -2434.64, train_accuracy: 0.832\n",
      "Epoch: 114, train_loss: -2432.11, train_accuracy: 0.833\n",
      "Epoch: 115, train_loss: -2430.12, train_accuracy: 0.834\n",
      "Epoch: 116, train_loss: -2428.12, train_accuracy: 0.835\n",
      "Epoch: 117, train_loss: -2425.84, train_accuracy: 0.835\n",
      "Epoch: 118, train_loss: -2423.94, train_accuracy: 0.835\n",
      "Epoch: 119, train_loss: -2422.43, train_accuracy: 0.836\n",
      "Epoch: 120, train_loss: -2421.05, train_accuracy: 0.836\n",
      "Epoch: 121, train_loss: -2419.43, train_accuracy: 0.837\n",
      "Epoch: 122, train_loss: -2417.84, train_accuracy: 0.837\n",
      "Epoch: 123, train_loss: -2416.35, train_accuracy: 0.837\n",
      "Epoch: 124, train_loss: -2415.17, train_accuracy: 0.837\n",
      "Epoch: 125, train_loss: -2413.23, train_accuracy: 0.838\n",
      "Epoch: 126, train_loss: -2411.25, train_accuracy: 0.837\n",
      "Epoch: 127, train_loss: -2409.23, train_accuracy: 0.837\n",
      "Epoch: 128, train_loss: -2407.35, train_accuracy: 0.837\n",
      "Epoch: 129, train_loss: -2405.10, train_accuracy: 0.837\n",
      "Epoch: 130, train_loss: -2402.28, train_accuracy: 0.837\n",
      "Epoch: 131, train_loss: -2400.38, train_accuracy: 0.837\n",
      "Epoch: 132, train_loss: -2398.12, train_accuracy: 0.837\n",
      "Epoch: 133, train_loss: -2395.60, train_accuracy: 0.836\n",
      "Epoch: 134, train_loss: -2394.02, train_accuracy: 0.837\n",
      "Epoch: 135, train_loss: -2393.05, train_accuracy: 0.837\n",
      "Epoch: 136, train_loss: -2392.76, train_accuracy: 0.837\n",
      "Epoch: 137, train_loss: -2392.33, train_accuracy: 0.836\n",
      "Epoch: 138, train_loss: -2391.66, train_accuracy: 0.837\n",
      "Epoch: 139, train_loss: -2391.36, train_accuracy: 0.837\n",
      "Epoch: 140, train_loss: -2390.82, train_accuracy: 0.837\n",
      "Epoch: 141, train_loss: -2390.00, train_accuracy: 0.837\n",
      "Epoch: 142, train_loss: -2388.75, train_accuracy: 0.837\n",
      "Epoch: 143, train_loss: -2387.28, train_accuracy: 0.837\n",
      "Epoch: 144, train_loss: -2385.36, train_accuracy: 0.837\n",
      "Epoch: 145, train_loss: -2382.48, train_accuracy: 0.837\n",
      "Epoch: 146, train_loss: -2379.50, train_accuracy: 0.836\n",
      "Epoch: 147, train_loss: -2376.96, train_accuracy: 0.836\n",
      "Epoch: 148, train_loss: -2374.89, train_accuracy: 0.836\n",
      "Epoch: 149, train_loss: -2372.89, train_accuracy: 0.837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, train_loss: -2371.29, train_accuracy: 0.838\n",
      "Epoch: 151, train_loss: -2369.62, train_accuracy: 0.838\n",
      "Epoch: 152, train_loss: -2367.80, train_accuracy: 0.838\n",
      "Epoch: 153, train_loss: -2366.40, train_accuracy: 0.838\n",
      "Epoch: 154, train_loss: -2365.06, train_accuracy: 0.838\n",
      "Epoch: 155, train_loss: -2363.81, train_accuracy: 0.837\n",
      "Epoch: 156, train_loss: -2362.33, train_accuracy: 0.837\n",
      "Epoch: 157, train_loss: -2360.94, train_accuracy: 0.837\n",
      "Epoch: 158, train_loss: -2359.52, train_accuracy: 0.837\n",
      "Epoch: 159, train_loss: -2357.95, train_accuracy: 0.837\n",
      "Epoch: 160, train_loss: -2356.18, train_accuracy: 0.837\n",
      "Epoch: 161, train_loss: -2354.55, train_accuracy: 0.838\n",
      "Epoch: 162, train_loss: -2353.29, train_accuracy: 0.839\n",
      "Epoch: 163, train_loss: -2352.20, train_accuracy: 0.838\n",
      "Epoch: 164, train_loss: -2351.62, train_accuracy: 0.839\n",
      "Epoch: 165, train_loss: -2351.76, train_accuracy: 0.840\n",
      "Epoch: 166, train_loss: -2352.22, train_accuracy: 0.841\n",
      "Epoch: 167, train_loss: -2352.84, train_accuracy: 0.841\n",
      "Epoch: 168, train_loss: -2353.09, train_accuracy: 0.841\n",
      "Epoch: 169, train_loss: -2354.11, train_accuracy: 0.841\n",
      "Epoch: 170, train_loss: -2355.39, train_accuracy: 0.841\n",
      "Epoch: 171, train_loss: -2356.21, train_accuracy: 0.841\n",
      "Epoch: 172, train_loss: -2356.92, train_accuracy: 0.841\n",
      "Epoch: 173, train_loss: -2358.48, train_accuracy: 0.840\n",
      "Epoch: 174, train_loss: -2360.42, train_accuracy: 0.839\n",
      "Epoch: 175, train_loss: -2364.06, train_accuracy: 0.839\n",
      "Epoch: 176, train_loss: -2368.67, train_accuracy: 0.839\n",
      "Epoch: 177, train_loss: -2373.23, train_accuracy: 0.837\n",
      "Epoch: 178, train_loss: -2376.39, train_accuracy: 0.837\n",
      "Epoch: 179, train_loss: -2379.16, train_accuracy: 0.837\n",
      "Epoch: 180, train_loss: -2383.65, train_accuracy: 0.837\n",
      "Epoch: 181, train_loss: -2387.89, train_accuracy: 0.836\n",
      "Epoch: 182, train_loss: -2392.75, train_accuracy: 0.836\n",
      "Epoch: 183, train_loss: -2395.62, train_accuracy: 0.836\n",
      "Epoch: 184, train_loss: -2396.57, train_accuracy: 0.836\n",
      "Epoch: 185, train_loss: -2394.59, train_accuracy: 0.835\n",
      "Epoch: 186, train_loss: -2389.50, train_accuracy: 0.837\n",
      "Epoch: 187, train_loss: -2382.23, train_accuracy: 0.838\n",
      "Epoch: 188, train_loss: -2373.22, train_accuracy: 0.838\n",
      "Epoch: 189, train_loss: -2364.46, train_accuracy: 0.839\n",
      "Epoch: 190, train_loss: -2356.08, train_accuracy: 0.838\n",
      "Epoch: 191, train_loss: -2349.05, train_accuracy: 0.838\n",
      "Epoch: 192, train_loss: -2343.26, train_accuracy: 0.840\n",
      "Epoch: 193, train_loss: -2336.38, train_accuracy: 0.841\n",
      "Epoch: 194, train_loss: -2329.27, train_accuracy: 0.841\n",
      "Epoch: 195, train_loss: -2323.09, train_accuracy: 0.843\n",
      "Epoch: 196, train_loss: -2318.22, train_accuracy: 0.843\n",
      "Epoch: 197, train_loss: -2314.51, train_accuracy: 0.843\n",
      "Epoch: 198, train_loss: -2311.76, train_accuracy: 0.843\n",
      "Epoch: 199, train_loss: -2309.91, train_accuracy: 0.843\n",
      "Epoch: 200, train_loss: -2308.87, train_accuracy: 0.843\n",
      "Epoch: 201, train_loss: -2308.54, train_accuracy: 0.843\n",
      "Epoch: 202, train_loss: -2308.75, train_accuracy: 0.843\n",
      "Epoch: 203, train_loss: -2309.18, train_accuracy: 0.843\n",
      "Epoch: 204, train_loss: -2311.43, train_accuracy: 0.842\n",
      "Epoch: 205, train_loss: -2314.64, train_accuracy: 0.843\n",
      "Epoch: 206, train_loss: -2317.48, train_accuracy: 0.842\n",
      "Epoch: 207, train_loss: -2319.46, train_accuracy: 0.841\n",
      "Epoch: 208, train_loss: -2321.66, train_accuracy: 0.842\n",
      "Epoch: 209, train_loss: -2323.08, train_accuracy: 0.842\n",
      "Epoch: 210, train_loss: -2323.26, train_accuracy: 0.842\n",
      "Epoch: 211, train_loss: -2323.38, train_accuracy: 0.841\n",
      "Epoch: 212, train_loss: -2322.01, train_accuracy: 0.841\n",
      "Epoch: 213, train_loss: -2321.08, train_accuracy: 0.841\n",
      "Epoch: 214, train_loss: -2320.23, train_accuracy: 0.841\n",
      "Epoch: 215, train_loss: -2319.43, train_accuracy: 0.840\n",
      "Epoch: 216, train_loss: -2317.30, train_accuracy: 0.840\n",
      "Epoch: 217, train_loss: -2313.57, train_accuracy: 0.840\n",
      "Epoch: 218, train_loss: -2309.50, train_accuracy: 0.840\n",
      "Epoch: 219, train_loss: -2305.33, train_accuracy: 0.840\n",
      "Epoch: 220, train_loss: -2301.61, train_accuracy: 0.840\n",
      "Epoch: 221, train_loss: -2299.05, train_accuracy: 0.841\n",
      "Epoch: 222, train_loss: -2297.11, train_accuracy: 0.841\n",
      "Epoch: 223, train_loss: -2295.33, train_accuracy: 0.841\n",
      "Epoch: 224, train_loss: -2293.90, train_accuracy: 0.842\n",
      "Epoch: 225, train_loss: -2292.86, train_accuracy: 0.842\n",
      "Epoch: 226, train_loss: -2292.61, train_accuracy: 0.842\n",
      "Epoch: 227, train_loss: -2293.12, train_accuracy: 0.842\n",
      "Epoch: 228, train_loss: -2294.15, train_accuracy: 0.842\n",
      "Epoch: 229, train_loss: -2295.62, train_accuracy: 0.841\n",
      "Epoch: 230, train_loss: -2297.64, train_accuracy: 0.841\n",
      "Epoch: 231, train_loss: -2299.06, train_accuracy: 0.842\n",
      "Epoch: 232, train_loss: -2300.61, train_accuracy: 0.841\n",
      "Epoch: 233, train_loss: -2301.49, train_accuracy: 0.841\n",
      "Epoch: 234, train_loss: -2302.41, train_accuracy: 0.841\n",
      "Epoch: 235, train_loss: -2303.24, train_accuracy: 0.842\n",
      "Epoch: 236, train_loss: -2304.10, train_accuracy: 0.843\n",
      "Epoch: 237, train_loss: -2305.05, train_accuracy: 0.843\n",
      "Epoch: 238, train_loss: -2304.26, train_accuracy: 0.842\n",
      "Epoch: 239, train_loss: -2303.39, train_accuracy: 0.842\n",
      "Epoch: 240, train_loss: -2301.90, train_accuracy: 0.842\n",
      "Epoch: 241, train_loss: -2299.87, train_accuracy: 0.842\n",
      "Epoch: 242, train_loss: -2297.45, train_accuracy: 0.842\n",
      "Epoch: 243, train_loss: -2294.66, train_accuracy: 0.842\n",
      "Epoch: 244, train_loss: -2293.37, train_accuracy: 0.842\n",
      "Epoch: 245, train_loss: -2291.85, train_accuracy: 0.842\n",
      "Epoch: 246, train_loss: -2291.23, train_accuracy: 0.842\n",
      "Epoch: 247, train_loss: -2289.75, train_accuracy: 0.843\n",
      "Epoch: 248, train_loss: -2290.26, train_accuracy: 0.843\n",
      "Epoch: 249, train_loss: -2289.70, train_accuracy: 0.844\n",
      "Epoch: 250, train_loss: -2290.65, train_accuracy: 0.844\n",
      "Epoch: 251, train_loss: -2291.45, train_accuracy: 0.844\n",
      "Epoch: 252, train_loss: -2291.08, train_accuracy: 0.844\n",
      "Epoch: 253, train_loss: -2290.09, train_accuracy: 0.845\n",
      "Epoch: 254, train_loss: -2289.97, train_accuracy: 0.844\n",
      "Epoch: 255, train_loss: -2290.20, train_accuracy: 0.845\n",
      "Epoch: 256, train_loss: -2290.21, train_accuracy: 0.844\n",
      "Epoch: 257, train_loss: -2290.33, train_accuracy: 0.843\n",
      "Epoch: 258, train_loss: -2290.16, train_accuracy: 0.843\n",
      "Epoch: 259, train_loss: -2288.99, train_accuracy: 0.844\n",
      "Epoch: 260, train_loss: -2287.47, train_accuracy: 0.843\n",
      "Epoch: 261, train_loss: -2286.56, train_accuracy: 0.844\n",
      "Epoch: 262, train_loss: -2286.51, train_accuracy: 0.844\n",
      "Epoch: 263, train_loss: -2286.20, train_accuracy: 0.845\n",
      "Epoch: 264, train_loss: -2285.74, train_accuracy: 0.845\n",
      "Epoch: 265, train_loss: -2284.95, train_accuracy: 0.845\n",
      "Epoch: 266, train_loss: -2283.98, train_accuracy: 0.845\n",
      "Epoch: 267, train_loss: -2282.90, train_accuracy: 0.846\n",
      "Epoch: 268, train_loss: -2282.15, train_accuracy: 0.846\n",
      "Epoch: 269, train_loss: -2281.18, train_accuracy: 0.847\n",
      "Epoch: 270, train_loss: -2279.94, train_accuracy: 0.847\n",
      "Epoch: 271, train_loss: -2278.07, train_accuracy: 0.847\n",
      "Epoch: 272, train_loss: -2276.07, train_accuracy: 0.847\n",
      "Epoch: 273, train_loss: -2274.73, train_accuracy: 0.847\n",
      "Epoch: 274, train_loss: -2273.85, train_accuracy: 0.847\n",
      "Epoch: 275, train_loss: -2273.07, train_accuracy: 0.847\n",
      "Epoch: 276, train_loss: -2272.07, train_accuracy: 0.848\n",
      "Epoch: 277, train_loss: -2270.71, train_accuracy: 0.848\n",
      "Epoch: 278, train_loss: -2269.10, train_accuracy: 0.848\n",
      "Epoch: 279, train_loss: -2267.06, train_accuracy: 0.849\n",
      "Epoch: 280, train_loss: -2264.95, train_accuracy: 0.849\n",
      "Epoch: 281, train_loss: -2263.17, train_accuracy: 0.849\n",
      "Epoch: 282, train_loss: -2261.99, train_accuracy: 0.849\n",
      "Epoch: 283, train_loss: -2261.96, train_accuracy: 0.849\n",
      "Epoch: 284, train_loss: -2261.72, train_accuracy: 0.849\n",
      "Epoch: 285, train_loss: -2259.99, train_accuracy: 0.849\n",
      "Epoch: 286, train_loss: -2258.72, train_accuracy: 0.849\n",
      "Epoch: 287, train_loss: -2257.07, train_accuracy: 0.849\n",
      "Epoch: 288, train_loss: -2255.67, train_accuracy: 0.849\n",
      "Epoch: 289, train_loss: -2254.44, train_accuracy: 0.849\n",
      "Epoch: 290, train_loss: -2253.22, train_accuracy: 0.849\n",
      "Epoch: 291, train_loss: -2252.18, train_accuracy: 0.849\n",
      "Epoch: 292, train_loss: -2250.77, train_accuracy: 0.849\n",
      "Epoch: 293, train_loss: -2249.72, train_accuracy: 0.849\n",
      "Epoch: 294, train_loss: -2247.80, train_accuracy: 0.848\n",
      "Epoch: 295, train_loss: -2246.02, train_accuracy: 0.849\n",
      "Epoch: 296, train_loss: -2243.89, train_accuracy: 0.849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 297, train_loss: -2242.67, train_accuracy: 0.850\n",
      "Epoch: 298, train_loss: -2241.37, train_accuracy: 0.851\n",
      "Epoch: 299, train_loss: -2240.64, train_accuracy: 0.850\n",
      "Epoch: 300, train_loss: -2239.78, train_accuracy: 0.850\n",
      "Epoch: 301, train_loss: -2238.74, train_accuracy: 0.850\n",
      "Epoch: 302, train_loss: -2238.26, train_accuracy: 0.850\n",
      "Epoch: 303, train_loss: -2237.96, train_accuracy: 0.849\n",
      "Epoch: 304, train_loss: -2237.93, train_accuracy: 0.849\n",
      "Epoch: 305, train_loss: -2237.77, train_accuracy: 0.849\n",
      "Epoch: 306, train_loss: -2237.83, train_accuracy: 0.849\n",
      "Epoch: 307, train_loss: -2237.88, train_accuracy: 0.849\n",
      "Epoch: 308, train_loss: -2237.91, train_accuracy: 0.849\n",
      "Epoch: 309, train_loss: -2238.11, train_accuracy: 0.850\n",
      "Epoch: 310, train_loss: -2238.58, train_accuracy: 0.850\n",
      "Epoch: 311, train_loss: -2238.98, train_accuracy: 0.849\n",
      "Epoch: 312, train_loss: -2238.43, train_accuracy: 0.849\n",
      "Epoch: 313, train_loss: -2237.82, train_accuracy: 0.849\n",
      "Epoch: 314, train_loss: -2237.30, train_accuracy: 0.849\n",
      "Epoch: 315, train_loss: -2236.47, train_accuracy: 0.849\n",
      "Epoch: 316, train_loss: -2236.04, train_accuracy: 0.849\n",
      "Epoch: 317, train_loss: -2235.52, train_accuracy: 0.849\n",
      "Epoch: 318, train_loss: -2235.08, train_accuracy: 0.849\n",
      "Epoch: 319, train_loss: -2235.02, train_accuracy: 0.848\n",
      "Epoch: 320, train_loss: -2234.93, train_accuracy: 0.847\n",
      "Epoch: 321, train_loss: -2235.01, train_accuracy: 0.847\n",
      "Epoch: 322, train_loss: -2235.06, train_accuracy: 0.847\n",
      "Epoch: 323, train_loss: -2235.30, train_accuracy: 0.847\n",
      "Epoch: 324, train_loss: -2236.19, train_accuracy: 0.847\n",
      "Epoch: 325, train_loss: -2237.22, train_accuracy: 0.848\n",
      "Epoch: 326, train_loss: -2237.58, train_accuracy: 0.848\n",
      "Epoch: 327, train_loss: -2237.56, train_accuracy: 0.848\n",
      "Epoch: 328, train_loss: -2237.33, train_accuracy: 0.849\n",
      "Epoch: 329, train_loss: -2236.41, train_accuracy: 0.849\n",
      "Epoch: 330, train_loss: -2235.91, train_accuracy: 0.849\n",
      "Epoch: 331, train_loss: -2235.85, train_accuracy: 0.849\n",
      "Epoch: 332, train_loss: -2236.27, train_accuracy: 0.849\n",
      "Epoch: 333, train_loss: -2237.26, train_accuracy: 0.848\n",
      "Epoch: 334, train_loss: -2237.39, train_accuracy: 0.848\n",
      "Epoch: 335, train_loss: -2237.44, train_accuracy: 0.848\n",
      "Epoch: 336, train_loss: -2237.90, train_accuracy: 0.848\n",
      "Epoch: 337, train_loss: -2237.53, train_accuracy: 0.848\n",
      "Epoch: 338, train_loss: -2236.62, train_accuracy: 0.848\n",
      "Epoch: 339, train_loss: -2235.47, train_accuracy: 0.848\n",
      "Epoch: 340, train_loss: -2235.56, train_accuracy: 0.849\n",
      "Epoch: 341, train_loss: -2234.38, train_accuracy: 0.849\n",
      "Epoch: 342, train_loss: -2232.86, train_accuracy: 0.850\n",
      "Epoch: 343, train_loss: -2231.93, train_accuracy: 0.850\n",
      "Epoch: 344, train_loss: -2230.75, train_accuracy: 0.850\n",
      "Epoch: 345, train_loss: -2229.53, train_accuracy: 0.851\n",
      "Epoch: 346, train_loss: -2227.58, train_accuracy: 0.851\n",
      "Epoch: 347, train_loss: -2225.75, train_accuracy: 0.851\n",
      "Epoch: 348, train_loss: -2224.48, train_accuracy: 0.851\n",
      "Epoch: 349, train_loss: -2223.51, train_accuracy: 0.851\n",
      "Epoch: 350, train_loss: -2222.61, train_accuracy: 0.851\n",
      "Epoch: 351, train_loss: -2222.16, train_accuracy: 0.851\n",
      "Epoch: 352, train_loss: -2222.15, train_accuracy: 0.852\n",
      "Epoch: 353, train_loss: -2222.07, train_accuracy: 0.852\n",
      "Epoch: 354, train_loss: -2221.82, train_accuracy: 0.852\n",
      "Epoch: 355, train_loss: -2221.66, train_accuracy: 0.852\n",
      "Epoch: 356, train_loss: -2221.34, train_accuracy: 0.851\n",
      "Epoch: 357, train_loss: -2220.71, train_accuracy: 0.852\n",
      "Epoch: 358, train_loss: -2220.12, train_accuracy: 0.851\n",
      "Epoch: 359, train_loss: -2219.43, train_accuracy: 0.851\n",
      "Epoch: 360, train_loss: -2219.55, train_accuracy: 0.850\n",
      "Epoch: 361, train_loss: -2219.46, train_accuracy: 0.849\n",
      "Epoch: 362, train_loss: -2219.64, train_accuracy: 0.850\n",
      "Epoch: 363, train_loss: -2219.42, train_accuracy: 0.849\n",
      "Epoch: 364, train_loss: -2219.28, train_accuracy: 0.849\n",
      "Epoch: 365, train_loss: -2218.94, train_accuracy: 0.849\n",
      "Epoch: 366, train_loss: -2218.73, train_accuracy: 0.849\n",
      "Epoch: 367, train_loss: -2218.81, train_accuracy: 0.850\n",
      "Epoch: 368, train_loss: -2218.96, train_accuracy: 0.851\n",
      "Epoch: 369, train_loss: -2218.88, train_accuracy: 0.850\n",
      "Epoch: 370, train_loss: -2219.06, train_accuracy: 0.850\n",
      "Epoch: 371, train_loss: -2219.43, train_accuracy: 0.850\n",
      "Epoch: 372, train_loss: -2220.26, train_accuracy: 0.850\n",
      "Epoch: 373, train_loss: -2220.71, train_accuracy: 0.852\n",
      "Epoch: 374, train_loss: -2221.25, train_accuracy: 0.851\n",
      "Epoch: 375, train_loss: -2222.23, train_accuracy: 0.852\n",
      "Epoch: 376, train_loss: -2222.16, train_accuracy: 0.851\n",
      "Epoch: 377, train_loss: -2222.31, train_accuracy: 0.851\n",
      "Epoch: 378, train_loss: -2221.91, train_accuracy: 0.851\n",
      "Epoch: 379, train_loss: -2221.37, train_accuracy: 0.851\n",
      "Epoch: 380, train_loss: -2220.54, train_accuracy: 0.851\n",
      "Epoch: 381, train_loss: -2219.68, train_accuracy: 0.851\n",
      "Epoch: 382, train_loss: -2218.38, train_accuracy: 0.852\n",
      "Epoch: 383, train_loss: -2217.53, train_accuracy: 0.852\n",
      "Epoch: 384, train_loss: -2216.03, train_accuracy: 0.852\n",
      "Epoch: 385, train_loss: -2214.62, train_accuracy: 0.852\n",
      "Epoch: 386, train_loss: -2213.52, train_accuracy: 0.852\n",
      "Epoch: 387, train_loss: -2211.73, train_accuracy: 0.852\n",
      "Epoch: 388, train_loss: -2210.22, train_accuracy: 0.853\n",
      "Epoch: 389, train_loss: -2208.88, train_accuracy: 0.853\n",
      "Epoch: 390, train_loss: -2206.55, train_accuracy: 0.852\n",
      "Epoch: 391, train_loss: -2205.19, train_accuracy: 0.853\n",
      "Epoch: 392, train_loss: -2202.53, train_accuracy: 0.853\n",
      "Epoch: 393, train_loss: -2199.88, train_accuracy: 0.854\n",
      "Epoch: 394, train_loss: -2198.33, train_accuracy: 0.854\n",
      "Epoch: 395, train_loss: -2197.49, train_accuracy: 0.855\n",
      "Epoch: 396, train_loss: -2197.92, train_accuracy: 0.854\n",
      "Epoch: 397, train_loss: -2199.33, train_accuracy: 0.854\n",
      "Epoch: 398, train_loss: -2201.13, train_accuracy: 0.853\n",
      "Epoch: 399, train_loss: -2201.95, train_accuracy: 0.853\n",
      "Epoch: 400, train_loss: -2202.60, train_accuracy: 0.853\n",
      "Epoch: 401, train_loss: -2200.65, train_accuracy: 0.853\n",
      "Epoch: 402, train_loss: -2199.05, train_accuracy: 0.853\n",
      "Epoch: 403, train_loss: -2196.77, train_accuracy: 0.854\n",
      "Epoch: 404, train_loss: -2194.11, train_accuracy: 0.854\n",
      "Epoch: 405, train_loss: -2192.39, train_accuracy: 0.854\n",
      "Epoch: 406, train_loss: -2190.82, train_accuracy: 0.855\n",
      "Epoch: 407, train_loss: -2189.97, train_accuracy: 0.855\n",
      "Epoch: 408, train_loss: -2190.04, train_accuracy: 0.855\n",
      "Epoch: 409, train_loss: -2190.25, train_accuracy: 0.854\n",
      "Epoch: 410, train_loss: -2191.13, train_accuracy: 0.854\n",
      "Epoch: 411, train_loss: -2192.44, train_accuracy: 0.853\n",
      "Epoch: 412, train_loss: -2194.05, train_accuracy: 0.854\n",
      "Epoch: 413, train_loss: -2196.14, train_accuracy: 0.853\n",
      "Epoch: 414, train_loss: -2197.83, train_accuracy: 0.854\n",
      "Epoch: 415, train_loss: -2196.82, train_accuracy: 0.853\n",
      "Epoch: 416, train_loss: -2195.23, train_accuracy: 0.853\n",
      "Epoch: 417, train_loss: -2192.45, train_accuracy: 0.854\n",
      "Epoch: 418, train_loss: -2189.38, train_accuracy: 0.854\n",
      "Epoch: 419, train_loss: -2186.08, train_accuracy: 0.854\n",
      "Epoch: 420, train_loss: -2183.61, train_accuracy: 0.854\n",
      "Epoch: 421, train_loss: -2181.24, train_accuracy: 0.855\n",
      "Epoch: 422, train_loss: -2177.86, train_accuracy: 0.855\n",
      "Epoch: 423, train_loss: -2174.62, train_accuracy: 0.856\n",
      "Epoch: 424, train_loss: -2171.77, train_accuracy: 0.856\n",
      "Epoch: 425, train_loss: -2169.73, train_accuracy: 0.855\n",
      "Epoch: 426, train_loss: -2168.25, train_accuracy: 0.855\n",
      "Epoch: 427, train_loss: -2167.53, train_accuracy: 0.855\n",
      "Epoch: 428, train_loss: -2166.90, train_accuracy: 0.856\n",
      "Epoch: 429, train_loss: -2166.43, train_accuracy: 0.856\n",
      "Epoch: 430, train_loss: -2166.14, train_accuracy: 0.856\n",
      "Epoch: 431, train_loss: -2166.09, train_accuracy: 0.855\n",
      "Epoch: 432, train_loss: -2166.12, train_accuracy: 0.854\n",
      "Epoch: 433, train_loss: -2165.72, train_accuracy: 0.855\n",
      "Epoch: 434, train_loss: -2165.48, train_accuracy: 0.855\n",
      "Epoch: 435, train_loss: -2165.19, train_accuracy: 0.854\n",
      "Epoch: 436, train_loss: -2164.80, train_accuracy: 0.854\n",
      "Epoch: 437, train_loss: -2164.58, train_accuracy: 0.854\n",
      "Epoch: 438, train_loss: -2164.68, train_accuracy: 0.853\n",
      "Epoch: 439, train_loss: -2164.70, train_accuracy: 0.853\n",
      "Epoch: 440, train_loss: -2165.25, train_accuracy: 0.854\n",
      "Epoch: 441, train_loss: -2165.97, train_accuracy: 0.853\n",
      "Epoch: 442, train_loss: -2166.70, train_accuracy: 0.853\n",
      "Epoch: 443, train_loss: -2167.42, train_accuracy: 0.854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 444, train_loss: -2168.02, train_accuracy: 0.854\n",
      "Epoch: 445, train_loss: -2168.56, train_accuracy: 0.854\n",
      "Epoch: 446, train_loss: -2169.04, train_accuracy: 0.854\n",
      "Epoch: 447, train_loss: -2169.09, train_accuracy: 0.854\n",
      "Epoch: 448, train_loss: -2168.90, train_accuracy: 0.854\n",
      "Epoch: 449, train_loss: -2168.29, train_accuracy: 0.854\n",
      "Epoch: 450, train_loss: -2167.80, train_accuracy: 0.854\n",
      "Epoch: 451, train_loss: -2167.08, train_accuracy: 0.854\n",
      "Epoch: 452, train_loss: -2166.65, train_accuracy: 0.855\n",
      "Epoch: 453, train_loss: -2165.53, train_accuracy: 0.855\n",
      "Epoch: 454, train_loss: -2164.36, train_accuracy: 0.855\n",
      "Epoch: 455, train_loss: -2163.49, train_accuracy: 0.856\n",
      "Epoch: 456, train_loss: -2162.93, train_accuracy: 0.855\n",
      "Epoch: 457, train_loss: -2162.60, train_accuracy: 0.856\n",
      "Epoch: 458, train_loss: -2162.93, train_accuracy: 0.855\n",
      "Epoch: 459, train_loss: -2163.65, train_accuracy: 0.856\n",
      "Epoch: 460, train_loss: -2164.78, train_accuracy: 0.855\n",
      "Epoch: 461, train_loss: -2167.61, train_accuracy: 0.854\n",
      "Epoch: 462, train_loss: -2169.72, train_accuracy: 0.854\n",
      "Epoch: 463, train_loss: -2172.06, train_accuracy: 0.855\n",
      "Epoch: 464, train_loss: -2174.03, train_accuracy: 0.855\n",
      "Epoch: 465, train_loss: -2176.49, train_accuracy: 0.856\n",
      "Epoch: 466, train_loss: -2180.08, train_accuracy: 0.856\n",
      "Epoch: 467, train_loss: -2184.04, train_accuracy: 0.856\n",
      "Epoch: 468, train_loss: -2187.86, train_accuracy: 0.855\n",
      "Epoch: 469, train_loss: -2190.20, train_accuracy: 0.855\n",
      "Epoch: 470, train_loss: -2191.35, train_accuracy: 0.855\n",
      "Epoch: 471, train_loss: -2193.32, train_accuracy: 0.855\n",
      "Epoch: 472, train_loss: -2194.74, train_accuracy: 0.855\n",
      "Epoch: 473, train_loss: -2194.10, train_accuracy: 0.854\n",
      "Epoch: 474, train_loss: -2192.92, train_accuracy: 0.855\n",
      "Epoch: 475, train_loss: -2189.27, train_accuracy: 0.855\n",
      "Epoch: 476, train_loss: -2186.22, train_accuracy: 0.855\n",
      "Epoch: 477, train_loss: -2181.60, train_accuracy: 0.856\n",
      "Epoch: 478, train_loss: -2179.34, train_accuracy: 0.855\n",
      "Epoch: 479, train_loss: -2178.17, train_accuracy: 0.855\n",
      "Epoch: 480, train_loss: -2176.22, train_accuracy: 0.854\n",
      "Epoch: 481, train_loss: -2175.76, train_accuracy: 0.855\n",
      "Epoch: 482, train_loss: -2176.55, train_accuracy: 0.855\n",
      "Epoch: 483, train_loss: -2177.22, train_accuracy: 0.854\n",
      "Epoch: 484, train_loss: -2177.70, train_accuracy: 0.854\n",
      "Epoch: 485, train_loss: -2178.68, train_accuracy: 0.854\n",
      "Epoch: 486, train_loss: -2179.29, train_accuracy: 0.854\n",
      "Epoch: 487, train_loss: -2179.43, train_accuracy: 0.855\n",
      "Epoch: 488, train_loss: -2180.36, train_accuracy: 0.855\n",
      "Epoch: 489, train_loss: -2181.00, train_accuracy: 0.854\n",
      "Epoch: 490, train_loss: -2181.41, train_accuracy: 0.854\n",
      "Epoch: 491, train_loss: -2181.57, train_accuracy: 0.854\n",
      "Epoch: 492, train_loss: -2182.31, train_accuracy: 0.854\n",
      "Epoch: 493, train_loss: -2181.92, train_accuracy: 0.853\n",
      "Epoch: 494, train_loss: -2182.35, train_accuracy: 0.853\n",
      "Epoch: 495, train_loss: -2182.83, train_accuracy: 0.853\n",
      "Epoch: 496, train_loss: -2183.16, train_accuracy: 0.853\n",
      "Epoch: 497, train_loss: -2183.20, train_accuracy: 0.853\n",
      "Epoch: 498, train_loss: -2182.92, train_accuracy: 0.852\n",
      "Epoch: 499, train_loss: -2181.03, train_accuracy: 0.852\n",
      "Epoch: 500, train_loss: -2179.80, train_accuracy: 0.852\n"
     ]
    }
   ],
   "source": [
    "one_hidden_layer_model = Neural_Network([30,19,1], [ReLU,sigmoid], [ReLU_grad,sigmoid_grad])\n",
    "one_hidden_layer_optimizer = Adam_optimizer(one_hidden_layer_model, learning_rate=0.003)\n",
    "\n",
    "train_model(one_hidden_layer_optimizer, X, y, 500, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se entrena los modelo con varias capas escondidas con los parámetros encontrados en reto anterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss: -4394.60, train_accuracy: 0.607\n",
      "Epoch: 2, train_loss: -4326.05, train_accuracy: 0.619\n",
      "Epoch: 3, train_loss: -4258.63, train_accuracy: 0.637\n",
      "Epoch: 4, train_loss: -4195.79, train_accuracy: 0.654\n",
      "Epoch: 5, train_loss: -4138.47, train_accuracy: 0.669\n",
      "Epoch: 6, train_loss: -4081.26, train_accuracy: 0.684\n",
      "Epoch: 7, train_loss: -4025.84, train_accuracy: 0.698\n",
      "Epoch: 8, train_loss: -3968.84, train_accuracy: 0.710\n",
      "Epoch: 9, train_loss: -3913.35, train_accuracy: 0.722\n",
      "Epoch: 10, train_loss: -3859.76, train_accuracy: 0.729\n",
      "Epoch: 11, train_loss: -3806.94, train_accuracy: 0.738\n",
      "Epoch: 12, train_loss: -3754.16, train_accuracy: 0.746\n",
      "Epoch: 13, train_loss: -3703.34, train_accuracy: 0.756\n",
      "Epoch: 14, train_loss: -3654.26, train_accuracy: 0.762\n",
      "Epoch: 15, train_loss: -3607.55, train_accuracy: 0.769\n",
      "Epoch: 16, train_loss: -3562.41, train_accuracy: 0.772\n",
      "Epoch: 17, train_loss: -3517.87, train_accuracy: 0.778\n",
      "Epoch: 18, train_loss: -3474.07, train_accuracy: 0.782\n",
      "Epoch: 19, train_loss: -3431.29, train_accuracy: 0.785\n",
      "Epoch: 20, train_loss: -3391.27, train_accuracy: 0.791\n",
      "Epoch: 21, train_loss: -3351.73, train_accuracy: 0.794\n",
      "Epoch: 22, train_loss: -3314.68, train_accuracy: 0.797\n",
      "Epoch: 23, train_loss: -3277.68, train_accuracy: 0.796\n",
      "Epoch: 24, train_loss: -3243.34, train_accuracy: 0.799\n",
      "Epoch: 25, train_loss: -3210.62, train_accuracy: 0.800\n",
      "Epoch: 26, train_loss: -3178.41, train_accuracy: 0.801\n",
      "Epoch: 27, train_loss: -3147.55, train_accuracy: 0.804\n",
      "Epoch: 28, train_loss: -3117.24, train_accuracy: 0.807\n",
      "Epoch: 29, train_loss: -3086.76, train_accuracy: 0.808\n",
      "Epoch: 30, train_loss: -3058.44, train_accuracy: 0.811\n",
      "Epoch: 31, train_loss: -3031.19, train_accuracy: 0.813\n",
      "Epoch: 32, train_loss: -3005.64, train_accuracy: 0.813\n",
      "Epoch: 33, train_loss: -2980.44, train_accuracy: 0.812\n",
      "Epoch: 34, train_loss: -2956.19, train_accuracy: 0.812\n",
      "Epoch: 35, train_loss: -2932.98, train_accuracy: 0.813\n",
      "Epoch: 36, train_loss: -2909.46, train_accuracy: 0.814\n",
      "Epoch: 37, train_loss: -2888.11, train_accuracy: 0.813\n",
      "Epoch: 38, train_loss: -2869.43, train_accuracy: 0.815\n",
      "Epoch: 39, train_loss: -2852.25, train_accuracy: 0.815\n",
      "Epoch: 40, train_loss: -2836.31, train_accuracy: 0.816\n",
      "Epoch: 41, train_loss: -2820.91, train_accuracy: 0.816\n",
      "Epoch: 42, train_loss: -2805.92, train_accuracy: 0.819\n",
      "Epoch: 43, train_loss: -2791.72, train_accuracy: 0.820\n",
      "Epoch: 44, train_loss: -2778.07, train_accuracy: 0.821\n",
      "Epoch: 45, train_loss: -2764.36, train_accuracy: 0.822\n",
      "Epoch: 46, train_loss: -2750.11, train_accuracy: 0.822\n",
      "Epoch: 47, train_loss: -2736.34, train_accuracy: 0.823\n",
      "Epoch: 48, train_loss: -2721.83, train_accuracy: 0.823\n",
      "Epoch: 49, train_loss: -2707.30, train_accuracy: 0.823\n",
      "Epoch: 50, train_loss: -2693.88, train_accuracy: 0.825\n",
      "Epoch: 51, train_loss: -2681.68, train_accuracy: 0.825\n",
      "Epoch: 52, train_loss: -2671.13, train_accuracy: 0.826\n",
      "Epoch: 53, train_loss: -2660.49, train_accuracy: 0.826\n",
      "Epoch: 54, train_loss: -2650.75, train_accuracy: 0.825\n",
      "Epoch: 55, train_loss: -2642.01, train_accuracy: 0.825\n",
      "Epoch: 56, train_loss: -2633.03, train_accuracy: 0.825\n",
      "Epoch: 57, train_loss: -2625.12, train_accuracy: 0.827\n",
      "Epoch: 58, train_loss: -2617.87, train_accuracy: 0.827\n",
      "Epoch: 59, train_loss: -2612.25, train_accuracy: 0.826\n",
      "Epoch: 60, train_loss: -2605.31, train_accuracy: 0.827\n",
      "Epoch: 61, train_loss: -2598.02, train_accuracy: 0.827\n",
      "Epoch: 62, train_loss: -2591.73, train_accuracy: 0.826\n",
      "Epoch: 63, train_loss: -2583.76, train_accuracy: 0.827\n",
      "Epoch: 64, train_loss: -2574.54, train_accuracy: 0.828\n",
      "Epoch: 65, train_loss: -2565.16, train_accuracy: 0.827\n",
      "Epoch: 66, train_loss: -2556.79, train_accuracy: 0.828\n",
      "Epoch: 67, train_loss: -2549.59, train_accuracy: 0.828\n",
      "Epoch: 68, train_loss: -2543.27, train_accuracy: 0.828\n",
      "Epoch: 69, train_loss: -2536.20, train_accuracy: 0.828\n",
      "Epoch: 70, train_loss: -2529.65, train_accuracy: 0.829\n",
      "Epoch: 71, train_loss: -2522.65, train_accuracy: 0.828\n",
      "Epoch: 72, train_loss: -2516.09, train_accuracy: 0.828\n",
      "Epoch: 73, train_loss: -2510.31, train_accuracy: 0.828\n",
      "Epoch: 74, train_loss: -2505.53, train_accuracy: 0.829\n",
      "Epoch: 75, train_loss: -2501.77, train_accuracy: 0.830\n",
      "Epoch: 76, train_loss: -2499.35, train_accuracy: 0.830\n",
      "Epoch: 77, train_loss: -2495.44, train_accuracy: 0.830\n",
      "Epoch: 78, train_loss: -2492.11, train_accuracy: 0.830\n",
      "Epoch: 79, train_loss: -2489.57, train_accuracy: 0.831\n",
      "Epoch: 80, train_loss: -2486.71, train_accuracy: 0.832\n",
      "Epoch: 81, train_loss: -2485.66, train_accuracy: 0.831\n",
      "Epoch: 82, train_loss: -2483.87, train_accuracy: 0.831\n",
      "Epoch: 83, train_loss: -2481.39, train_accuracy: 0.831\n",
      "Epoch: 84, train_loss: -2479.79, train_accuracy: 0.831\n",
      "Epoch: 85, train_loss: -2477.58, train_accuracy: 0.832\n",
      "Epoch: 86, train_loss: -2475.11, train_accuracy: 0.833\n",
      "Epoch: 87, train_loss: -2472.19, train_accuracy: 0.833\n",
      "Epoch: 88, train_loss: -2467.72, train_accuracy: 0.833\n",
      "Epoch: 89, train_loss: -2462.70, train_accuracy: 0.834\n",
      "Epoch: 90, train_loss: -2458.36, train_accuracy: 0.834\n",
      "Epoch: 91, train_loss: -2454.01, train_accuracy: 0.834\n",
      "Epoch: 92, train_loss: -2449.87, train_accuracy: 0.833\n",
      "Epoch: 93, train_loss: -2446.29, train_accuracy: 0.832\n",
      "Epoch: 94, train_loss: -2443.81, train_accuracy: 0.833\n",
      "Epoch: 95, train_loss: -2443.06, train_accuracy: 0.833\n",
      "Epoch: 96, train_loss: -2442.92, train_accuracy: 0.833\n",
      "Epoch: 97, train_loss: -2442.92, train_accuracy: 0.834\n",
      "Epoch: 98, train_loss: -2441.82, train_accuracy: 0.834\n",
      "Epoch: 99, train_loss: -2439.23, train_accuracy: 0.833\n",
      "Epoch: 100, train_loss: -2435.44, train_accuracy: 0.833\n",
      "Epoch: 101, train_loss: -2430.84, train_accuracy: 0.834\n",
      "Epoch: 102, train_loss: -2426.51, train_accuracy: 0.833\n",
      "Epoch: 103, train_loss: -2423.44, train_accuracy: 0.833\n",
      "Epoch: 104, train_loss: -2420.05, train_accuracy: 0.835\n",
      "Epoch: 105, train_loss: -2416.44, train_accuracy: 0.835\n",
      "Epoch: 106, train_loss: -2412.74, train_accuracy: 0.835\n",
      "Epoch: 107, train_loss: -2409.25, train_accuracy: 0.835\n",
      "Epoch: 108, train_loss: -2405.90, train_accuracy: 0.836\n",
      "Epoch: 109, train_loss: -2402.38, train_accuracy: 0.836\n",
      "Epoch: 110, train_loss: -2398.66, train_accuracy: 0.836\n",
      "Epoch: 111, train_loss: -2394.75, train_accuracy: 0.837\n",
      "Epoch: 112, train_loss: -2391.52, train_accuracy: 0.838\n",
      "Epoch: 113, train_loss: -2388.21, train_accuracy: 0.838\n",
      "Epoch: 114, train_loss: -2385.32, train_accuracy: 0.839\n",
      "Epoch: 115, train_loss: -2382.80, train_accuracy: 0.838\n",
      "Epoch: 116, train_loss: -2381.09, train_accuracy: 0.839\n",
      "Epoch: 117, train_loss: -2379.97, train_accuracy: 0.838\n",
      "Epoch: 118, train_loss: -2378.82, train_accuracy: 0.837\n",
      "Epoch: 119, train_loss: -2377.22, train_accuracy: 0.838\n",
      "Epoch: 120, train_loss: -2375.60, train_accuracy: 0.838\n",
      "Epoch: 121, train_loss: -2373.46, train_accuracy: 0.839\n",
      "Epoch: 122, train_loss: -2372.17, train_accuracy: 0.839\n",
      "Epoch: 123, train_loss: -2370.95, train_accuracy: 0.839\n",
      "Epoch: 124, train_loss: -2370.06, train_accuracy: 0.838\n",
      "Epoch: 125, train_loss: -2369.43, train_accuracy: 0.838\n",
      "Epoch: 126, train_loss: -2368.61, train_accuracy: 0.838\n",
      "Epoch: 127, train_loss: -2367.98, train_accuracy: 0.838\n",
      "Epoch: 128, train_loss: -2367.19, train_accuracy: 0.838\n",
      "Epoch: 129, train_loss: -2366.07, train_accuracy: 0.839\n",
      "Epoch: 130, train_loss: -2364.15, train_accuracy: 0.840\n",
      "Epoch: 131, train_loss: -2362.49, train_accuracy: 0.840\n",
      "Epoch: 132, train_loss: -2360.79, train_accuracy: 0.840\n",
      "Epoch: 133, train_loss: -2359.54, train_accuracy: 0.841\n",
      "Epoch: 134, train_loss: -2358.16, train_accuracy: 0.841\n",
      "Epoch: 135, train_loss: -2356.85, train_accuracy: 0.840\n",
      "Epoch: 136, train_loss: -2357.31, train_accuracy: 0.840\n",
      "Epoch: 137, train_loss: -2358.51, train_accuracy: 0.840\n",
      "Epoch: 138, train_loss: -2359.16, train_accuracy: 0.839\n",
      "Epoch: 139, train_loss: -2361.09, train_accuracy: 0.841\n",
      "Epoch: 140, train_loss: -2362.74, train_accuracy: 0.840\n",
      "Epoch: 141, train_loss: -2363.78, train_accuracy: 0.840\n",
      "Epoch: 142, train_loss: -2363.90, train_accuracy: 0.840\n",
      "Epoch: 143, train_loss: -2363.07, train_accuracy: 0.840\n",
      "Epoch: 144, train_loss: -2360.66, train_accuracy: 0.840\n",
      "Epoch: 145, train_loss: -2357.80, train_accuracy: 0.840\n",
      "Epoch: 146, train_loss: -2354.83, train_accuracy: 0.841\n",
      "Epoch: 147, train_loss: -2351.30, train_accuracy: 0.841\n",
      "Epoch: 148, train_loss: -2347.96, train_accuracy: 0.841\n",
      "Epoch: 149, train_loss: -2346.08, train_accuracy: 0.842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, train_loss: -2345.58, train_accuracy: 0.842\n",
      "Epoch: 151, train_loss: -2344.08, train_accuracy: 0.842\n",
      "Epoch: 152, train_loss: -2340.66, train_accuracy: 0.842\n",
      "Epoch: 153, train_loss: -2338.30, train_accuracy: 0.841\n",
      "Epoch: 154, train_loss: -2336.04, train_accuracy: 0.841\n",
      "Epoch: 155, train_loss: -2334.23, train_accuracy: 0.841\n",
      "Epoch: 156, train_loss: -2332.36, train_accuracy: 0.841\n",
      "Epoch: 157, train_loss: -2329.26, train_accuracy: 0.841\n",
      "Epoch: 158, train_loss: -2325.90, train_accuracy: 0.841\n",
      "Epoch: 159, train_loss: -2323.43, train_accuracy: 0.841\n",
      "Epoch: 160, train_loss: -2321.79, train_accuracy: 0.841\n",
      "Epoch: 161, train_loss: -2320.36, train_accuracy: 0.841\n",
      "Epoch: 162, train_loss: -2318.53, train_accuracy: 0.842\n",
      "Epoch: 163, train_loss: -2316.41, train_accuracy: 0.841\n",
      "Epoch: 164, train_loss: -2313.80, train_accuracy: 0.841\n",
      "Epoch: 165, train_loss: -2311.53, train_accuracy: 0.841\n",
      "Epoch: 166, train_loss: -2309.48, train_accuracy: 0.842\n",
      "Epoch: 167, train_loss: -2307.47, train_accuracy: 0.842\n",
      "Epoch: 168, train_loss: -2305.42, train_accuracy: 0.841\n",
      "Epoch: 169, train_loss: -2303.84, train_accuracy: 0.841\n",
      "Epoch: 170, train_loss: -2302.32, train_accuracy: 0.842\n",
      "Epoch: 171, train_loss: -2300.79, train_accuracy: 0.841\n",
      "Epoch: 172, train_loss: -2299.13, train_accuracy: 0.841\n",
      "Epoch: 173, train_loss: -2297.75, train_accuracy: 0.842\n",
      "Epoch: 174, train_loss: -2296.68, train_accuracy: 0.842\n",
      "Epoch: 175, train_loss: -2295.63, train_accuracy: 0.843\n",
      "Epoch: 176, train_loss: -2294.65, train_accuracy: 0.844\n",
      "Epoch: 177, train_loss: -2293.00, train_accuracy: 0.844\n",
      "Epoch: 178, train_loss: -2291.25, train_accuracy: 0.844\n",
      "Epoch: 179, train_loss: -2289.25, train_accuracy: 0.844\n",
      "Epoch: 180, train_loss: -2287.68, train_accuracy: 0.844\n",
      "Epoch: 181, train_loss: -2286.37, train_accuracy: 0.844\n",
      "Epoch: 182, train_loss: -2286.57, train_accuracy: 0.844\n",
      "Epoch: 183, train_loss: -2286.91, train_accuracy: 0.844\n",
      "Epoch: 184, train_loss: -2286.84, train_accuracy: 0.844\n",
      "Epoch: 185, train_loss: -2286.20, train_accuracy: 0.844\n",
      "Epoch: 186, train_loss: -2285.63, train_accuracy: 0.844\n",
      "Epoch: 187, train_loss: -2284.40, train_accuracy: 0.843\n",
      "Epoch: 188, train_loss: -2283.64, train_accuracy: 0.844\n",
      "Epoch: 189, train_loss: -2283.16, train_accuracy: 0.843\n",
      "Epoch: 190, train_loss: -2282.51, train_accuracy: 0.843\n",
      "Epoch: 191, train_loss: -2281.15, train_accuracy: 0.844\n",
      "Epoch: 192, train_loss: -2279.74, train_accuracy: 0.844\n",
      "Epoch: 193, train_loss: -2278.29, train_accuracy: 0.844\n",
      "Epoch: 194, train_loss: -2277.63, train_accuracy: 0.844\n",
      "Epoch: 195, train_loss: -2277.81, train_accuracy: 0.844\n",
      "Epoch: 196, train_loss: -2277.87, train_accuracy: 0.844\n",
      "Epoch: 197, train_loss: -2278.10, train_accuracy: 0.844\n",
      "Epoch: 198, train_loss: -2277.97, train_accuracy: 0.845\n",
      "Epoch: 199, train_loss: -2279.38, train_accuracy: 0.844\n",
      "Epoch: 200, train_loss: -2280.00, train_accuracy: 0.844\n",
      "Epoch: 201, train_loss: -2280.56, train_accuracy: 0.844\n",
      "Epoch: 202, train_loss: -2280.77, train_accuracy: 0.844\n",
      "Epoch: 203, train_loss: -2280.99, train_accuracy: 0.844\n",
      "Epoch: 204, train_loss: -2281.09, train_accuracy: 0.845\n",
      "Epoch: 205, train_loss: -2281.34, train_accuracy: 0.845\n",
      "Epoch: 206, train_loss: -2280.27, train_accuracy: 0.844\n",
      "Epoch: 207, train_loss: -2279.02, train_accuracy: 0.845\n",
      "Epoch: 208, train_loss: -2277.52, train_accuracy: 0.844\n",
      "Epoch: 209, train_loss: -2276.47, train_accuracy: 0.844\n",
      "Epoch: 210, train_loss: -2276.68, train_accuracy: 0.844\n",
      "Epoch: 211, train_loss: -2276.43, train_accuracy: 0.845\n",
      "Epoch: 212, train_loss: -2276.57, train_accuracy: 0.844\n",
      "Epoch: 213, train_loss: -2276.17, train_accuracy: 0.843\n",
      "Epoch: 214, train_loss: -2275.69, train_accuracy: 0.843\n",
      "Epoch: 215, train_loss: -2275.89, train_accuracy: 0.842\n",
      "Epoch: 216, train_loss: -2276.50, train_accuracy: 0.843\n",
      "Epoch: 217, train_loss: -2277.82, train_accuracy: 0.843\n",
      "Epoch: 218, train_loss: -2280.31, train_accuracy: 0.843\n",
      "Epoch: 219, train_loss: -2283.08, train_accuracy: 0.842\n",
      "Epoch: 220, train_loss: -2285.47, train_accuracy: 0.843\n",
      "Epoch: 221, train_loss: -2285.63, train_accuracy: 0.843\n",
      "Epoch: 222, train_loss: -2285.26, train_accuracy: 0.843\n",
      "Epoch: 223, train_loss: -2284.26, train_accuracy: 0.843\n",
      "Epoch: 224, train_loss: -2284.88, train_accuracy: 0.843\n",
      "Epoch: 225, train_loss: -2287.47, train_accuracy: 0.843\n",
      "Epoch: 226, train_loss: -2288.18, train_accuracy: 0.843\n",
      "Epoch: 227, train_loss: -2285.35, train_accuracy: 0.843\n",
      "Epoch: 228, train_loss: -2282.14, train_accuracy: 0.843\n",
      "Epoch: 229, train_loss: -2279.46, train_accuracy: 0.843\n",
      "Epoch: 230, train_loss: -2277.89, train_accuracy: 0.844\n",
      "Epoch: 231, train_loss: -2275.00, train_accuracy: 0.844\n",
      "Epoch: 232, train_loss: -2270.33, train_accuracy: 0.845\n",
      "Epoch: 233, train_loss: -2263.91, train_accuracy: 0.847\n",
      "Epoch: 234, train_loss: -2257.31, train_accuracy: 0.847\n",
      "Epoch: 235, train_loss: -2252.29, train_accuracy: 0.848\n",
      "Epoch: 236, train_loss: -2248.99, train_accuracy: 0.848\n",
      "Epoch: 237, train_loss: -2247.13, train_accuracy: 0.848\n",
      "Epoch: 238, train_loss: -2246.27, train_accuracy: 0.849\n",
      "Epoch: 239, train_loss: -2245.45, train_accuracy: 0.850\n",
      "Epoch: 240, train_loss: -2245.56, train_accuracy: 0.850\n",
      "Epoch: 241, train_loss: -2246.72, train_accuracy: 0.849\n",
      "Epoch: 242, train_loss: -2248.31, train_accuracy: 0.849\n",
      "Epoch: 243, train_loss: -2250.30, train_accuracy: 0.849\n",
      "Epoch: 244, train_loss: -2252.23, train_accuracy: 0.849\n",
      "Epoch: 245, train_loss: -2253.23, train_accuracy: 0.848\n",
      "Epoch: 246, train_loss: -2254.49, train_accuracy: 0.848\n",
      "Epoch: 247, train_loss: -2255.35, train_accuracy: 0.848\n",
      "Epoch: 248, train_loss: -2257.10, train_accuracy: 0.848\n",
      "Epoch: 249, train_loss: -2258.53, train_accuracy: 0.848\n",
      "Epoch: 250, train_loss: -2258.98, train_accuracy: 0.848\n",
      "Epoch: 251, train_loss: -2257.83, train_accuracy: 0.848\n",
      "Epoch: 252, train_loss: -2256.36, train_accuracy: 0.848\n",
      "Epoch: 253, train_loss: -2254.89, train_accuracy: 0.848\n",
      "Epoch: 254, train_loss: -2253.92, train_accuracy: 0.847\n",
      "Epoch: 255, train_loss: -2253.21, train_accuracy: 0.848\n",
      "Epoch: 256, train_loss: -2252.76, train_accuracy: 0.848\n",
      "Epoch: 257, train_loss: -2251.71, train_accuracy: 0.848\n",
      "Epoch: 258, train_loss: -2251.25, train_accuracy: 0.847\n",
      "Epoch: 259, train_loss: -2251.24, train_accuracy: 0.848\n",
      "Epoch: 260, train_loss: -2251.56, train_accuracy: 0.849\n",
      "Epoch: 261, train_loss: -2251.99, train_accuracy: 0.849\n",
      "Epoch: 262, train_loss: -2251.94, train_accuracy: 0.848\n",
      "Epoch: 263, train_loss: -2251.61, train_accuracy: 0.848\n",
      "Epoch: 264, train_loss: -2251.25, train_accuracy: 0.848\n",
      "Epoch: 265, train_loss: -2250.58, train_accuracy: 0.848\n",
      "Epoch: 266, train_loss: -2250.06, train_accuracy: 0.848\n",
      "Epoch: 267, train_loss: -2249.65, train_accuracy: 0.848\n",
      "Epoch: 268, train_loss: -2248.29, train_accuracy: 0.848\n",
      "Epoch: 269, train_loss: -2247.16, train_accuracy: 0.848\n",
      "Epoch: 270, train_loss: -2245.86, train_accuracy: 0.849\n",
      "Epoch: 271, train_loss: -2244.64, train_accuracy: 0.849\n",
      "Epoch: 272, train_loss: -2244.61, train_accuracy: 0.849\n",
      "Epoch: 273, train_loss: -2244.09, train_accuracy: 0.849\n",
      "Epoch: 274, train_loss: -2243.18, train_accuracy: 0.849\n",
      "Epoch: 275, train_loss: -2242.10, train_accuracy: 0.848\n",
      "Epoch: 276, train_loss: -2240.73, train_accuracy: 0.848\n",
      "Epoch: 277, train_loss: -2239.11, train_accuracy: 0.848\n",
      "Epoch: 278, train_loss: -2238.25, train_accuracy: 0.848\n",
      "Epoch: 279, train_loss: -2235.91, train_accuracy: 0.848\n",
      "Epoch: 280, train_loss: -2236.57, train_accuracy: 0.848\n",
      "Epoch: 281, train_loss: -2237.95, train_accuracy: 0.848\n",
      "Epoch: 282, train_loss: -2239.04, train_accuracy: 0.848\n",
      "Epoch: 283, train_loss: -2238.97, train_accuracy: 0.848\n",
      "Epoch: 284, train_loss: -2236.87, train_accuracy: 0.848\n",
      "Epoch: 285, train_loss: -2235.45, train_accuracy: 0.849\n",
      "Epoch: 286, train_loss: -2234.57, train_accuracy: 0.849\n",
      "Epoch: 287, train_loss: -2232.03, train_accuracy: 0.849\n",
      "Epoch: 288, train_loss: -2227.24, train_accuracy: 0.851\n",
      "Epoch: 289, train_loss: -2223.17, train_accuracy: 0.851\n",
      "Epoch: 290, train_loss: -2220.69, train_accuracy: 0.851\n",
      "Epoch: 291, train_loss: -2219.36, train_accuracy: 0.851\n",
      "Epoch: 292, train_loss: -2218.68, train_accuracy: 0.851\n",
      "Epoch: 293, train_loss: -2219.12, train_accuracy: 0.851\n",
      "Epoch: 294, train_loss: -2218.02, train_accuracy: 0.852\n",
      "Epoch: 295, train_loss: -2216.79, train_accuracy: 0.851\n",
      "Epoch: 296, train_loss: -2214.95, train_accuracy: 0.851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 297, train_loss: -2213.68, train_accuracy: 0.850\n",
      "Epoch: 298, train_loss: -2214.05, train_accuracy: 0.850\n",
      "Epoch: 299, train_loss: -2215.35, train_accuracy: 0.850\n",
      "Epoch: 300, train_loss: -2217.66, train_accuracy: 0.850\n",
      "Epoch: 301, train_loss: -2220.57, train_accuracy: 0.851\n",
      "Epoch: 302, train_loss: -2223.22, train_accuracy: 0.850\n",
      "Epoch: 303, train_loss: -2226.74, train_accuracy: 0.849\n",
      "Epoch: 304, train_loss: -2229.61, train_accuracy: 0.849\n",
      "Epoch: 305, train_loss: -2231.34, train_accuracy: 0.849\n",
      "Epoch: 306, train_loss: -2231.57, train_accuracy: 0.849\n",
      "Epoch: 307, train_loss: -2230.99, train_accuracy: 0.850\n",
      "Epoch: 308, train_loss: -2230.45, train_accuracy: 0.849\n",
      "Epoch: 309, train_loss: -2229.96, train_accuracy: 0.848\n",
      "Epoch: 310, train_loss: -2230.49, train_accuracy: 0.848\n",
      "Epoch: 311, train_loss: -2228.40, train_accuracy: 0.848\n",
      "Epoch: 312, train_loss: -2226.50, train_accuracy: 0.848\n",
      "Epoch: 313, train_loss: -2224.85, train_accuracy: 0.848\n",
      "Epoch: 314, train_loss: -2222.69, train_accuracy: 0.848\n",
      "Epoch: 315, train_loss: -2219.39, train_accuracy: 0.849\n",
      "Epoch: 316, train_loss: -2213.54, train_accuracy: 0.851\n",
      "Epoch: 317, train_loss: -2208.91, train_accuracy: 0.852\n",
      "Epoch: 318, train_loss: -2205.50, train_accuracy: 0.852\n",
      "Epoch: 319, train_loss: -2203.27, train_accuracy: 0.852\n",
      "Epoch: 320, train_loss: -2200.65, train_accuracy: 0.851\n",
      "Epoch: 321, train_loss: -2198.90, train_accuracy: 0.852\n",
      "Epoch: 322, train_loss: -2196.74, train_accuracy: 0.853\n",
      "Epoch: 323, train_loss: -2195.12, train_accuracy: 0.852\n",
      "Epoch: 324, train_loss: -2193.56, train_accuracy: 0.853\n",
      "Epoch: 325, train_loss: -2192.77, train_accuracy: 0.852\n",
      "Epoch: 326, train_loss: -2192.19, train_accuracy: 0.853\n",
      "Epoch: 327, train_loss: -2193.72, train_accuracy: 0.853\n",
      "Epoch: 328, train_loss: -2195.00, train_accuracy: 0.852\n",
      "Epoch: 329, train_loss: -2196.44, train_accuracy: 0.852\n",
      "Epoch: 330, train_loss: -2200.92, train_accuracy: 0.851\n",
      "Epoch: 331, train_loss: -2206.03, train_accuracy: 0.851\n",
      "Epoch: 332, train_loss: -2212.14, train_accuracy: 0.852\n",
      "Epoch: 333, train_loss: -2216.45, train_accuracy: 0.852\n",
      "Epoch: 334, train_loss: -2222.30, train_accuracy: 0.851\n",
      "Epoch: 335, train_loss: -2225.29, train_accuracy: 0.851\n",
      "Epoch: 336, train_loss: -2222.18, train_accuracy: 0.851\n",
      "Epoch: 337, train_loss: -2221.48, train_accuracy: 0.851\n",
      "Epoch: 338, train_loss: -2224.10, train_accuracy: 0.850\n",
      "Epoch: 339, train_loss: -2223.52, train_accuracy: 0.851\n",
      "Epoch: 340, train_loss: -2218.62, train_accuracy: 0.851\n",
      "Epoch: 341, train_loss: -2216.68, train_accuracy: 0.851\n",
      "Epoch: 342, train_loss: -2212.50, train_accuracy: 0.851\n",
      "Epoch: 343, train_loss: -2209.31, train_accuracy: 0.851\n",
      "Epoch: 344, train_loss: -2202.04, train_accuracy: 0.852\n",
      "Epoch: 345, train_loss: -2195.65, train_accuracy: 0.853\n",
      "Epoch: 346, train_loss: -2189.29, train_accuracy: 0.854\n",
      "Epoch: 347, train_loss: -2183.88, train_accuracy: 0.855\n",
      "Epoch: 348, train_loss: -2179.90, train_accuracy: 0.855\n",
      "Epoch: 349, train_loss: -2176.68, train_accuracy: 0.854\n",
      "Epoch: 350, train_loss: -2174.32, train_accuracy: 0.856\n",
      "Epoch: 351, train_loss: -2171.03, train_accuracy: 0.856\n",
      "Epoch: 352, train_loss: -2168.04, train_accuracy: 0.856\n",
      "Epoch: 353, train_loss: -2165.69, train_accuracy: 0.856\n",
      "Epoch: 354, train_loss: -2163.74, train_accuracy: 0.856\n",
      "Epoch: 355, train_loss: -2162.55, train_accuracy: 0.856\n",
      "Epoch: 356, train_loss: -2161.68, train_accuracy: 0.856\n",
      "Epoch: 357, train_loss: -2160.15, train_accuracy: 0.854\n",
      "Epoch: 358, train_loss: -2159.62, train_accuracy: 0.854\n",
      "Epoch: 359, train_loss: -2158.93, train_accuracy: 0.853\n",
      "Epoch: 360, train_loss: -2159.16, train_accuracy: 0.852\n",
      "Epoch: 361, train_loss: -2157.79, train_accuracy: 0.852\n",
      "Epoch: 362, train_loss: -2156.21, train_accuracy: 0.852\n",
      "Epoch: 363, train_loss: -2155.30, train_accuracy: 0.853\n",
      "Epoch: 364, train_loss: -2154.75, train_accuracy: 0.853\n",
      "Epoch: 365, train_loss: -2153.76, train_accuracy: 0.853\n",
      "Epoch: 366, train_loss: -2153.21, train_accuracy: 0.852\n",
      "Epoch: 367, train_loss: -2153.01, train_accuracy: 0.853\n",
      "Epoch: 368, train_loss: -2152.19, train_accuracy: 0.853\n",
      "Epoch: 369, train_loss: -2151.79, train_accuracy: 0.853\n",
      "Epoch: 370, train_loss: -2149.99, train_accuracy: 0.853\n",
      "Epoch: 371, train_loss: -2148.53, train_accuracy: 0.853\n",
      "Epoch: 372, train_loss: -2148.24, train_accuracy: 0.854\n",
      "Epoch: 373, train_loss: -2149.20, train_accuracy: 0.852\n",
      "Epoch: 374, train_loss: -2150.93, train_accuracy: 0.852\n",
      "Epoch: 375, train_loss: -2154.54, train_accuracy: 0.852\n",
      "Epoch: 376, train_loss: -2160.38, train_accuracy: 0.851\n",
      "Epoch: 377, train_loss: -2165.69, train_accuracy: 0.851\n",
      "Epoch: 378, train_loss: -2173.42, train_accuracy: 0.849\n",
      "Epoch: 379, train_loss: -2180.29, train_accuracy: 0.849\n",
      "Epoch: 380, train_loss: -2186.65, train_accuracy: 0.849\n",
      "Epoch: 381, train_loss: -2184.96, train_accuracy: 0.849\n",
      "Epoch: 382, train_loss: -2179.83, train_accuracy: 0.849\n",
      "Epoch: 383, train_loss: -2172.51, train_accuracy: 0.850\n",
      "Epoch: 384, train_loss: -2163.94, train_accuracy: 0.850\n",
      "Epoch: 385, train_loss: -2155.52, train_accuracy: 0.852\n",
      "Epoch: 386, train_loss: -2147.61, train_accuracy: 0.854\n",
      "Epoch: 387, train_loss: -2142.58, train_accuracy: 0.853\n",
      "Epoch: 388, train_loss: -2141.04, train_accuracy: 0.854\n",
      "Epoch: 389, train_loss: -2142.45, train_accuracy: 0.854\n",
      "Epoch: 390, train_loss: -2145.58, train_accuracy: 0.854\n",
      "Epoch: 391, train_loss: -2151.55, train_accuracy: 0.855\n",
      "Epoch: 392, train_loss: -2157.46, train_accuracy: 0.854\n",
      "Epoch: 393, train_loss: -2166.19, train_accuracy: 0.855\n",
      "Epoch: 394, train_loss: -2173.61, train_accuracy: 0.854\n",
      "Epoch: 395, train_loss: -2173.49, train_accuracy: 0.855\n",
      "Epoch: 396, train_loss: -2170.94, train_accuracy: 0.855\n",
      "Epoch: 397, train_loss: -2169.17, train_accuracy: 0.856\n",
      "Epoch: 398, train_loss: -2167.37, train_accuracy: 0.856\n",
      "Epoch: 399, train_loss: -2159.10, train_accuracy: 0.856\n",
      "Epoch: 400, train_loss: -2153.96, train_accuracy: 0.856\n",
      "Epoch: 401, train_loss: -2150.23, train_accuracy: 0.855\n",
      "Epoch: 402, train_loss: -2148.78, train_accuracy: 0.854\n",
      "Epoch: 403, train_loss: -2148.82, train_accuracy: 0.854\n",
      "Epoch: 404, train_loss: -2148.84, train_accuracy: 0.855\n",
      "Epoch: 405, train_loss: -2147.93, train_accuracy: 0.855\n",
      "Epoch: 406, train_loss: -2146.68, train_accuracy: 0.854\n",
      "Epoch: 407, train_loss: -2146.35, train_accuracy: 0.854\n",
      "Epoch: 408, train_loss: -2146.94, train_accuracy: 0.854\n",
      "Epoch: 409, train_loss: -2147.18, train_accuracy: 0.854\n",
      "Epoch: 410, train_loss: -2146.96, train_accuracy: 0.855\n",
      "Epoch: 411, train_loss: -2146.42, train_accuracy: 0.855\n",
      "Epoch: 412, train_loss: -2146.75, train_accuracy: 0.855\n",
      "Epoch: 413, train_loss: -2146.51, train_accuracy: 0.853\n",
      "Epoch: 414, train_loss: -2146.72, train_accuracy: 0.854\n",
      "Epoch: 415, train_loss: -2146.69, train_accuracy: 0.854\n",
      "Epoch: 416, train_loss: -2145.96, train_accuracy: 0.855\n",
      "Epoch: 417, train_loss: -2145.35, train_accuracy: 0.855\n",
      "Epoch: 418, train_loss: -2143.42, train_accuracy: 0.856\n",
      "Epoch: 419, train_loss: -2142.11, train_accuracy: 0.855\n",
      "Epoch: 420, train_loss: -2141.09, train_accuracy: 0.856\n",
      "Epoch: 421, train_loss: -2139.82, train_accuracy: 0.856\n",
      "Epoch: 422, train_loss: -2139.01, train_accuracy: 0.856\n",
      "Epoch: 423, train_loss: -2138.93, train_accuracy: 0.857\n",
      "Epoch: 424, train_loss: -2138.90, train_accuracy: 0.857\n",
      "Epoch: 425, train_loss: -2139.16, train_accuracy: 0.858\n",
      "Epoch: 426, train_loss: -2139.59, train_accuracy: 0.857\n",
      "Epoch: 427, train_loss: -2138.47, train_accuracy: 0.858\n",
      "Epoch: 428, train_loss: -2137.46, train_accuracy: 0.858\n",
      "Epoch: 429, train_loss: -2136.91, train_accuracy: 0.858\n",
      "Epoch: 430, train_loss: -2135.05, train_accuracy: 0.858\n",
      "Epoch: 431, train_loss: -2134.24, train_accuracy: 0.858\n",
      "Epoch: 432, train_loss: -2134.02, train_accuracy: 0.857\n",
      "Epoch: 433, train_loss: -2134.27, train_accuracy: 0.856\n",
      "Epoch: 434, train_loss: -2135.65, train_accuracy: 0.855\n",
      "Epoch: 435, train_loss: -2138.01, train_accuracy: 0.854\n",
      "Epoch: 436, train_loss: -2140.40, train_accuracy: 0.854\n",
      "Epoch: 437, train_loss: -2144.57, train_accuracy: 0.854\n",
      "Epoch: 438, train_loss: -2146.96, train_accuracy: 0.853\n",
      "Epoch: 439, train_loss: -2149.60, train_accuracy: 0.853\n",
      "Epoch: 440, train_loss: -2150.17, train_accuracy: 0.853\n",
      "Epoch: 441, train_loss: -2146.77, train_accuracy: 0.853\n",
      "Epoch: 442, train_loss: -2142.55, train_accuracy: 0.854\n",
      "Epoch: 443, train_loss: -2138.78, train_accuracy: 0.854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 444, train_loss: -2134.87, train_accuracy: 0.854\n",
      "Epoch: 445, train_loss: -2130.43, train_accuracy: 0.855\n",
      "Epoch: 446, train_loss: -2126.89, train_accuracy: 0.855\n",
      "Epoch: 447, train_loss: -2122.72, train_accuracy: 0.856\n",
      "Epoch: 448, train_loss: -2119.63, train_accuracy: 0.857\n",
      "Epoch: 449, train_loss: -2117.48, train_accuracy: 0.859\n",
      "Epoch: 450, train_loss: -2116.86, train_accuracy: 0.860\n",
      "Epoch: 451, train_loss: -2116.91, train_accuracy: 0.861\n",
      "Epoch: 452, train_loss: -2118.50, train_accuracy: 0.861\n",
      "Epoch: 453, train_loss: -2121.05, train_accuracy: 0.862\n",
      "Epoch: 454, train_loss: -2123.11, train_accuracy: 0.860\n",
      "Epoch: 455, train_loss: -2126.64, train_accuracy: 0.861\n",
      "Epoch: 456, train_loss: -2131.13, train_accuracy: 0.860\n",
      "Epoch: 457, train_loss: -2128.55, train_accuracy: 0.860\n",
      "Epoch: 458, train_loss: -2126.34, train_accuracy: 0.859\n",
      "Epoch: 459, train_loss: -2124.20, train_accuracy: 0.859\n",
      "Epoch: 460, train_loss: -2123.42, train_accuracy: 0.860\n",
      "Epoch: 461, train_loss: -2122.47, train_accuracy: 0.861\n",
      "Epoch: 462, train_loss: -2123.98, train_accuracy: 0.861\n",
      "Epoch: 463, train_loss: -2124.95, train_accuracy: 0.862\n",
      "Epoch: 464, train_loss: -2126.38, train_accuracy: 0.861\n",
      "Epoch: 465, train_loss: -2125.01, train_accuracy: 0.861\n",
      "Epoch: 466, train_loss: -2123.31, train_accuracy: 0.862\n",
      "Epoch: 467, train_loss: -2122.85, train_accuracy: 0.863\n",
      "Epoch: 468, train_loss: -2120.90, train_accuracy: 0.862\n",
      "Epoch: 469, train_loss: -2122.34, train_accuracy: 0.863\n",
      "Epoch: 470, train_loss: -2125.81, train_accuracy: 0.862\n",
      "Epoch: 471, train_loss: -2127.68, train_accuracy: 0.862\n",
      "Epoch: 472, train_loss: -2129.65, train_accuracy: 0.861\n",
      "Epoch: 473, train_loss: -2129.22, train_accuracy: 0.862\n",
      "Epoch: 474, train_loss: -2130.07, train_accuracy: 0.861\n",
      "Epoch: 475, train_loss: -2127.59, train_accuracy: 0.861\n",
      "Epoch: 476, train_loss: -2125.26, train_accuracy: 0.862\n",
      "Epoch: 477, train_loss: -2123.86, train_accuracy: 0.863\n",
      "Epoch: 478, train_loss: -2125.24, train_accuracy: 0.862\n",
      "Epoch: 479, train_loss: -2121.70, train_accuracy: 0.862\n",
      "Epoch: 480, train_loss: -2119.72, train_accuracy: 0.863\n",
      "Epoch: 481, train_loss: -2119.25, train_accuracy: 0.864\n",
      "Epoch: 482, train_loss: -2117.93, train_accuracy: 0.864\n",
      "Epoch: 483, train_loss: -2116.89, train_accuracy: 0.865\n",
      "Epoch: 484, train_loss: -2116.98, train_accuracy: 0.864\n",
      "Epoch: 485, train_loss: -2117.51, train_accuracy: 0.863\n",
      "Epoch: 486, train_loss: -2117.97, train_accuracy: 0.862\n",
      "Epoch: 487, train_loss: -2117.62, train_accuracy: 0.862\n",
      "Epoch: 488, train_loss: -2117.77, train_accuracy: 0.863\n",
      "Epoch: 489, train_loss: -2118.74, train_accuracy: 0.862\n",
      "Epoch: 490, train_loss: -2119.48, train_accuracy: 0.863\n",
      "Epoch: 491, train_loss: -2117.13, train_accuracy: 0.862\n",
      "Epoch: 492, train_loss: -2113.81, train_accuracy: 0.862\n",
      "Epoch: 493, train_loss: -2111.71, train_accuracy: 0.862\n",
      "Epoch: 494, train_loss: -2113.01, train_accuracy: 0.862\n",
      "Epoch: 495, train_loss: -2111.84, train_accuracy: 0.862\n",
      "Epoch: 496, train_loss: -2110.82, train_accuracy: 0.862\n",
      "Epoch: 497, train_loss: -2111.24, train_accuracy: 0.862\n",
      "Epoch: 498, train_loss: -2111.17, train_accuracy: 0.862\n",
      "Epoch: 499, train_loss: -2109.84, train_accuracy: 0.862\n",
      "Epoch: 500, train_loss: -2108.11, train_accuracy: 0.863\n",
      "Epoch: 1, train_loss: -5192.42, train_accuracy: 0.483\n",
      "Epoch: 2, train_loss: -5116.20, train_accuracy: 0.483\n",
      "Epoch: 3, train_loss: -5046.50, train_accuracy: 0.483\n",
      "Epoch: 4, train_loss: -4981.53, train_accuracy: 0.483\n",
      "Epoch: 5, train_loss: -4924.54, train_accuracy: 0.483\n",
      "Epoch: 6, train_loss: -4872.19, train_accuracy: 0.483\n",
      "Epoch: 7, train_loss: -4826.89, train_accuracy: 0.483\n",
      "Epoch: 8, train_loss: -4786.89, train_accuracy: 0.484\n",
      "Epoch: 9, train_loss: -4750.43, train_accuracy: 0.485\n",
      "Epoch: 10, train_loss: -4714.35, train_accuracy: 0.486\n",
      "Epoch: 11, train_loss: -4680.11, train_accuracy: 0.487\n",
      "Epoch: 12, train_loss: -4647.72, train_accuracy: 0.488\n",
      "Epoch: 13, train_loss: -4615.16, train_accuracy: 0.490\n",
      "Epoch: 14, train_loss: -4583.71, train_accuracy: 0.495\n",
      "Epoch: 15, train_loss: -4551.87, train_accuracy: 0.502\n",
      "Epoch: 16, train_loss: -4520.83, train_accuracy: 0.508\n",
      "Epoch: 17, train_loss: -4489.29, train_accuracy: 0.517\n",
      "Epoch: 18, train_loss: -4457.22, train_accuracy: 0.526\n",
      "Epoch: 19, train_loss: -4423.73, train_accuracy: 0.539\n",
      "Epoch: 20, train_loss: -4390.21, train_accuracy: 0.556\n",
      "Epoch: 21, train_loss: -4354.38, train_accuracy: 0.571\n",
      "Epoch: 22, train_loss: -4316.47, train_accuracy: 0.588\n",
      "Epoch: 23, train_loss: -4277.24, train_accuracy: 0.608\n",
      "Epoch: 24, train_loss: -4237.17, train_accuracy: 0.628\n",
      "Epoch: 25, train_loss: -4196.51, train_accuracy: 0.645\n",
      "Epoch: 26, train_loss: -4155.04, train_accuracy: 0.663\n",
      "Epoch: 27, train_loss: -4112.85, train_accuracy: 0.683\n",
      "Epoch: 28, train_loss: -4068.58, train_accuracy: 0.699\n",
      "Epoch: 29, train_loss: -4025.57, train_accuracy: 0.714\n",
      "Epoch: 30, train_loss: -3981.28, train_accuracy: 0.724\n",
      "Epoch: 31, train_loss: -3935.89, train_accuracy: 0.736\n",
      "Epoch: 32, train_loss: -3887.40, train_accuracy: 0.747\n",
      "Epoch: 33, train_loss: -3838.67, train_accuracy: 0.756\n",
      "Epoch: 34, train_loss: -3789.63, train_accuracy: 0.762\n",
      "Epoch: 35, train_loss: -3738.98, train_accuracy: 0.767\n",
      "Epoch: 36, train_loss: -3690.77, train_accuracy: 0.773\n",
      "Epoch: 37, train_loss: -3644.69, train_accuracy: 0.777\n",
      "Epoch: 38, train_loss: -3597.53, train_accuracy: 0.781\n",
      "Epoch: 39, train_loss: -3550.37, train_accuracy: 0.785\n",
      "Epoch: 40, train_loss: -3500.99, train_accuracy: 0.787\n",
      "Epoch: 41, train_loss: -3454.82, train_accuracy: 0.790\n",
      "Epoch: 42, train_loss: -3408.22, train_accuracy: 0.792\n",
      "Epoch: 43, train_loss: -3365.52, train_accuracy: 0.795\n",
      "Epoch: 44, train_loss: -3323.33, train_accuracy: 0.796\n",
      "Epoch: 45, train_loss: -3281.44, train_accuracy: 0.799\n",
      "Epoch: 46, train_loss: -3238.32, train_accuracy: 0.799\n",
      "Epoch: 47, train_loss: -3197.03, train_accuracy: 0.801\n",
      "Epoch: 48, train_loss: -3156.49, train_accuracy: 0.803\n",
      "Epoch: 49, train_loss: -3116.76, train_accuracy: 0.804\n",
      "Epoch: 50, train_loss: -3081.08, train_accuracy: 0.805\n",
      "Epoch: 51, train_loss: -3047.19, train_accuracy: 0.805\n",
      "Epoch: 52, train_loss: -3016.36, train_accuracy: 0.806\n",
      "Epoch: 53, train_loss: -2988.64, train_accuracy: 0.807\n",
      "Epoch: 54, train_loss: -2962.57, train_accuracy: 0.807\n",
      "Epoch: 55, train_loss: -2937.12, train_accuracy: 0.807\n",
      "Epoch: 56, train_loss: -2912.24, train_accuracy: 0.809\n",
      "Epoch: 57, train_loss: -2888.86, train_accuracy: 0.810\n",
      "Epoch: 58, train_loss: -2865.70, train_accuracy: 0.810\n",
      "Epoch: 59, train_loss: -2844.45, train_accuracy: 0.812\n",
      "Epoch: 60, train_loss: -2827.89, train_accuracy: 0.812\n",
      "Epoch: 61, train_loss: -2813.06, train_accuracy: 0.813\n",
      "Epoch: 62, train_loss: -2799.84, train_accuracy: 0.813\n",
      "Epoch: 63, train_loss: -2788.10, train_accuracy: 0.813\n",
      "Epoch: 64, train_loss: -2778.08, train_accuracy: 0.815\n",
      "Epoch: 65, train_loss: -2767.95, train_accuracy: 0.814\n",
      "Epoch: 66, train_loss: -2758.34, train_accuracy: 0.815\n",
      "Epoch: 67, train_loss: -2750.73, train_accuracy: 0.815\n",
      "Epoch: 68, train_loss: -2742.90, train_accuracy: 0.816\n",
      "Epoch: 69, train_loss: -2735.36, train_accuracy: 0.816\n",
      "Epoch: 70, train_loss: -2728.11, train_accuracy: 0.816\n",
      "Epoch: 71, train_loss: -2721.19, train_accuracy: 0.816\n",
      "Epoch: 72, train_loss: -2716.14, train_accuracy: 0.816\n",
      "Epoch: 73, train_loss: -2711.32, train_accuracy: 0.816\n",
      "Epoch: 74, train_loss: -2705.28, train_accuracy: 0.817\n",
      "Epoch: 75, train_loss: -2699.85, train_accuracy: 0.817\n",
      "Epoch: 76, train_loss: -2694.08, train_accuracy: 0.817\n",
      "Epoch: 77, train_loss: -2688.97, train_accuracy: 0.818\n",
      "Epoch: 78, train_loss: -2682.26, train_accuracy: 0.819\n",
      "Epoch: 79, train_loss: -2675.90, train_accuracy: 0.819\n",
      "Epoch: 80, train_loss: -2669.69, train_accuracy: 0.821\n",
      "Epoch: 81, train_loss: -2662.27, train_accuracy: 0.821\n",
      "Epoch: 82, train_loss: -2656.68, train_accuracy: 0.821\n",
      "Epoch: 83, train_loss: -2651.39, train_accuracy: 0.822\n",
      "Epoch: 84, train_loss: -2646.13, train_accuracy: 0.822\n",
      "Epoch: 85, train_loss: -2640.81, train_accuracy: 0.821\n",
      "Epoch: 86, train_loss: -2635.96, train_accuracy: 0.821\n",
      "Epoch: 87, train_loss: -2631.14, train_accuracy: 0.821\n",
      "Epoch: 88, train_loss: -2627.07, train_accuracy: 0.822\n",
      "Epoch: 89, train_loss: -2622.95, train_accuracy: 0.822\n",
      "Epoch: 90, train_loss: -2620.43, train_accuracy: 0.823\n",
      "Epoch: 91, train_loss: -2619.81, train_accuracy: 0.822\n",
      "Epoch: 92, train_loss: -2618.92, train_accuracy: 0.823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93, train_loss: -2617.32, train_accuracy: 0.823\n",
      "Epoch: 94, train_loss: -2613.74, train_accuracy: 0.824\n",
      "Epoch: 95, train_loss: -2610.24, train_accuracy: 0.825\n",
      "Epoch: 96, train_loss: -2606.11, train_accuracy: 0.826\n",
      "Epoch: 97, train_loss: -2602.05, train_accuracy: 0.825\n",
      "Epoch: 98, train_loss: -2596.55, train_accuracy: 0.826\n",
      "Epoch: 99, train_loss: -2592.72, train_accuracy: 0.827\n",
      "Epoch: 100, train_loss: -2588.50, train_accuracy: 0.826\n",
      "Epoch: 101, train_loss: -2585.20, train_accuracy: 0.827\n",
      "Epoch: 102, train_loss: -2580.51, train_accuracy: 0.828\n",
      "Epoch: 103, train_loss: -2574.26, train_accuracy: 0.828\n",
      "Epoch: 104, train_loss: -2567.41, train_accuracy: 0.829\n",
      "Epoch: 105, train_loss: -2555.60, train_accuracy: 0.828\n",
      "Epoch: 106, train_loss: -2545.31, train_accuracy: 0.828\n",
      "Epoch: 107, train_loss: -2537.32, train_accuracy: 0.829\n",
      "Epoch: 108, train_loss: -2531.48, train_accuracy: 0.828\n",
      "Epoch: 109, train_loss: -2527.38, train_accuracy: 0.828\n",
      "Epoch: 110, train_loss: -2524.53, train_accuracy: 0.828\n",
      "Epoch: 111, train_loss: -2522.75, train_accuracy: 0.829\n",
      "Epoch: 112, train_loss: -2522.38, train_accuracy: 0.830\n",
      "Epoch: 113, train_loss: -2522.21, train_accuracy: 0.831\n",
      "Epoch: 114, train_loss: -2524.13, train_accuracy: 0.831\n",
      "Epoch: 115, train_loss: -2523.14, train_accuracy: 0.830\n",
      "Epoch: 116, train_loss: -2520.62, train_accuracy: 0.830\n",
      "Epoch: 117, train_loss: -2518.26, train_accuracy: 0.830\n",
      "Epoch: 118, train_loss: -2517.28, train_accuracy: 0.829\n",
      "Epoch: 119, train_loss: -2515.22, train_accuracy: 0.829\n",
      "Epoch: 120, train_loss: -2512.30, train_accuracy: 0.829\n",
      "Epoch: 121, train_loss: -2507.51, train_accuracy: 0.831\n",
      "Epoch: 122, train_loss: -2504.28, train_accuracy: 0.832\n",
      "Epoch: 123, train_loss: -2499.68, train_accuracy: 0.832\n",
      "Epoch: 124, train_loss: -2495.44, train_accuracy: 0.833\n",
      "Epoch: 125, train_loss: -2492.80, train_accuracy: 0.833\n",
      "Epoch: 126, train_loss: -2490.72, train_accuracy: 0.833\n",
      "Epoch: 127, train_loss: -2487.10, train_accuracy: 0.833\n",
      "Epoch: 128, train_loss: -2484.82, train_accuracy: 0.834\n",
      "Epoch: 129, train_loss: -2483.27, train_accuracy: 0.833\n",
      "Epoch: 130, train_loss: -2482.35, train_accuracy: 0.834\n",
      "Epoch: 131, train_loss: -2482.05, train_accuracy: 0.834\n",
      "Epoch: 132, train_loss: -2480.32, train_accuracy: 0.833\n",
      "Epoch: 133, train_loss: -2478.40, train_accuracy: 0.834\n",
      "Epoch: 134, train_loss: -2477.06, train_accuracy: 0.835\n",
      "Epoch: 135, train_loss: -2475.68, train_accuracy: 0.835\n",
      "Epoch: 136, train_loss: -2475.70, train_accuracy: 0.835\n",
      "Epoch: 137, train_loss: -2476.55, train_accuracy: 0.835\n",
      "Epoch: 138, train_loss: -2478.83, train_accuracy: 0.835\n",
      "Epoch: 139, train_loss: -2481.41, train_accuracy: 0.836\n",
      "Epoch: 140, train_loss: -2483.32, train_accuracy: 0.835\n",
      "Epoch: 141, train_loss: -2483.86, train_accuracy: 0.835\n",
      "Epoch: 142, train_loss: -2482.86, train_accuracy: 0.835\n",
      "Epoch: 143, train_loss: -2480.43, train_accuracy: 0.835\n",
      "Epoch: 144, train_loss: -2476.50, train_accuracy: 0.836\n",
      "Epoch: 145, train_loss: -2473.76, train_accuracy: 0.836\n",
      "Epoch: 146, train_loss: -2469.77, train_accuracy: 0.836\n",
      "Epoch: 147, train_loss: -2468.82, train_accuracy: 0.835\n",
      "Epoch: 148, train_loss: -2467.49, train_accuracy: 0.834\n",
      "Epoch: 149, train_loss: -2465.61, train_accuracy: 0.836\n",
      "Epoch: 150, train_loss: -2459.94, train_accuracy: 0.835\n",
      "Epoch: 151, train_loss: -2453.77, train_accuracy: 0.835\n",
      "Epoch: 152, train_loss: -2449.26, train_accuracy: 0.835\n",
      "Epoch: 153, train_loss: -2446.19, train_accuracy: 0.835\n",
      "Epoch: 154, train_loss: -2443.30, train_accuracy: 0.835\n",
      "Epoch: 155, train_loss: -2441.37, train_accuracy: 0.835\n",
      "Epoch: 156, train_loss: -2438.16, train_accuracy: 0.836\n",
      "Epoch: 157, train_loss: -2433.02, train_accuracy: 0.835\n",
      "Epoch: 158, train_loss: -2428.78, train_accuracy: 0.836\n",
      "Epoch: 159, train_loss: -2426.50, train_accuracy: 0.836\n",
      "Epoch: 160, train_loss: -2424.84, train_accuracy: 0.836\n",
      "Epoch: 161, train_loss: -2423.89, train_accuracy: 0.836\n",
      "Epoch: 162, train_loss: -2423.31, train_accuracy: 0.836\n",
      "Epoch: 163, train_loss: -2421.26, train_accuracy: 0.837\n",
      "Epoch: 164, train_loss: -2420.47, train_accuracy: 0.837\n",
      "Epoch: 165, train_loss: -2418.89, train_accuracy: 0.837\n",
      "Epoch: 166, train_loss: -2417.61, train_accuracy: 0.837\n",
      "Epoch: 167, train_loss: -2415.48, train_accuracy: 0.837\n",
      "Epoch: 168, train_loss: -2413.58, train_accuracy: 0.836\n",
      "Epoch: 169, train_loss: -2411.68, train_accuracy: 0.837\n",
      "Epoch: 170, train_loss: -2409.34, train_accuracy: 0.837\n",
      "Epoch: 171, train_loss: -2406.91, train_accuracy: 0.836\n",
      "Epoch: 172, train_loss: -2404.96, train_accuracy: 0.837\n",
      "Epoch: 173, train_loss: -2403.31, train_accuracy: 0.836\n",
      "Epoch: 174, train_loss: -2401.63, train_accuracy: 0.836\n",
      "Epoch: 175, train_loss: -2399.24, train_accuracy: 0.835\n",
      "Epoch: 176, train_loss: -2397.63, train_accuracy: 0.836\n",
      "Epoch: 177, train_loss: -2395.84, train_accuracy: 0.836\n",
      "Epoch: 178, train_loss: -2393.88, train_accuracy: 0.837\n",
      "Epoch: 179, train_loss: -2392.17, train_accuracy: 0.837\n",
      "Epoch: 180, train_loss: -2391.11, train_accuracy: 0.837\n",
      "Epoch: 181, train_loss: -2391.13, train_accuracy: 0.835\n",
      "Epoch: 182, train_loss: -2392.15, train_accuracy: 0.836\n",
      "Epoch: 183, train_loss: -2394.96, train_accuracy: 0.836\n",
      "Epoch: 184, train_loss: -2397.87, train_accuracy: 0.836\n",
      "Epoch: 185, train_loss: -2401.07, train_accuracy: 0.835\n",
      "Epoch: 186, train_loss: -2403.22, train_accuracy: 0.837\n",
      "Epoch: 187, train_loss: -2403.66, train_accuracy: 0.836\n",
      "Epoch: 188, train_loss: -2402.41, train_accuracy: 0.836\n",
      "Epoch: 189, train_loss: -2399.07, train_accuracy: 0.837\n",
      "Epoch: 190, train_loss: -2396.74, train_accuracy: 0.838\n",
      "Epoch: 191, train_loss: -2396.19, train_accuracy: 0.837\n",
      "Epoch: 192, train_loss: -2397.17, train_accuracy: 0.838\n",
      "Epoch: 193, train_loss: -2397.12, train_accuracy: 0.839\n",
      "Epoch: 194, train_loss: -2397.70, train_accuracy: 0.838\n",
      "Epoch: 195, train_loss: -2399.40, train_accuracy: 0.838\n",
      "Epoch: 196, train_loss: -2399.10, train_accuracy: 0.838\n",
      "Epoch: 197, train_loss: -2399.76, train_accuracy: 0.838\n",
      "Epoch: 198, train_loss: -2399.84, train_accuracy: 0.838\n",
      "Epoch: 199, train_loss: -2401.81, train_accuracy: 0.837\n",
      "Epoch: 200, train_loss: -2397.19, train_accuracy: 0.838\n",
      "Epoch: 201, train_loss: -2392.36, train_accuracy: 0.838\n",
      "Epoch: 202, train_loss: -2387.24, train_accuracy: 0.838\n",
      "Epoch: 203, train_loss: -2382.87, train_accuracy: 0.838\n",
      "Epoch: 204, train_loss: -2382.51, train_accuracy: 0.837\n",
      "Epoch: 205, train_loss: -2381.97, train_accuracy: 0.837\n",
      "Epoch: 206, train_loss: -2380.78, train_accuracy: 0.837\n",
      "Epoch: 207, train_loss: -2378.87, train_accuracy: 0.837\n",
      "Epoch: 208, train_loss: -2376.13, train_accuracy: 0.838\n",
      "Epoch: 209, train_loss: -2372.68, train_accuracy: 0.838\n",
      "Epoch: 210, train_loss: -2370.16, train_accuracy: 0.838\n",
      "Epoch: 211, train_loss: -2367.06, train_accuracy: 0.838\n",
      "Epoch: 212, train_loss: -2364.03, train_accuracy: 0.838\n",
      "Epoch: 213, train_loss: -2361.61, train_accuracy: 0.839\n",
      "Epoch: 214, train_loss: -2358.56, train_accuracy: 0.838\n",
      "Epoch: 215, train_loss: -2355.96, train_accuracy: 0.840\n",
      "Epoch: 216, train_loss: -2355.20, train_accuracy: 0.840\n",
      "Epoch: 217, train_loss: -2353.74, train_accuracy: 0.840\n",
      "Epoch: 218, train_loss: -2352.26, train_accuracy: 0.840\n",
      "Epoch: 219, train_loss: -2351.05, train_accuracy: 0.840\n",
      "Epoch: 220, train_loss: -2349.92, train_accuracy: 0.839\n",
      "Epoch: 221, train_loss: -2348.80, train_accuracy: 0.839\n",
      "Epoch: 222, train_loss: -2346.55, train_accuracy: 0.838\n",
      "Epoch: 223, train_loss: -2344.35, train_accuracy: 0.839\n",
      "Epoch: 224, train_loss: -2341.31, train_accuracy: 0.840\n",
      "Epoch: 225, train_loss: -2339.66, train_accuracy: 0.839\n",
      "Epoch: 226, train_loss: -2337.73, train_accuracy: 0.840\n",
      "Epoch: 227, train_loss: -2336.73, train_accuracy: 0.840\n",
      "Epoch: 228, train_loss: -2336.56, train_accuracy: 0.840\n",
      "Epoch: 229, train_loss: -2335.09, train_accuracy: 0.840\n",
      "Epoch: 230, train_loss: -2333.28, train_accuracy: 0.840\n",
      "Epoch: 231, train_loss: -2331.72, train_accuracy: 0.840\n",
      "Epoch: 232, train_loss: -2329.71, train_accuracy: 0.840\n",
      "Epoch: 233, train_loss: -2328.11, train_accuracy: 0.840\n",
      "Epoch: 234, train_loss: -2326.81, train_accuracy: 0.840\n",
      "Epoch: 235, train_loss: -2325.92, train_accuracy: 0.840\n",
      "Epoch: 236, train_loss: -2325.18, train_accuracy: 0.840\n",
      "Epoch: 237, train_loss: -2324.84, train_accuracy: 0.840\n",
      "Epoch: 238, train_loss: -2324.51, train_accuracy: 0.841\n",
      "Epoch: 239, train_loss: -2324.00, train_accuracy: 0.841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240, train_loss: -2323.44, train_accuracy: 0.841\n",
      "Epoch: 241, train_loss: -2322.97, train_accuracy: 0.841\n",
      "Epoch: 242, train_loss: -2322.81, train_accuracy: 0.840\n",
      "Epoch: 243, train_loss: -2322.89, train_accuracy: 0.840\n",
      "Epoch: 244, train_loss: -2323.40, train_accuracy: 0.840\n",
      "Epoch: 245, train_loss: -2323.94, train_accuracy: 0.840\n",
      "Epoch: 246, train_loss: -2324.58, train_accuracy: 0.840\n",
      "Epoch: 247, train_loss: -2325.16, train_accuracy: 0.840\n",
      "Epoch: 248, train_loss: -2324.06, train_accuracy: 0.841\n",
      "Epoch: 249, train_loss: -2322.85, train_accuracy: 0.841\n",
      "Epoch: 250, train_loss: -2322.18, train_accuracy: 0.842\n",
      "Epoch: 251, train_loss: -2322.07, train_accuracy: 0.842\n",
      "Epoch: 252, train_loss: -2321.03, train_accuracy: 0.842\n",
      "Epoch: 253, train_loss: -2320.33, train_accuracy: 0.842\n",
      "Epoch: 254, train_loss: -2320.30, train_accuracy: 0.842\n",
      "Epoch: 255, train_loss: -2317.96, train_accuracy: 0.842\n",
      "Epoch: 256, train_loss: -2315.02, train_accuracy: 0.843\n",
      "Epoch: 257, train_loss: -2313.14, train_accuracy: 0.844\n",
      "Epoch: 258, train_loss: -2312.01, train_accuracy: 0.844\n",
      "Epoch: 259, train_loss: -2308.17, train_accuracy: 0.844\n",
      "Epoch: 260, train_loss: -2304.49, train_accuracy: 0.844\n",
      "Epoch: 261, train_loss: -2300.55, train_accuracy: 0.844\n",
      "Epoch: 262, train_loss: -2297.93, train_accuracy: 0.845\n",
      "Epoch: 263, train_loss: -2296.31, train_accuracy: 0.845\n",
      "Epoch: 264, train_loss: -2294.81, train_accuracy: 0.845\n",
      "Epoch: 265, train_loss: -2293.66, train_accuracy: 0.844\n",
      "Epoch: 266, train_loss: -2293.10, train_accuracy: 0.844\n",
      "Epoch: 267, train_loss: -2291.92, train_accuracy: 0.845\n",
      "Epoch: 268, train_loss: -2291.15, train_accuracy: 0.845\n",
      "Epoch: 269, train_loss: -2290.97, train_accuracy: 0.845\n",
      "Epoch: 270, train_loss: -2290.81, train_accuracy: 0.845\n",
      "Epoch: 271, train_loss: -2290.81, train_accuracy: 0.846\n",
      "Epoch: 272, train_loss: -2289.91, train_accuracy: 0.845\n",
      "Epoch: 273, train_loss: -2288.74, train_accuracy: 0.845\n",
      "Epoch: 274, train_loss: -2288.09, train_accuracy: 0.845\n",
      "Epoch: 275, train_loss: -2287.22, train_accuracy: 0.845\n",
      "Epoch: 276, train_loss: -2284.60, train_accuracy: 0.845\n",
      "Epoch: 277, train_loss: -2282.72, train_accuracy: 0.844\n",
      "Epoch: 278, train_loss: -2281.99, train_accuracy: 0.844\n",
      "Epoch: 279, train_loss: -2282.11, train_accuracy: 0.844\n",
      "Epoch: 280, train_loss: -2282.30, train_accuracy: 0.844\n",
      "Epoch: 281, train_loss: -2284.23, train_accuracy: 0.844\n",
      "Epoch: 282, train_loss: -2287.69, train_accuracy: 0.846\n",
      "Epoch: 283, train_loss: -2293.13, train_accuracy: 0.846\n",
      "Epoch: 284, train_loss: -2299.40, train_accuracy: 0.846\n",
      "Epoch: 285, train_loss: -2303.40, train_accuracy: 0.846\n",
      "Epoch: 286, train_loss: -2307.16, train_accuracy: 0.846\n",
      "Epoch: 287, train_loss: -2310.58, train_accuracy: 0.846\n",
      "Epoch: 288, train_loss: -2314.08, train_accuracy: 0.846\n",
      "Epoch: 289, train_loss: -2317.27, train_accuracy: 0.846\n",
      "Epoch: 290, train_loss: -2320.17, train_accuracy: 0.845\n",
      "Epoch: 291, train_loss: -2322.17, train_accuracy: 0.845\n",
      "Epoch: 292, train_loss: -2324.46, train_accuracy: 0.845\n",
      "Epoch: 293, train_loss: -2326.54, train_accuracy: 0.846\n",
      "Epoch: 294, train_loss: -2328.82, train_accuracy: 0.847\n",
      "Epoch: 295, train_loss: -2331.09, train_accuracy: 0.848\n",
      "Epoch: 296, train_loss: -2331.51, train_accuracy: 0.849\n",
      "Epoch: 297, train_loss: -2329.09, train_accuracy: 0.849\n",
      "Epoch: 298, train_loss: -2326.04, train_accuracy: 0.849\n",
      "Epoch: 299, train_loss: -2323.53, train_accuracy: 0.849\n",
      "Epoch: 300, train_loss: -2320.89, train_accuracy: 0.850\n",
      "Epoch: 301, train_loss: -2322.72, train_accuracy: 0.851\n",
      "Epoch: 302, train_loss: -2326.53, train_accuracy: 0.850\n",
      "Epoch: 303, train_loss: -2328.09, train_accuracy: 0.850\n",
      "Epoch: 304, train_loss: -2325.31, train_accuracy: 0.849\n",
      "Epoch: 305, train_loss: -2317.04, train_accuracy: 0.850\n",
      "Epoch: 306, train_loss: -2307.35, train_accuracy: 0.851\n",
      "Epoch: 307, train_loss: -2301.29, train_accuracy: 0.852\n",
      "Epoch: 308, train_loss: -2290.58, train_accuracy: 0.851\n",
      "Epoch: 309, train_loss: -2281.66, train_accuracy: 0.851\n",
      "Epoch: 310, train_loss: -2275.68, train_accuracy: 0.851\n",
      "Epoch: 311, train_loss: -2272.33, train_accuracy: 0.850\n",
      "Epoch: 312, train_loss: -2273.47, train_accuracy: 0.850\n",
      "Epoch: 313, train_loss: -2277.92, train_accuracy: 0.848\n",
      "Epoch: 314, train_loss: -2286.19, train_accuracy: 0.846\n",
      "Epoch: 315, train_loss: -2296.10, train_accuracy: 0.846\n",
      "Epoch: 316, train_loss: -2310.41, train_accuracy: 0.841\n",
      "Epoch: 317, train_loss: -2322.28, train_accuracy: 0.841\n",
      "Epoch: 318, train_loss: -2329.26, train_accuracy: 0.842\n",
      "Epoch: 319, train_loss: -2330.56, train_accuracy: 0.842\n",
      "Epoch: 320, train_loss: -2328.81, train_accuracy: 0.842\n",
      "Epoch: 321, train_loss: -2323.09, train_accuracy: 0.843\n",
      "Epoch: 322, train_loss: -2313.73, train_accuracy: 0.843\n",
      "Epoch: 323, train_loss: -2301.74, train_accuracy: 0.843\n",
      "Epoch: 324, train_loss: -2295.73, train_accuracy: 0.845\n",
      "Epoch: 325, train_loss: -2288.43, train_accuracy: 0.848\n",
      "Epoch: 326, train_loss: -2280.69, train_accuracy: 0.849\n",
      "Epoch: 327, train_loss: -2273.84, train_accuracy: 0.850\n",
      "Epoch: 328, train_loss: -2267.94, train_accuracy: 0.850\n",
      "Epoch: 329, train_loss: -2263.26, train_accuracy: 0.850\n",
      "Epoch: 330, train_loss: -2259.57, train_accuracy: 0.849\n",
      "Epoch: 331, train_loss: -2257.99, train_accuracy: 0.849\n",
      "Epoch: 332, train_loss: -2259.25, train_accuracy: 0.851\n",
      "Epoch: 333, train_loss: -2260.80, train_accuracy: 0.850\n",
      "Epoch: 334, train_loss: -2262.61, train_accuracy: 0.849\n",
      "Epoch: 335, train_loss: -2265.98, train_accuracy: 0.850\n",
      "Epoch: 336, train_loss: -2271.69, train_accuracy: 0.850\n",
      "Epoch: 337, train_loss: -2279.49, train_accuracy: 0.849\n",
      "Epoch: 338, train_loss: -2282.51, train_accuracy: 0.850\n",
      "Epoch: 339, train_loss: -2282.51, train_accuracy: 0.851\n",
      "Epoch: 340, train_loss: -2276.25, train_accuracy: 0.851\n",
      "Epoch: 341, train_loss: -2273.28, train_accuracy: 0.850\n",
      "Epoch: 342, train_loss: -2267.97, train_accuracy: 0.850\n",
      "Epoch: 343, train_loss: -2261.02, train_accuracy: 0.849\n",
      "Epoch: 344, train_loss: -2257.46, train_accuracy: 0.850\n",
      "Epoch: 345, train_loss: -2253.98, train_accuracy: 0.850\n",
      "Epoch: 346, train_loss: -2249.34, train_accuracy: 0.850\n",
      "Epoch: 347, train_loss: -2248.24, train_accuracy: 0.849\n",
      "Epoch: 348, train_loss: -2248.64, train_accuracy: 0.849\n",
      "Epoch: 349, train_loss: -2247.81, train_accuracy: 0.850\n",
      "Epoch: 350, train_loss: -2249.20, train_accuracy: 0.850\n",
      "Epoch: 351, train_loss: -2249.75, train_accuracy: 0.849\n",
      "Epoch: 352, train_loss: -2248.29, train_accuracy: 0.850\n",
      "Epoch: 353, train_loss: -2244.41, train_accuracy: 0.851\n",
      "Epoch: 354, train_loss: -2238.13, train_accuracy: 0.851\n",
      "Epoch: 355, train_loss: -2233.90, train_accuracy: 0.851\n",
      "Epoch: 356, train_loss: -2232.68, train_accuracy: 0.852\n",
      "Epoch: 357, train_loss: -2231.24, train_accuracy: 0.851\n",
      "Epoch: 358, train_loss: -2225.07, train_accuracy: 0.852\n",
      "Epoch: 359, train_loss: -2220.90, train_accuracy: 0.852\n",
      "Epoch: 360, train_loss: -2218.57, train_accuracy: 0.852\n",
      "Epoch: 361, train_loss: -2218.37, train_accuracy: 0.851\n",
      "Epoch: 362, train_loss: -2220.03, train_accuracy: 0.853\n",
      "Epoch: 363, train_loss: -2220.80, train_accuracy: 0.851\n",
      "Epoch: 364, train_loss: -2220.85, train_accuracy: 0.851\n",
      "Epoch: 365, train_loss: -2221.16, train_accuracy: 0.851\n",
      "Epoch: 366, train_loss: -2222.25, train_accuracy: 0.850\n",
      "Epoch: 367, train_loss: -2223.62, train_accuracy: 0.850\n",
      "Epoch: 368, train_loss: -2222.72, train_accuracy: 0.851\n",
      "Epoch: 369, train_loss: -2222.30, train_accuracy: 0.851\n",
      "Epoch: 370, train_loss: -2220.48, train_accuracy: 0.851\n",
      "Epoch: 371, train_loss: -2218.84, train_accuracy: 0.851\n",
      "Epoch: 372, train_loss: -2218.91, train_accuracy: 0.850\n",
      "Epoch: 373, train_loss: -2217.93, train_accuracy: 0.851\n",
      "Epoch: 374, train_loss: -2216.84, train_accuracy: 0.851\n",
      "Epoch: 375, train_loss: -2215.21, train_accuracy: 0.851\n",
      "Epoch: 376, train_loss: -2211.00, train_accuracy: 0.850\n",
      "Epoch: 377, train_loss: -2206.02, train_accuracy: 0.852\n",
      "Epoch: 378, train_loss: -2203.58, train_accuracy: 0.852\n",
      "Epoch: 379, train_loss: -2202.66, train_accuracy: 0.853\n",
      "Epoch: 380, train_loss: -2202.85, train_accuracy: 0.853\n",
      "Epoch: 381, train_loss: -2202.84, train_accuracy: 0.853\n",
      "Epoch: 382, train_loss: -2204.23, train_accuracy: 0.852\n",
      "Epoch: 383, train_loss: -2206.87, train_accuracy: 0.852\n",
      "Epoch: 384, train_loss: -2209.61, train_accuracy: 0.852\n",
      "Epoch: 385, train_loss: -2212.51, train_accuracy: 0.852\n",
      "Epoch: 386, train_loss: -2214.53, train_accuracy: 0.852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 387, train_loss: -2216.31, train_accuracy: 0.851\n",
      "Epoch: 388, train_loss: -2216.62, train_accuracy: 0.851\n",
      "Epoch: 389, train_loss: -2218.10, train_accuracy: 0.851\n",
      "Epoch: 390, train_loss: -2217.62, train_accuracy: 0.852\n",
      "Epoch: 391, train_loss: -2218.21, train_accuracy: 0.851\n",
      "Epoch: 392, train_loss: -2220.61, train_accuracy: 0.851\n",
      "Epoch: 393, train_loss: -2223.43, train_accuracy: 0.851\n",
      "Epoch: 394, train_loss: -2224.22, train_accuracy: 0.851\n",
      "Epoch: 395, train_loss: -2225.78, train_accuracy: 0.851\n",
      "Epoch: 396, train_loss: -2225.58, train_accuracy: 0.851\n",
      "Epoch: 397, train_loss: -2226.48, train_accuracy: 0.851\n",
      "Epoch: 398, train_loss: -2225.56, train_accuracy: 0.851\n",
      "Epoch: 399, train_loss: -2226.57, train_accuracy: 0.850\n",
      "Epoch: 400, train_loss: -2226.28, train_accuracy: 0.850\n",
      "Epoch: 401, train_loss: -2223.53, train_accuracy: 0.849\n",
      "Epoch: 402, train_loss: -2219.05, train_accuracy: 0.850\n",
      "Epoch: 403, train_loss: -2213.90, train_accuracy: 0.851\n",
      "Epoch: 404, train_loss: -2211.32, train_accuracy: 0.851\n",
      "Epoch: 405, train_loss: -2208.53, train_accuracy: 0.852\n",
      "Epoch: 406, train_loss: -2204.89, train_accuracy: 0.852\n",
      "Epoch: 407, train_loss: -2201.23, train_accuracy: 0.852\n",
      "Epoch: 408, train_loss: -2199.09, train_accuracy: 0.852\n",
      "Epoch: 409, train_loss: -2196.61, train_accuracy: 0.853\n",
      "Epoch: 410, train_loss: -2193.53, train_accuracy: 0.853\n",
      "Epoch: 411, train_loss: -2190.20, train_accuracy: 0.853\n",
      "Epoch: 412, train_loss: -2188.24, train_accuracy: 0.853\n",
      "Epoch: 413, train_loss: -2186.00, train_accuracy: 0.853\n",
      "Epoch: 414, train_loss: -2185.17, train_accuracy: 0.853\n",
      "Epoch: 415, train_loss: -2185.00, train_accuracy: 0.853\n",
      "Epoch: 416, train_loss: -2185.20, train_accuracy: 0.853\n",
      "Epoch: 417, train_loss: -2181.29, train_accuracy: 0.853\n",
      "Epoch: 418, train_loss: -2179.68, train_accuracy: 0.853\n",
      "Epoch: 419, train_loss: -2179.19, train_accuracy: 0.853\n",
      "Epoch: 420, train_loss: -2181.23, train_accuracy: 0.855\n",
      "Epoch: 421, train_loss: -2183.66, train_accuracy: 0.856\n",
      "Epoch: 422, train_loss: -2185.18, train_accuracy: 0.856\n",
      "Epoch: 423, train_loss: -2184.02, train_accuracy: 0.856\n",
      "Epoch: 424, train_loss: -2177.58, train_accuracy: 0.856\n",
      "Epoch: 425, train_loss: -2172.96, train_accuracy: 0.855\n",
      "Epoch: 426, train_loss: -2169.76, train_accuracy: 0.856\n",
      "Epoch: 427, train_loss: -2169.06, train_accuracy: 0.856\n",
      "Epoch: 428, train_loss: -2170.80, train_accuracy: 0.856\n",
      "Epoch: 429, train_loss: -2174.30, train_accuracy: 0.856\n",
      "Epoch: 430, train_loss: -2178.05, train_accuracy: 0.856\n",
      "Epoch: 431, train_loss: -2181.23, train_accuracy: 0.856\n",
      "Epoch: 432, train_loss: -2186.60, train_accuracy: 0.856\n",
      "Epoch: 433, train_loss: -2190.86, train_accuracy: 0.856\n",
      "Epoch: 434, train_loss: -2192.11, train_accuracy: 0.856\n",
      "Epoch: 435, train_loss: -2190.91, train_accuracy: 0.856\n",
      "Epoch: 436, train_loss: -2190.99, train_accuracy: 0.856\n",
      "Epoch: 437, train_loss: -2186.00, train_accuracy: 0.855\n",
      "Epoch: 438, train_loss: -2179.78, train_accuracy: 0.855\n",
      "Epoch: 439, train_loss: -2172.91, train_accuracy: 0.855\n",
      "Epoch: 440, train_loss: -2166.19, train_accuracy: 0.855\n",
      "Epoch: 441, train_loss: -2159.13, train_accuracy: 0.854\n",
      "Epoch: 442, train_loss: -2155.65, train_accuracy: 0.855\n",
      "Epoch: 443, train_loss: -2154.65, train_accuracy: 0.853\n",
      "Epoch: 444, train_loss: -2155.21, train_accuracy: 0.854\n",
      "Epoch: 445, train_loss: -2156.22, train_accuracy: 0.854\n",
      "Epoch: 446, train_loss: -2158.80, train_accuracy: 0.854\n",
      "Epoch: 447, train_loss: -2163.10, train_accuracy: 0.854\n",
      "Epoch: 448, train_loss: -2166.16, train_accuracy: 0.854\n",
      "Epoch: 449, train_loss: -2169.12, train_accuracy: 0.854\n",
      "Epoch: 450, train_loss: -2170.15, train_accuracy: 0.853\n",
      "Epoch: 451, train_loss: -2169.78, train_accuracy: 0.853\n",
      "Epoch: 452, train_loss: -2169.59, train_accuracy: 0.852\n",
      "Epoch: 453, train_loss: -2171.06, train_accuracy: 0.851\n",
      "Epoch: 454, train_loss: -2174.52, train_accuracy: 0.850\n",
      "Epoch: 455, train_loss: -2178.19, train_accuracy: 0.849\n",
      "Epoch: 456, train_loss: -2181.66, train_accuracy: 0.848\n",
      "Epoch: 457, train_loss: -2188.64, train_accuracy: 0.848\n",
      "Epoch: 458, train_loss: -2193.98, train_accuracy: 0.848\n",
      "Epoch: 459, train_loss: -2197.92, train_accuracy: 0.847\n",
      "Epoch: 460, train_loss: -2205.00, train_accuracy: 0.847\n",
      "Epoch: 461, train_loss: -2209.97, train_accuracy: 0.847\n",
      "Epoch: 462, train_loss: -2215.15, train_accuracy: 0.847\n",
      "Epoch: 463, train_loss: -2214.79, train_accuracy: 0.846\n",
      "Epoch: 464, train_loss: -2212.92, train_accuracy: 0.847\n",
      "Epoch: 465, train_loss: -2207.29, train_accuracy: 0.848\n",
      "Epoch: 466, train_loss: -2204.53, train_accuracy: 0.848\n",
      "Epoch: 467, train_loss: -2200.77, train_accuracy: 0.848\n",
      "Epoch: 468, train_loss: -2195.53, train_accuracy: 0.849\n",
      "Epoch: 469, train_loss: -2191.31, train_accuracy: 0.850\n",
      "Epoch: 470, train_loss: -2184.36, train_accuracy: 0.850\n",
      "Epoch: 471, train_loss: -2178.04, train_accuracy: 0.852\n",
      "Epoch: 472, train_loss: -2174.27, train_accuracy: 0.853\n",
      "Epoch: 473, train_loss: -2175.21, train_accuracy: 0.852\n",
      "Epoch: 474, train_loss: -2177.05, train_accuracy: 0.852\n",
      "Epoch: 475, train_loss: -2177.65, train_accuracy: 0.852\n",
      "Epoch: 476, train_loss: -2172.71, train_accuracy: 0.853\n",
      "Epoch: 477, train_loss: -2167.02, train_accuracy: 0.854\n",
      "Epoch: 478, train_loss: -2162.52, train_accuracy: 0.854\n",
      "Epoch: 479, train_loss: -2161.35, train_accuracy: 0.854\n",
      "Epoch: 480, train_loss: -2157.53, train_accuracy: 0.853\n",
      "Epoch: 481, train_loss: -2155.07, train_accuracy: 0.853\n",
      "Epoch: 482, train_loss: -2153.82, train_accuracy: 0.853\n",
      "Epoch: 483, train_loss: -2152.02, train_accuracy: 0.853\n",
      "Epoch: 484, train_loss: -2148.13, train_accuracy: 0.853\n",
      "Epoch: 485, train_loss: -2142.67, train_accuracy: 0.854\n",
      "Epoch: 486, train_loss: -2139.29, train_accuracy: 0.854\n",
      "Epoch: 487, train_loss: -2138.71, train_accuracy: 0.854\n",
      "Epoch: 488, train_loss: -2138.50, train_accuracy: 0.854\n",
      "Epoch: 489, train_loss: -2137.86, train_accuracy: 0.854\n",
      "Epoch: 490, train_loss: -2139.52, train_accuracy: 0.854\n",
      "Epoch: 491, train_loss: -2139.36, train_accuracy: 0.854\n",
      "Epoch: 492, train_loss: -2140.03, train_accuracy: 0.854\n",
      "Epoch: 493, train_loss: -2140.31, train_accuracy: 0.855\n",
      "Epoch: 494, train_loss: -2140.67, train_accuracy: 0.854\n",
      "Epoch: 495, train_loss: -2142.37, train_accuracy: 0.854\n",
      "Epoch: 496, train_loss: -2142.73, train_accuracy: 0.855\n",
      "Epoch: 497, train_loss: -2140.22, train_accuracy: 0.855\n",
      "Epoch: 498, train_loss: -2136.95, train_accuracy: 0.855\n",
      "Epoch: 499, train_loss: -2131.99, train_accuracy: 0.856\n",
      "Epoch: 500, train_loss: -2126.21, train_accuracy: 0.856\n",
      "Epoch: 1, train_loss: -5035.65, train_accuracy: 0.483\n",
      "Epoch: 2, train_loss: -4992.63, train_accuracy: 0.483\n",
      "Epoch: 3, train_loss: -4944.99, train_accuracy: 0.483\n",
      "Epoch: 4, train_loss: -4893.57, train_accuracy: 0.483\n",
      "Epoch: 5, train_loss: -4843.16, train_accuracy: 0.483\n",
      "Epoch: 6, train_loss: -4798.09, train_accuracy: 0.483\n",
      "Epoch: 7, train_loss: -4751.08, train_accuracy: 0.483\n",
      "Epoch: 8, train_loss: -4702.65, train_accuracy: 0.483\n",
      "Epoch: 9, train_loss: -4655.34, train_accuracy: 0.483\n",
      "Epoch: 10, train_loss: -4611.83, train_accuracy: 0.483\n",
      "Epoch: 11, train_loss: -4574.89, train_accuracy: 0.483\n",
      "Epoch: 12, train_loss: -4544.91, train_accuracy: 0.483\n",
      "Epoch: 13, train_loss: -4520.61, train_accuracy: 0.483\n",
      "Epoch: 14, train_loss: -4502.19, train_accuracy: 0.483\n",
      "Epoch: 15, train_loss: -4488.55, train_accuracy: 0.483\n",
      "Epoch: 16, train_loss: -4478.84, train_accuracy: 0.483\n",
      "Epoch: 17, train_loss: -4471.32, train_accuracy: 0.483\n",
      "Epoch: 18, train_loss: -4465.72, train_accuracy: 0.483\n",
      "Epoch: 19, train_loss: -4461.24, train_accuracy: 0.483\n",
      "Epoch: 20, train_loss: -4457.45, train_accuracy: 0.483\n",
      "Epoch: 21, train_loss: -4454.65, train_accuracy: 0.483\n",
      "Epoch: 22, train_loss: -4450.03, train_accuracy: 0.483\n",
      "Epoch: 23, train_loss: -4445.60, train_accuracy: 0.483\n",
      "Epoch: 24, train_loss: -4440.06, train_accuracy: 0.483\n",
      "Epoch: 25, train_loss: -4435.39, train_accuracy: 0.483\n",
      "Epoch: 26, train_loss: -4429.17, train_accuracy: 0.483\n",
      "Epoch: 27, train_loss: -4421.97, train_accuracy: 0.483\n",
      "Epoch: 28, train_loss: -4414.56, train_accuracy: 0.483\n",
      "Epoch: 29, train_loss: -4405.56, train_accuracy: 0.483\n",
      "Epoch: 30, train_loss: -4396.50, train_accuracy: 0.483\n",
      "Epoch: 31, train_loss: -4387.18, train_accuracy: 0.483\n",
      "Epoch: 32, train_loss: -4377.73, train_accuracy: 0.483\n",
      "Epoch: 33, train_loss: -4368.10, train_accuracy: 0.483\n",
      "Epoch: 34, train_loss: -4358.91, train_accuracy: 0.483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, train_loss: -4349.54, train_accuracy: 0.483\n",
      "Epoch: 36, train_loss: -4339.85, train_accuracy: 0.483\n",
      "Epoch: 37, train_loss: -4330.18, train_accuracy: 0.483\n",
      "Epoch: 38, train_loss: -4321.01, train_accuracy: 0.483\n",
      "Epoch: 39, train_loss: -4312.35, train_accuracy: 0.483\n",
      "Epoch: 40, train_loss: -4304.36, train_accuracy: 0.483\n",
      "Epoch: 41, train_loss: -4297.14, train_accuracy: 0.483\n",
      "Epoch: 42, train_loss: -4290.37, train_accuracy: 0.483\n",
      "Epoch: 43, train_loss: -4284.17, train_accuracy: 0.483\n",
      "Epoch: 44, train_loss: -4277.60, train_accuracy: 0.483\n",
      "Epoch: 45, train_loss: -4270.83, train_accuracy: 0.483\n",
      "Epoch: 46, train_loss: -4264.32, train_accuracy: 0.483\n",
      "Epoch: 47, train_loss: -4257.48, train_accuracy: 0.483\n",
      "Epoch: 48, train_loss: -4250.83, train_accuracy: 0.483\n",
      "Epoch: 49, train_loss: -4244.36, train_accuracy: 0.483\n",
      "Epoch: 50, train_loss: -4237.50, train_accuracy: 0.483\n",
      "Epoch: 51, train_loss: -4230.25, train_accuracy: 0.483\n",
      "Epoch: 52, train_loss: -4222.90, train_accuracy: 0.483\n",
      "Epoch: 53, train_loss: -4215.47, train_accuracy: 0.483\n",
      "Epoch: 54, train_loss: -4207.59, train_accuracy: 0.483\n",
      "Epoch: 55, train_loss: -4199.63, train_accuracy: 0.483\n",
      "Epoch: 56, train_loss: -4191.50, train_accuracy: 0.483\n",
      "Epoch: 57, train_loss: -4182.89, train_accuracy: 0.483\n",
      "Epoch: 58, train_loss: -4174.02, train_accuracy: 0.483\n",
      "Epoch: 59, train_loss: -4164.98, train_accuracy: 0.483\n",
      "Epoch: 60, train_loss: -4155.60, train_accuracy: 0.483\n",
      "Epoch: 61, train_loss: -4146.09, train_accuracy: 0.483\n",
      "Epoch: 62, train_loss: -4136.91, train_accuracy: 0.483\n",
      "Epoch: 63, train_loss: -4127.64, train_accuracy: 0.483\n",
      "Epoch: 64, train_loss: -4118.62, train_accuracy: 0.483\n",
      "Epoch: 65, train_loss: -4110.01, train_accuracy: 0.483\n",
      "Epoch: 66, train_loss: -4101.20, train_accuracy: 0.483\n",
      "Epoch: 67, train_loss: -4091.59, train_accuracy: 0.483\n",
      "Epoch: 68, train_loss: -4080.90, train_accuracy: 0.483\n",
      "Epoch: 69, train_loss: -4069.78, train_accuracy: 0.483\n",
      "Epoch: 70, train_loss: -4058.40, train_accuracy: 0.483\n",
      "Epoch: 71, train_loss: -4046.88, train_accuracy: 0.483\n",
      "Epoch: 72, train_loss: -4034.66, train_accuracy: 0.483\n",
      "Epoch: 73, train_loss: -4022.03, train_accuracy: 0.483\n",
      "Epoch: 74, train_loss: -4009.38, train_accuracy: 0.483\n",
      "Epoch: 75, train_loss: -3995.95, train_accuracy: 0.483\n",
      "Epoch: 76, train_loss: -3982.29, train_accuracy: 0.804\n",
      "Epoch: 77, train_loss: -3969.13, train_accuracy: 0.805\n",
      "Epoch: 78, train_loss: -3956.19, train_accuracy: 0.804\n",
      "Epoch: 79, train_loss: -3943.61, train_accuracy: 0.803\n",
      "Epoch: 80, train_loss: -3931.89, train_accuracy: 0.803\n",
      "Epoch: 81, train_loss: -3920.52, train_accuracy: 0.803\n",
      "Epoch: 82, train_loss: -3911.56, train_accuracy: 0.802\n",
      "Epoch: 83, train_loss: -3902.56, train_accuracy: 0.800\n",
      "Epoch: 84, train_loss: -3895.22, train_accuracy: 0.797\n",
      "Epoch: 85, train_loss: -3887.70, train_accuracy: 0.796\n",
      "Epoch: 86, train_loss: -3880.76, train_accuracy: 0.796\n",
      "Epoch: 87, train_loss: -3871.70, train_accuracy: 0.796\n",
      "Epoch: 88, train_loss: -3863.16, train_accuracy: 0.798\n",
      "Epoch: 89, train_loss: -3855.52, train_accuracy: 0.798\n",
      "Epoch: 90, train_loss: -3847.26, train_accuracy: 0.800\n",
      "Epoch: 91, train_loss: -3839.28, train_accuracy: 0.800\n",
      "Epoch: 92, train_loss: -3829.76, train_accuracy: 0.803\n",
      "Epoch: 93, train_loss: -3821.02, train_accuracy: 0.806\n",
      "Epoch: 94, train_loss: -3813.48, train_accuracy: 0.810\n",
      "Epoch: 95, train_loss: -3806.58, train_accuracy: 0.811\n",
      "Epoch: 96, train_loss: -3798.96, train_accuracy: 0.812\n",
      "Epoch: 97, train_loss: -3792.18, train_accuracy: 0.812\n",
      "Epoch: 98, train_loss: -3786.18, train_accuracy: 0.811\n",
      "Epoch: 99, train_loss: -3780.90, train_accuracy: 0.810\n",
      "Epoch: 100, train_loss: -3776.20, train_accuracy: 0.810\n",
      "Epoch: 101, train_loss: -3769.95, train_accuracy: 0.810\n",
      "Epoch: 102, train_loss: -3762.87, train_accuracy: 0.810\n",
      "Epoch: 103, train_loss: -3755.66, train_accuracy: 0.810\n",
      "Epoch: 104, train_loss: -3746.08, train_accuracy: 0.811\n",
      "Epoch: 105, train_loss: -3735.42, train_accuracy: 0.812\n",
      "Epoch: 106, train_loss: -3723.65, train_accuracy: 0.814\n",
      "Epoch: 107, train_loss: -3711.23, train_accuracy: 0.816\n",
      "Epoch: 108, train_loss: -3699.75, train_accuracy: 0.817\n",
      "Epoch: 109, train_loss: -3689.19, train_accuracy: 0.819\n",
      "Epoch: 110, train_loss: -3679.33, train_accuracy: 0.819\n",
      "Epoch: 111, train_loss: -3669.26, train_accuracy: 0.820\n",
      "Epoch: 112, train_loss: -3659.16, train_accuracy: 0.821\n",
      "Epoch: 113, train_loss: -3649.76, train_accuracy: 0.822\n",
      "Epoch: 114, train_loss: -3641.34, train_accuracy: 0.822\n",
      "Epoch: 115, train_loss: -3633.72, train_accuracy: 0.822\n",
      "Epoch: 116, train_loss: -3626.67, train_accuracy: 0.822\n",
      "Epoch: 117, train_loss: -3618.08, train_accuracy: 0.821\n",
      "Epoch: 118, train_loss: -3609.72, train_accuracy: 0.821\n",
      "Epoch: 119, train_loss: -3600.99, train_accuracy: 0.822\n",
      "Epoch: 120, train_loss: -3593.77, train_accuracy: 0.823\n",
      "Epoch: 121, train_loss: -3586.59, train_accuracy: 0.823\n",
      "Epoch: 122, train_loss: -3580.25, train_accuracy: 0.824\n",
      "Epoch: 123, train_loss: -3575.01, train_accuracy: 0.824\n",
      "Epoch: 124, train_loss: -3569.14, train_accuracy: 0.824\n",
      "Epoch: 125, train_loss: -3564.07, train_accuracy: 0.825\n",
      "Epoch: 126, train_loss: -3559.13, train_accuracy: 0.826\n",
      "Epoch: 127, train_loss: -3554.95, train_accuracy: 0.827\n",
      "Epoch: 128, train_loss: -3551.78, train_accuracy: 0.828\n",
      "Epoch: 129, train_loss: -3548.63, train_accuracy: 0.827\n",
      "Epoch: 130, train_loss: -3544.93, train_accuracy: 0.827\n",
      "Epoch: 131, train_loss: -3540.33, train_accuracy: 0.827\n",
      "Epoch: 132, train_loss: -3535.88, train_accuracy: 0.827\n",
      "Epoch: 133, train_loss: -3533.84, train_accuracy: 0.828\n",
      "Epoch: 134, train_loss: -3531.91, train_accuracy: 0.826\n",
      "Epoch: 135, train_loss: -3529.02, train_accuracy: 0.826\n",
      "Epoch: 136, train_loss: -3526.22, train_accuracy: 0.826\n",
      "Epoch: 137, train_loss: -3523.14, train_accuracy: 0.826\n",
      "Epoch: 138, train_loss: -3520.16, train_accuracy: 0.826\n",
      "Epoch: 139, train_loss: -3516.43, train_accuracy: 0.826\n",
      "Epoch: 140, train_loss: -3511.87, train_accuracy: 0.826\n",
      "Epoch: 141, train_loss: -3507.18, train_accuracy: 0.827\n",
      "Epoch: 142, train_loss: -3502.11, train_accuracy: 0.826\n",
      "Epoch: 143, train_loss: -3495.31, train_accuracy: 0.827\n",
      "Epoch: 144, train_loss: -3487.68, train_accuracy: 0.828\n",
      "Epoch: 145, train_loss: -3480.15, train_accuracy: 0.827\n",
      "Epoch: 146, train_loss: -3472.39, train_accuracy: 0.826\n",
      "Epoch: 147, train_loss: -3465.19, train_accuracy: 0.826\n",
      "Epoch: 148, train_loss: -3457.53, train_accuracy: 0.827\n",
      "Epoch: 149, train_loss: -3453.19, train_accuracy: 0.826\n",
      "Epoch: 150, train_loss: -3447.38, train_accuracy: 0.827\n",
      "Epoch: 151, train_loss: -3441.66, train_accuracy: 0.827\n",
      "Epoch: 152, train_loss: -3434.78, train_accuracy: 0.827\n",
      "Epoch: 153, train_loss: -3427.03, train_accuracy: 0.827\n",
      "Epoch: 154, train_loss: -3420.37, train_accuracy: 0.829\n",
      "Epoch: 155, train_loss: -3414.77, train_accuracy: 0.828\n",
      "Epoch: 156, train_loss: -3409.39, train_accuracy: 0.828\n",
      "Epoch: 157, train_loss: -3404.61, train_accuracy: 0.828\n",
      "Epoch: 158, train_loss: -3400.37, train_accuracy: 0.828\n",
      "Epoch: 159, train_loss: -3397.24, train_accuracy: 0.829\n",
      "Epoch: 160, train_loss: -3394.24, train_accuracy: 0.830\n",
      "Epoch: 161, train_loss: -3390.91, train_accuracy: 0.831\n",
      "Epoch: 162, train_loss: -3388.74, train_accuracy: 0.830\n",
      "Epoch: 163, train_loss: -3385.56, train_accuracy: 0.830\n",
      "Epoch: 164, train_loss: -3381.51, train_accuracy: 0.829\n",
      "Epoch: 165, train_loss: -3377.45, train_accuracy: 0.830\n",
      "Epoch: 166, train_loss: -3372.88, train_accuracy: 0.828\n",
      "Epoch: 167, train_loss: -3369.08, train_accuracy: 0.829\n",
      "Epoch: 168, train_loss: -3365.83, train_accuracy: 0.830\n",
      "Epoch: 169, train_loss: -3363.09, train_accuracy: 0.830\n",
      "Epoch: 170, train_loss: -3360.95, train_accuracy: 0.831\n",
      "Epoch: 171, train_loss: -3359.18, train_accuracy: 0.830\n",
      "Epoch: 172, train_loss: -3358.14, train_accuracy: 0.830\n",
      "Epoch: 173, train_loss: -3357.94, train_accuracy: 0.830\n",
      "Epoch: 174, train_loss: -3358.62, train_accuracy: 0.830\n",
      "Epoch: 175, train_loss: -3357.29, train_accuracy: 0.830\n",
      "Epoch: 176, train_loss: -3356.39, train_accuracy: 0.830\n",
      "Epoch: 177, train_loss: -3356.18, train_accuracy: 0.831\n",
      "Epoch: 178, train_loss: -3355.41, train_accuracy: 0.830\n",
      "Epoch: 179, train_loss: -3356.07, train_accuracy: 0.830\n",
      "Epoch: 180, train_loss: -3356.13, train_accuracy: 0.830\n",
      "Epoch: 181, train_loss: -3353.97, train_accuracy: 0.830\n",
      "Epoch: 182, train_loss: -3351.13, train_accuracy: 0.831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 183, train_loss: -3348.70, train_accuracy: 0.831\n",
      "Epoch: 184, train_loss: -3348.07, train_accuracy: 0.831\n",
      "Epoch: 185, train_loss: -3346.13, train_accuracy: 0.832\n",
      "Epoch: 186, train_loss: -3342.23, train_accuracy: 0.831\n",
      "Epoch: 187, train_loss: -3338.83, train_accuracy: 0.832\n",
      "Epoch: 188, train_loss: -3335.43, train_accuracy: 0.832\n",
      "Epoch: 189, train_loss: -3332.41, train_accuracy: 0.832\n",
      "Epoch: 190, train_loss: -3327.73, train_accuracy: 0.833\n",
      "Epoch: 191, train_loss: -3324.61, train_accuracy: 0.833\n",
      "Epoch: 192, train_loss: -3324.25, train_accuracy: 0.832\n",
      "Epoch: 193, train_loss: -3325.45, train_accuracy: 0.834\n",
      "Epoch: 194, train_loss: -3327.02, train_accuracy: 0.834\n",
      "Epoch: 195, train_loss: -3329.15, train_accuracy: 0.834\n",
      "Epoch: 196, train_loss: -3334.02, train_accuracy: 0.833\n",
      "Epoch: 197, train_loss: -3335.85, train_accuracy: 0.833\n",
      "Epoch: 198, train_loss: -3336.42, train_accuracy: 0.833\n",
      "Epoch: 199, train_loss: -3335.04, train_accuracy: 0.833\n",
      "Epoch: 200, train_loss: -3335.12, train_accuracy: 0.833\n",
      "Epoch: 201, train_loss: -3334.17, train_accuracy: 0.833\n",
      "Epoch: 202, train_loss: -3329.65, train_accuracy: 0.832\n",
      "Epoch: 203, train_loss: -3327.91, train_accuracy: 0.832\n",
      "Epoch: 204, train_loss: -3321.58, train_accuracy: 0.832\n",
      "Epoch: 205, train_loss: -3314.92, train_accuracy: 0.833\n",
      "Epoch: 206, train_loss: -3306.51, train_accuracy: 0.833\n",
      "Epoch: 207, train_loss: -3299.38, train_accuracy: 0.834\n",
      "Epoch: 208, train_loss: -3285.93, train_accuracy: 0.834\n",
      "Epoch: 209, train_loss: -3274.69, train_accuracy: 0.834\n",
      "Epoch: 210, train_loss: -3265.78, train_accuracy: 0.833\n",
      "Epoch: 211, train_loss: -3259.41, train_accuracy: 0.834\n",
      "Epoch: 212, train_loss: -3252.56, train_accuracy: 0.835\n",
      "Epoch: 213, train_loss: -3246.22, train_accuracy: 0.836\n",
      "Epoch: 214, train_loss: -3241.32, train_accuracy: 0.836\n",
      "Epoch: 215, train_loss: -3237.58, train_accuracy: 0.836\n",
      "Epoch: 216, train_loss: -3233.57, train_accuracy: 0.836\n",
      "Epoch: 217, train_loss: -3229.74, train_accuracy: 0.836\n",
      "Epoch: 218, train_loss: -3226.52, train_accuracy: 0.837\n",
      "Epoch: 219, train_loss: -3222.90, train_accuracy: 0.837\n",
      "Epoch: 220, train_loss: -3219.87, train_accuracy: 0.837\n",
      "Epoch: 221, train_loss: -3216.82, train_accuracy: 0.836\n",
      "Epoch: 222, train_loss: -3213.42, train_accuracy: 0.836\n",
      "Epoch: 223, train_loss: -3210.78, train_accuracy: 0.836\n",
      "Epoch: 224, train_loss: -3208.43, train_accuracy: 0.837\n",
      "Epoch: 225, train_loss: -3206.28, train_accuracy: 0.837\n",
      "Epoch: 226, train_loss: -3204.27, train_accuracy: 0.837\n",
      "Epoch: 227, train_loss: -3202.04, train_accuracy: 0.837\n",
      "Epoch: 228, train_loss: -3200.03, train_accuracy: 0.838\n",
      "Epoch: 229, train_loss: -3198.52, train_accuracy: 0.837\n",
      "Epoch: 230, train_loss: -3197.24, train_accuracy: 0.837\n",
      "Epoch: 231, train_loss: -3196.05, train_accuracy: 0.838\n",
      "Epoch: 232, train_loss: -3195.64, train_accuracy: 0.837\n",
      "Epoch: 233, train_loss: -3195.08, train_accuracy: 0.838\n",
      "Epoch: 234, train_loss: -3194.90, train_accuracy: 0.838\n",
      "Epoch: 235, train_loss: -3193.60, train_accuracy: 0.838\n",
      "Epoch: 236, train_loss: -3193.70, train_accuracy: 0.838\n",
      "Epoch: 237, train_loss: -3192.86, train_accuracy: 0.838\n",
      "Epoch: 238, train_loss: -3190.36, train_accuracy: 0.838\n",
      "Epoch: 239, train_loss: -3186.62, train_accuracy: 0.838\n",
      "Epoch: 240, train_loss: -3181.02, train_accuracy: 0.837\n",
      "Epoch: 241, train_loss: -3176.51, train_accuracy: 0.838\n",
      "Epoch: 242, train_loss: -3171.36, train_accuracy: 0.838\n",
      "Epoch: 243, train_loss: -3168.07, train_accuracy: 0.838\n",
      "Epoch: 244, train_loss: -3165.30, train_accuracy: 0.837\n",
      "Epoch: 245, train_loss: -3161.72, train_accuracy: 0.838\n",
      "Epoch: 246, train_loss: -3158.35, train_accuracy: 0.838\n",
      "Epoch: 247, train_loss: -3155.47, train_accuracy: 0.838\n",
      "Epoch: 248, train_loss: -3152.74, train_accuracy: 0.838\n",
      "Epoch: 249, train_loss: -3150.27, train_accuracy: 0.837\n",
      "Epoch: 250, train_loss: -3147.78, train_accuracy: 0.837\n",
      "Epoch: 251, train_loss: -3145.10, train_accuracy: 0.838\n",
      "Epoch: 252, train_loss: -3142.65, train_accuracy: 0.838\n",
      "Epoch: 253, train_loss: -3140.39, train_accuracy: 0.836\n",
      "Epoch: 254, train_loss: -3138.16, train_accuracy: 0.837\n",
      "Epoch: 255, train_loss: -3136.23, train_accuracy: 0.836\n",
      "Epoch: 256, train_loss: -3134.15, train_accuracy: 0.836\n",
      "Epoch: 257, train_loss: -3132.17, train_accuracy: 0.836\n",
      "Epoch: 258, train_loss: -3130.59, train_accuracy: 0.836\n",
      "Epoch: 259, train_loss: -3128.93, train_accuracy: 0.837\n",
      "Epoch: 260, train_loss: -3127.59, train_accuracy: 0.837\n",
      "Epoch: 261, train_loss: -3127.91, train_accuracy: 0.836\n",
      "Epoch: 262, train_loss: -3129.08, train_accuracy: 0.836\n",
      "Epoch: 263, train_loss: -3130.35, train_accuracy: 0.835\n",
      "Epoch: 264, train_loss: -3129.21, train_accuracy: 0.835\n",
      "Epoch: 265, train_loss: -3127.60, train_accuracy: 0.834\n",
      "Epoch: 266, train_loss: -3125.65, train_accuracy: 0.834\n",
      "Epoch: 267, train_loss: -3124.39, train_accuracy: 0.835\n",
      "Epoch: 268, train_loss: -3122.41, train_accuracy: 0.835\n",
      "Epoch: 269, train_loss: -3120.13, train_accuracy: 0.834\n",
      "Epoch: 270, train_loss: -3117.97, train_accuracy: 0.834\n",
      "Epoch: 271, train_loss: -3116.60, train_accuracy: 0.834\n",
      "Epoch: 272, train_loss: -3114.04, train_accuracy: 0.833\n",
      "Epoch: 273, train_loss: -3111.37, train_accuracy: 0.834\n",
      "Epoch: 274, train_loss: -3109.35, train_accuracy: 0.835\n",
      "Epoch: 275, train_loss: -3106.58, train_accuracy: 0.835\n",
      "Epoch: 276, train_loss: -3104.28, train_accuracy: 0.835\n",
      "Epoch: 277, train_loss: -3101.35, train_accuracy: 0.836\n",
      "Epoch: 278, train_loss: -3098.90, train_accuracy: 0.837\n",
      "Epoch: 279, train_loss: -3096.90, train_accuracy: 0.836\n",
      "Epoch: 280, train_loss: -3093.69, train_accuracy: 0.835\n",
      "Epoch: 281, train_loss: -3089.94, train_accuracy: 0.835\n",
      "Epoch: 282, train_loss: -3086.53, train_accuracy: 0.834\n",
      "Epoch: 283, train_loss: -3083.23, train_accuracy: 0.836\n",
      "Epoch: 284, train_loss: -3080.16, train_accuracy: 0.836\n",
      "Epoch: 285, train_loss: -3077.30, train_accuracy: 0.838\n",
      "Epoch: 286, train_loss: -3074.26, train_accuracy: 0.839\n",
      "Epoch: 287, train_loss: -3071.41, train_accuracy: 0.838\n",
      "Epoch: 288, train_loss: -3068.75, train_accuracy: 0.839\n",
      "Epoch: 289, train_loss: -3066.55, train_accuracy: 0.840\n",
      "Epoch: 290, train_loss: -3065.29, train_accuracy: 0.840\n",
      "Epoch: 291, train_loss: -3064.65, train_accuracy: 0.840\n",
      "Epoch: 292, train_loss: -3064.88, train_accuracy: 0.840\n",
      "Epoch: 293, train_loss: -3066.70, train_accuracy: 0.839\n",
      "Epoch: 294, train_loss: -3067.96, train_accuracy: 0.838\n",
      "Epoch: 295, train_loss: -3067.67, train_accuracy: 0.837\n",
      "Epoch: 296, train_loss: -3064.24, train_accuracy: 0.836\n",
      "Epoch: 297, train_loss: -3062.15, train_accuracy: 0.836\n",
      "Epoch: 298, train_loss: -3061.27, train_accuracy: 0.837\n",
      "Epoch: 299, train_loss: -3060.52, train_accuracy: 0.837\n",
      "Epoch: 300, train_loss: -3059.82, train_accuracy: 0.836\n",
      "Epoch: 301, train_loss: -3058.10, train_accuracy: 0.837\n",
      "Epoch: 302, train_loss: -3058.32, train_accuracy: 0.836\n",
      "Epoch: 303, train_loss: -3060.50, train_accuracy: 0.837\n",
      "Epoch: 304, train_loss: -3060.48, train_accuracy: 0.836\n",
      "Epoch: 305, train_loss: -3059.24, train_accuracy: 0.836\n",
      "Epoch: 306, train_loss: -3057.71, train_accuracy: 0.836\n",
      "Epoch: 307, train_loss: -3054.50, train_accuracy: 0.838\n",
      "Epoch: 308, train_loss: -3050.44, train_accuracy: 0.839\n",
      "Epoch: 309, train_loss: -3047.08, train_accuracy: 0.841\n",
      "Epoch: 310, train_loss: -3045.53, train_accuracy: 0.842\n",
      "Epoch: 311, train_loss: -3044.81, train_accuracy: 0.841\n",
      "Epoch: 312, train_loss: -3043.97, train_accuracy: 0.840\n",
      "Epoch: 313, train_loss: -3041.92, train_accuracy: 0.840\n",
      "Epoch: 314, train_loss: -3036.97, train_accuracy: 0.840\n",
      "Epoch: 315, train_loss: -3033.48, train_accuracy: 0.840\n",
      "Epoch: 316, train_loss: -3030.07, train_accuracy: 0.841\n",
      "Epoch: 317, train_loss: -3025.15, train_accuracy: 0.842\n",
      "Epoch: 318, train_loss: -3019.53, train_accuracy: 0.842\n",
      "Epoch: 319, train_loss: -3014.97, train_accuracy: 0.842\n",
      "Epoch: 320, train_loss: -3012.16, train_accuracy: 0.843\n",
      "Epoch: 321, train_loss: -3008.88, train_accuracy: 0.843\n",
      "Epoch: 322, train_loss: -3006.67, train_accuracy: 0.843\n",
      "Epoch: 323, train_loss: -3001.61, train_accuracy: 0.843\n",
      "Epoch: 324, train_loss: -2996.47, train_accuracy: 0.843\n",
      "Epoch: 325, train_loss: -2990.56, train_accuracy: 0.844\n",
      "Epoch: 326, train_loss: -2985.93, train_accuracy: 0.844\n",
      "Epoch: 327, train_loss: -2981.01, train_accuracy: 0.844\n",
      "Epoch: 328, train_loss: -2976.60, train_accuracy: 0.843\n",
      "Epoch: 329, train_loss: -2971.90, train_accuracy: 0.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330, train_loss: -2969.39, train_accuracy: 0.845\n",
      "Epoch: 331, train_loss: -2967.50, train_accuracy: 0.845\n",
      "Epoch: 332, train_loss: -2965.90, train_accuracy: 0.845\n",
      "Epoch: 333, train_loss: -2965.60, train_accuracy: 0.844\n",
      "Epoch: 334, train_loss: -2964.59, train_accuracy: 0.844\n",
      "Epoch: 335, train_loss: -2963.36, train_accuracy: 0.844\n",
      "Epoch: 336, train_loss: -2961.78, train_accuracy: 0.844\n",
      "Epoch: 337, train_loss: -2960.24, train_accuracy: 0.844\n",
      "Epoch: 338, train_loss: -2959.55, train_accuracy: 0.845\n",
      "Epoch: 339, train_loss: -2959.41, train_accuracy: 0.845\n",
      "Epoch: 340, train_loss: -2958.42, train_accuracy: 0.845\n",
      "Epoch: 341, train_loss: -2956.00, train_accuracy: 0.845\n",
      "Epoch: 342, train_loss: -2953.40, train_accuracy: 0.844\n",
      "Epoch: 343, train_loss: -2950.79, train_accuracy: 0.845\n",
      "Epoch: 344, train_loss: -2948.96, train_accuracy: 0.845\n",
      "Epoch: 345, train_loss: -2945.49, train_accuracy: 0.844\n",
      "Epoch: 346, train_loss: -2939.96, train_accuracy: 0.846\n",
      "Epoch: 347, train_loss: -2935.60, train_accuracy: 0.846\n",
      "Epoch: 348, train_loss: -2931.30, train_accuracy: 0.847\n",
      "Epoch: 349, train_loss: -2928.05, train_accuracy: 0.849\n",
      "Epoch: 350, train_loss: -2925.36, train_accuracy: 0.849\n",
      "Epoch: 351, train_loss: -2922.45, train_accuracy: 0.848\n",
      "Epoch: 352, train_loss: -2919.68, train_accuracy: 0.849\n",
      "Epoch: 353, train_loss: -2916.48, train_accuracy: 0.849\n",
      "Epoch: 354, train_loss: -2914.06, train_accuracy: 0.849\n",
      "Epoch: 355, train_loss: -2912.24, train_accuracy: 0.849\n",
      "Epoch: 356, train_loss: -2910.57, train_accuracy: 0.849\n",
      "Epoch: 357, train_loss: -2909.00, train_accuracy: 0.849\n",
      "Epoch: 358, train_loss: -2907.09, train_accuracy: 0.849\n",
      "Epoch: 359, train_loss: -2905.32, train_accuracy: 0.848\n",
      "Epoch: 360, train_loss: -2903.67, train_accuracy: 0.849\n",
      "Epoch: 361, train_loss: -2902.16, train_accuracy: 0.849\n",
      "Epoch: 362, train_loss: -2900.69, train_accuracy: 0.849\n",
      "Epoch: 363, train_loss: -2899.97, train_accuracy: 0.849\n",
      "Epoch: 364, train_loss: -2899.64, train_accuracy: 0.848\n",
      "Epoch: 365, train_loss: -2900.58, train_accuracy: 0.848\n",
      "Epoch: 366, train_loss: -2900.65, train_accuracy: 0.848\n",
      "Epoch: 367, train_loss: -2901.47, train_accuracy: 0.847\n",
      "Epoch: 368, train_loss: -2902.63, train_accuracy: 0.847\n",
      "Epoch: 369, train_loss: -2902.92, train_accuracy: 0.847\n",
      "Epoch: 370, train_loss: -2906.61, train_accuracy: 0.847\n",
      "Epoch: 371, train_loss: -2909.13, train_accuracy: 0.847\n",
      "Epoch: 372, train_loss: -2911.33, train_accuracy: 0.846\n",
      "Epoch: 373, train_loss: -2909.23, train_accuracy: 0.847\n",
      "Epoch: 374, train_loss: -2908.27, train_accuracy: 0.847\n",
      "Epoch: 375, train_loss: -2904.60, train_accuracy: 0.847\n",
      "Epoch: 376, train_loss: -2900.76, train_accuracy: 0.848\n",
      "Epoch: 377, train_loss: -2894.55, train_accuracy: 0.848\n",
      "Epoch: 378, train_loss: -2887.53, train_accuracy: 0.848\n",
      "Epoch: 379, train_loss: -2881.04, train_accuracy: 0.848\n",
      "Epoch: 380, train_loss: -2876.23, train_accuracy: 0.848\n",
      "Epoch: 381, train_loss: -2872.74, train_accuracy: 0.849\n",
      "Epoch: 382, train_loss: -2869.71, train_accuracy: 0.849\n",
      "Epoch: 383, train_loss: -2868.01, train_accuracy: 0.850\n",
      "Epoch: 384, train_loss: -2867.04, train_accuracy: 0.851\n",
      "Epoch: 385, train_loss: -2867.95, train_accuracy: 0.850\n",
      "Epoch: 386, train_loss: -2870.29, train_accuracy: 0.848\n",
      "Epoch: 387, train_loss: -2872.74, train_accuracy: 0.847\n",
      "Epoch: 388, train_loss: -2873.96, train_accuracy: 0.847\n",
      "Epoch: 389, train_loss: -2874.42, train_accuracy: 0.847\n",
      "Epoch: 390, train_loss: -2875.20, train_accuracy: 0.846\n",
      "Epoch: 391, train_loss: -2875.96, train_accuracy: 0.847\n",
      "Epoch: 392, train_loss: -2875.22, train_accuracy: 0.847\n",
      "Epoch: 393, train_loss: -2872.94, train_accuracy: 0.847\n",
      "Epoch: 394, train_loss: -2871.14, train_accuracy: 0.847\n",
      "Epoch: 395, train_loss: -2868.67, train_accuracy: 0.847\n",
      "Epoch: 396, train_loss: -2865.02, train_accuracy: 0.848\n",
      "Epoch: 397, train_loss: -2862.30, train_accuracy: 0.848\n",
      "Epoch: 398, train_loss: -2860.24, train_accuracy: 0.848\n",
      "Epoch: 399, train_loss: -2856.69, train_accuracy: 0.848\n",
      "Epoch: 400, train_loss: -2851.56, train_accuracy: 0.847\n",
      "Epoch: 401, train_loss: -2844.25, train_accuracy: 0.849\n",
      "Epoch: 402, train_loss: -2838.58, train_accuracy: 0.850\n",
      "Epoch: 403, train_loss: -2833.40, train_accuracy: 0.850\n",
      "Epoch: 404, train_loss: -2828.35, train_accuracy: 0.850\n",
      "Epoch: 405, train_loss: -2824.70, train_accuracy: 0.850\n",
      "Epoch: 406, train_loss: -2821.88, train_accuracy: 0.850\n",
      "Epoch: 407, train_loss: -2819.68, train_accuracy: 0.850\n",
      "Epoch: 408, train_loss: -2818.31, train_accuracy: 0.850\n",
      "Epoch: 409, train_loss: -2817.12, train_accuracy: 0.849\n",
      "Epoch: 410, train_loss: -2815.82, train_accuracy: 0.849\n",
      "Epoch: 411, train_loss: -2814.19, train_accuracy: 0.850\n",
      "Epoch: 412, train_loss: -2812.67, train_accuracy: 0.850\n",
      "Epoch: 413, train_loss: -2811.95, train_accuracy: 0.850\n",
      "Epoch: 414, train_loss: -2812.70, train_accuracy: 0.851\n",
      "Epoch: 415, train_loss: -2814.73, train_accuracy: 0.850\n",
      "Epoch: 416, train_loss: -2815.53, train_accuracy: 0.851\n",
      "Epoch: 417, train_loss: -2817.34, train_accuracy: 0.851\n",
      "Epoch: 418, train_loss: -2816.18, train_accuracy: 0.851\n",
      "Epoch: 419, train_loss: -2814.80, train_accuracy: 0.852\n",
      "Epoch: 420, train_loss: -2813.88, train_accuracy: 0.851\n",
      "Epoch: 421, train_loss: -2811.52, train_accuracy: 0.852\n",
      "Epoch: 422, train_loss: -2811.41, train_accuracy: 0.851\n",
      "Epoch: 423, train_loss: -2808.27, train_accuracy: 0.851\n",
      "Epoch: 424, train_loss: -2804.45, train_accuracy: 0.851\n",
      "Epoch: 425, train_loss: -2799.99, train_accuracy: 0.852\n",
      "Epoch: 426, train_loss: -2795.16, train_accuracy: 0.852\n",
      "Epoch: 427, train_loss: -2790.42, train_accuracy: 0.853\n",
      "Epoch: 428, train_loss: -2786.43, train_accuracy: 0.853\n",
      "Epoch: 429, train_loss: -2783.87, train_accuracy: 0.852\n",
      "Epoch: 430, train_loss: -2781.54, train_accuracy: 0.854\n",
      "Epoch: 431, train_loss: -2780.25, train_accuracy: 0.853\n",
      "Epoch: 432, train_loss: -2778.73, train_accuracy: 0.853\n",
      "Epoch: 433, train_loss: -2776.70, train_accuracy: 0.853\n",
      "Epoch: 434, train_loss: -2775.25, train_accuracy: 0.853\n",
      "Epoch: 435, train_loss: -2773.90, train_accuracy: 0.853\n",
      "Epoch: 436, train_loss: -2773.00, train_accuracy: 0.853\n",
      "Epoch: 437, train_loss: -2771.99, train_accuracy: 0.853\n",
      "Epoch: 438, train_loss: -2771.31, train_accuracy: 0.853\n",
      "Epoch: 439, train_loss: -2769.85, train_accuracy: 0.853\n",
      "Epoch: 440, train_loss: -2768.59, train_accuracy: 0.853\n",
      "Epoch: 441, train_loss: -2767.62, train_accuracy: 0.853\n",
      "Epoch: 442, train_loss: -2767.03, train_accuracy: 0.853\n",
      "Epoch: 443, train_loss: -2766.39, train_accuracy: 0.854\n",
      "Epoch: 444, train_loss: -2765.97, train_accuracy: 0.853\n",
      "Epoch: 445, train_loss: -2765.69, train_accuracy: 0.853\n",
      "Epoch: 446, train_loss: -2766.84, train_accuracy: 0.854\n",
      "Epoch: 447, train_loss: -2768.26, train_accuracy: 0.852\n",
      "Epoch: 448, train_loss: -2769.97, train_accuracy: 0.851\n",
      "Epoch: 449, train_loss: -2770.99, train_accuracy: 0.849\n",
      "Epoch: 450, train_loss: -2772.34, train_accuracy: 0.849\n",
      "Epoch: 451, train_loss: -2773.11, train_accuracy: 0.848\n",
      "Epoch: 452, train_loss: -2774.97, train_accuracy: 0.848\n",
      "Epoch: 453, train_loss: -2776.51, train_accuracy: 0.847\n",
      "Epoch: 454, train_loss: -2781.30, train_accuracy: 0.846\n",
      "Epoch: 455, train_loss: -2786.93, train_accuracy: 0.845\n",
      "Epoch: 456, train_loss: -2792.74, train_accuracy: 0.843\n",
      "Epoch: 457, train_loss: -2798.49, train_accuracy: 0.842\n",
      "Epoch: 458, train_loss: -2805.33, train_accuracy: 0.841\n",
      "Epoch: 459, train_loss: -2813.01, train_accuracy: 0.839\n",
      "Epoch: 460, train_loss: -2816.84, train_accuracy: 0.839\n",
      "Epoch: 461, train_loss: -2816.10, train_accuracy: 0.839\n",
      "Epoch: 462, train_loss: -2813.73, train_accuracy: 0.840\n",
      "Epoch: 463, train_loss: -2811.00, train_accuracy: 0.839\n",
      "Epoch: 464, train_loss: -2808.14, train_accuracy: 0.839\n",
      "Epoch: 465, train_loss: -2804.34, train_accuracy: 0.840\n",
      "Epoch: 466, train_loss: -2796.68, train_accuracy: 0.841\n",
      "Epoch: 467, train_loss: -2787.75, train_accuracy: 0.842\n",
      "Epoch: 468, train_loss: -2781.17, train_accuracy: 0.841\n",
      "Epoch: 469, train_loss: -2769.13, train_accuracy: 0.844\n",
      "Epoch: 470, train_loss: -2760.29, train_accuracy: 0.846\n",
      "Epoch: 471, train_loss: -2753.07, train_accuracy: 0.847\n",
      "Epoch: 472, train_loss: -2748.27, train_accuracy: 0.849\n",
      "Epoch: 473, train_loss: -2745.49, train_accuracy: 0.850\n",
      "Epoch: 474, train_loss: -2745.22, train_accuracy: 0.851\n",
      "Epoch: 475, train_loss: -2745.52, train_accuracy: 0.852\n",
      "Epoch: 476, train_loss: -2748.92, train_accuracy: 0.851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 477, train_loss: -2753.08, train_accuracy: 0.851\n",
      "Epoch: 478, train_loss: -2757.63, train_accuracy: 0.851\n",
      "Epoch: 479, train_loss: -2761.40, train_accuracy: 0.851\n",
      "Epoch: 480, train_loss: -2761.69, train_accuracy: 0.850\n",
      "Epoch: 481, train_loss: -2760.73, train_accuracy: 0.850\n",
      "Epoch: 482, train_loss: -2756.76, train_accuracy: 0.850\n",
      "Epoch: 483, train_loss: -2748.48, train_accuracy: 0.851\n",
      "Epoch: 484, train_loss: -2742.11, train_accuracy: 0.852\n",
      "Epoch: 485, train_loss: -2738.53, train_accuracy: 0.852\n",
      "Epoch: 486, train_loss: -2732.94, train_accuracy: 0.851\n",
      "Epoch: 487, train_loss: -2727.64, train_accuracy: 0.851\n",
      "Epoch: 488, train_loss: -2722.43, train_accuracy: 0.852\n",
      "Epoch: 489, train_loss: -2720.35, train_accuracy: 0.852\n",
      "Epoch: 490, train_loss: -2718.82, train_accuracy: 0.853\n",
      "Epoch: 491, train_loss: -2717.91, train_accuracy: 0.852\n",
      "Epoch: 492, train_loss: -2718.20, train_accuracy: 0.851\n",
      "Epoch: 493, train_loss: -2719.05, train_accuracy: 0.851\n",
      "Epoch: 494, train_loss: -2719.82, train_accuracy: 0.853\n",
      "Epoch: 495, train_loss: -2720.10, train_accuracy: 0.853\n",
      "Epoch: 496, train_loss: -2719.45, train_accuracy: 0.853\n",
      "Epoch: 497, train_loss: -2719.26, train_accuracy: 0.853\n",
      "Epoch: 498, train_loss: -2718.52, train_accuracy: 0.854\n",
      "Epoch: 499, train_loss: -2715.57, train_accuracy: 0.855\n",
      "Epoch: 500, train_loss: -2712.80, train_accuracy: 0.855\n"
     ]
    }
   ],
   "source": [
    "two_hidden_layer_model = Neural_Network([30,19,13,1],\n",
    "                                        [ReLU,ReLU,sigmoid], \n",
    "                                        [ReLU_grad,ReLU_grad,sigmoid_grad])\n",
    "two_hidden_layer_optimizer = Adam_optimizer(two_hidden_layer_model, \n",
    "                                            learning_rate=0.003)\n",
    "train_model(two_hidden_layer_optimizer, X, y, 500, 32)\n",
    "\n",
    "\n",
    "tree_hidden_layer_model = Neural_Network([30,19,13,7,1], \n",
    "                                         [ReLU,ReLU,ReLU,sigmoid], \n",
    "                                         [ReLU_grad,ReLU_grad,ReLU_grad,sigmoid_grad])\n",
    "tree_hidden_layer_optimizer = Adam_optimizer(tree_hidden_layer_model, \n",
    "                                             learning_rate=0.003)\n",
    "train_model(tree_hidden_layer_optimizer, X, y, 500, 32)\n",
    "\n",
    "\n",
    "four_hidden_layer_model = Neural_Network([30,19,13,7,3,1], \n",
    "                                         [ReLU,ReLU,ReLU,ReLU,sigmoid], \n",
    "                                         [ReLU_grad,ReLU_grad,ReLU_grad,ReLU_grad,sigmoid_grad])\n",
    "four_hidden_layer_optimizer = Adam_optimizer(four_hidden_layer_model, \n",
    "                                             learning_rate=0.003)\n",
    "train_model(four_hidden_layer_optimizer, X, y, 500, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se comparan los diversos modelos entrenados. Se evidencia que, en general, se obtivieron resultados comparables con los obtenidos con TensorFlow; por lo que es posible concluir que se implementó adecuadamente tanto el procedimiento de retropropagación como el algoritmo de optimización "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud con una neurona oculta: 83.41 %\n",
      "Exactitud con dos neuronas ocultas: 83.35 %\n",
      "Exactitud con tres neuronas ocultas: 82.51 %\n",
      "Exactitud con cuatro neuronas ocultas: 82.40 %\n",
      "Esta estimaciones poseen un 3.4% de precisión con una confianza del 95.79%\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_accuracy(one_hidden_layer_model, X_test, y_test)\n",
    "print('Exactitud con una neurona oculta: %.2f %%' % (accuracy*100))\n",
    "accuracy = calculate_accuracy(two_hidden_layer_model, X_test, y_test)\n",
    "print('Exactitud con dos neuronas ocultas: %.2f %%' % (accuracy*100))\n",
    "accuracy = calculate_accuracy(tree_hidden_layer_model, X_test, y_test)\n",
    "print('Exactitud con tres neuronas ocultas: %.2f %%' % (accuracy*100))\n",
    "accuracy = calculate_accuracy(four_hidden_layer_model, X_test, y_test)\n",
    "print('Exactitud con cuatro neuronas ocultas: %.2f %%' % (accuracy*100))\n",
    "\n",
    "precision = 0.034\n",
    "confianza = (1 - (2*math.exp(-2*(precision**2)*total_test_data))) *100\n",
    "\n",
    "print('Esta estimaciones poseen un %.1f%% de precisión con una confianza del %.2f%%' % (precision*100, confianza))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se evidencia, igual que en reto anterior, que los modelos con mejores rendimientos son quellos con menos capas escondidas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
